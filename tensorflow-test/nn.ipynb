{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from model_profiler import model_profiler\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 90\n",
    "epoch_size = 2500\n",
    "test_len = 8\n",
    "conv_factor = 8\n",
    "dense_base = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape\n",
      "(79, 53)\n",
      "\n",
      "output shape\n",
      "(66,)\n"
     ]
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def get_txt_data(path: Path) -> np.ndarray:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        result: list = json.loads(f.readline())\n",
    "        return np.array(flatten(result))\n",
    "\n",
    "\n",
    "input_data: np.ndarray = np.load('data/train_input.npy')\n",
    "output_data: np.ndarray = np.load('data/train_output.npy')\n",
    "input_shape = input_data.shape[1:]\n",
    "output_shape = output_data.shape[1:]\n",
    "\n",
    "print('input shape')\n",
    "print(input_shape)\n",
    "print()\n",
    "print('output shape')\n",
    "print(output_shape)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    l = len(input_data)\n",
    "    last = l - 1\n",
    "    first = int(l-l/test_len)\n",
    "    train_images = input_data[0:first]\n",
    "    train_labels = output_data[0:first]\n",
    "\n",
    "    test_images = input_data[first:last]\n",
    "    test_labels = output_data[first:last]\n",
    "    print('Train shape: {}'.format(train_images.shape))\n",
    "    print('Test shape: {}'.format(test_images.shape))\n",
    "    print('Test labels shape: {}'.format(test_labels.shape))\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (17544, 79, 53)\n",
      "Test shape: (2506, 79, 53)\n",
      "Test labels shape: (2506, 66)\n",
      "4187\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = load_data()\n",
    "input_shape = train_images.shape[1:3]\n",
    "labels_shape = test_labels.shape[1]\n",
    "print(np.prod(input_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalization = tf.keras.layers.Normalization(axis=None)\n",
    "normalization.adapt([0,255.0])\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(input_shape))\n",
    "model.add(normalization)\n",
    "model.add(tf.keras.layers.Conv1D(4 * conv_factor, 32, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(2))\n",
    "model.add(tf.keras.layers.Conv1D(4 * conv_factor * 2, 3, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(dense_base * conv_factor, 'relu'))\n",
    "model.add(tf.keras.layers.Dense(dense_base * conv_factor, 'relu'))\n",
    "model.add(tf.keras.layers.Dense(labels_shape))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = model_profiler(model, batch_size)\n",
    "\n",
    "# print(profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2500\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 131.7876 - val_loss: 110.6066\n",
      "Epoch 2/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 95.4903 - val_loss: 81.6021\n",
      "Epoch 3/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 76.8719 - val_loss: 70.7797\n",
      "Epoch 4/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 69.1326 - val_loss: 65.2098\n",
      "Epoch 5/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 63.8959 - val_loss: 60.6982\n",
      "Epoch 6/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 59.5373 - val_loss: 62.3717\n",
      "Epoch 7/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 56.0001 - val_loss: 56.6147\n",
      "Epoch 8/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 53.4154 - val_loss: 52.1691\n",
      "Epoch 9/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 51.7906 - val_loss: 53.1374\n",
      "Epoch 10/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 49.2956 - val_loss: 48.0121\n",
      "Epoch 11/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 47.5422 - val_loss: 47.1111\n",
      "Epoch 12/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 45.3617 - val_loss: 45.7777\n",
      "Epoch 13/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 43.4475 - val_loss: 43.1509\n",
      "Epoch 14/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 42.0003 - val_loss: 42.2932\n",
      "Epoch 15/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 40.4926 - val_loss: 41.8219\n",
      "Epoch 16/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 39.2286 - val_loss: 39.2822\n",
      "Epoch 17/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.1445 - val_loss: 38.2580\n",
      "Epoch 18/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 37.5745 - val_loss: 38.1411\n",
      "Epoch 19/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 36.1732 - val_loss: 37.5599\n",
      "Epoch 20/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 35.6242 - val_loss: 37.0866\n",
      "Epoch 21/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 34.0610 - val_loss: 35.2201\n",
      "Epoch 22/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 33.4093 - val_loss: 34.6136\n",
      "Epoch 23/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 32.8958 - val_loss: 39.7553\n",
      "Epoch 24/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 32.0096 - val_loss: 33.6728\n",
      "Epoch 25/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 31.1492 - val_loss: 32.1802\n",
      "Epoch 26/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 30.6979 - val_loss: 32.6779\n",
      "Epoch 27/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 29.9518 - val_loss: 33.7480\n",
      "Epoch 28/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 29.1363 - val_loss: 30.4265\n",
      "Epoch 29/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 27.9931 - val_loss: 30.3457\n",
      "Epoch 30/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 26.9699 - val_loss: 30.0031\n",
      "Epoch 31/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 26.2314 - val_loss: 29.4971\n",
      "Epoch 32/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 25.3363 - val_loss: 26.8440\n",
      "Epoch 33/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 23.7071 - val_loss: 28.0760\n",
      "Epoch 34/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 23.2913 - val_loss: 28.2949\n",
      "Epoch 35/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 22.3608 - val_loss: 27.5360\n",
      "Epoch 36/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 21.4995 - val_loss: 25.8077\n",
      "Epoch 37/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 20.7563 - val_loss: 24.5910\n",
      "Epoch 38/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 20.0152 - val_loss: 24.3994\n",
      "Epoch 39/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 19.5534 - val_loss: 23.0043\n",
      "Epoch 40/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 19.2900 - val_loss: 22.7759\n",
      "Epoch 41/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 18.2075 - val_loss: 20.7890\n",
      "Epoch 42/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 17.8157 - val_loss: 23.0790\n",
      "Epoch 43/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 17.0841 - val_loss: 22.1796\n",
      "Epoch 44/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 17.2300 - val_loss: 20.7166\n",
      "Epoch 45/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 16.7280 - val_loss: 22.2194\n",
      "Epoch 46/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 16.2204 - val_loss: 20.1859\n",
      "Epoch 47/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 15.5981 - val_loss: 20.1942\n",
      "Epoch 48/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 15.1082 - val_loss: 19.5509\n",
      "Epoch 49/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 15.4513 - val_loss: 22.0693\n",
      "Epoch 50/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 15.0668 - val_loss: 19.1032\n",
      "Epoch 51/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 14.4362 - val_loss: 19.6028\n",
      "Epoch 52/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 14.2262 - val_loss: 21.0758\n",
      "Epoch 53/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 14.2814 - val_loss: 18.8329\n",
      "Epoch 54/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 14.2374 - val_loss: 19.4651\n",
      "Epoch 55/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 13.3771 - val_loss: 18.7765\n",
      "Epoch 56/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 13.2350 - val_loss: 18.8246\n",
      "Epoch 57/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 12.9945 - val_loss: 17.8780\n",
      "Epoch 58/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 12.8942 - val_loss: 17.3468\n",
      "Epoch 59/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 13.1422 - val_loss: 16.9811\n",
      "Epoch 60/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 12.1361 - val_loss: 17.4278\n",
      "Epoch 61/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 12.7691 - val_loss: 19.6475\n",
      "Epoch 62/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 12.1358 - val_loss: 17.4498\n",
      "Epoch 63/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 12.5836 - val_loss: 17.7276\n",
      "Epoch 64/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 11.6685 - val_loss: 16.0280\n",
      "Epoch 65/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 11.4647 - val_loss: 16.9564\n",
      "Epoch 66/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 11.9838 - val_loss: 15.6696\n",
      "Epoch 67/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 11.1718 - val_loss: 16.6006\n",
      "Epoch 68/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 11.0729 - val_loss: 15.4170\n",
      "Epoch 69/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 11.4477 - val_loss: 16.6817\n",
      "Epoch 70/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.8765 - val_loss: 15.9174\n",
      "Epoch 71/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.9376 - val_loss: 15.6004\n",
      "Epoch 72/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 11.2418 - val_loss: 15.1408\n",
      "Epoch 73/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.4246 - val_loss: 14.9082\n",
      "Epoch 74/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.7558 - val_loss: 16.4580\n",
      "Epoch 75/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.2636 - val_loss: 14.4588\n",
      "Epoch 76/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.0182 - val_loss: 15.2004\n",
      "Epoch 77/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.6555 - val_loss: 15.4031\n",
      "Epoch 78/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.1952 - val_loss: 15.6332\n",
      "Epoch 79/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.9130 - val_loss: 16.7564\n",
      "Epoch 80/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.4240 - val_loss: 14.8144\n",
      "Epoch 81/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 10.7283 - val_loss: 14.7515\n",
      "Epoch 82/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.4322 - val_loss: 14.2031\n",
      "Epoch 83/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.8804 - val_loss: 18.3137\n",
      "Epoch 84/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.6698 - val_loss: 15.0717\n",
      "Epoch 85/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.0169 - val_loss: 14.5827\n",
      "Epoch 86/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.1181 - val_loss: 15.9324\n",
      "Epoch 87/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.9185 - val_loss: 15.4697\n",
      "Epoch 88/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.0238 - val_loss: 14.2687\n",
      "Epoch 89/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.3721 - val_loss: 14.4766\n",
      "Epoch 90/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.1918 - val_loss: 13.7888\n",
      "Epoch 91/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.2919 - val_loss: 20.8491\n",
      "Epoch 92/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.9122 - val_loss: 13.9732\n",
      "Epoch 93/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.0007 - val_loss: 14.5963\n",
      "Epoch 94/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.7709 - val_loss: 14.1606\n",
      "Epoch 95/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.2784 - val_loss: 13.6460\n",
      "Epoch 96/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.7782 - val_loss: 14.5435\n",
      "Epoch 97/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.3672 - val_loss: 13.9282\n",
      "Epoch 98/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.3601 - val_loss: 15.2490\n",
      "Epoch 99/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.2974 - val_loss: 14.2385\n",
      "Epoch 100/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.3374 - val_loss: 12.9313\n",
      "Epoch 101/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.7088 - val_loss: 13.4020\n",
      "Epoch 102/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.8122 - val_loss: 13.7406\n",
      "Epoch 103/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 9.0225 - val_loss: 14.0542\n",
      "Epoch 104/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.1928 - val_loss: 13.1090\n",
      "Epoch 105/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.3282 - val_loss: 13.1798\n",
      "Epoch 106/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.2277 - val_loss: 13.8778\n",
      "Epoch 107/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.4006 - val_loss: 12.9820\n",
      "Epoch 108/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.5476 - val_loss: 14.8622\n",
      "Epoch 109/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 8.1932 - val_loss: 14.3447\n",
      "Epoch 110/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.4844 - val_loss: 16.7224\n",
      "Epoch 111/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.9882 - val_loss: 12.9162\n",
      "Epoch 112/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.6019 - val_loss: 12.8382\n",
      "Epoch 113/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.6294 - val_loss: 14.8335\n",
      "Epoch 114/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.7442 - val_loss: 16.1646\n",
      "Epoch 115/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.5970 - val_loss: 12.7054\n",
      "Epoch 116/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.9491 - val_loss: 14.7336\n",
      "Epoch 117/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.3242 - val_loss: 12.4667\n",
      "Epoch 118/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.0218 - val_loss: 13.0730\n",
      "Epoch 119/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.1215 - val_loss: 12.6353\n",
      "Epoch 120/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.9509 - val_loss: 13.7379\n",
      "Epoch 121/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.0587 - val_loss: 13.1499\n",
      "Epoch 122/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.8899 - val_loss: 12.9768\n",
      "Epoch 123/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.2680 - val_loss: 12.7650\n",
      "Epoch 124/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.9148 - val_loss: 12.6176\n",
      "Epoch 125/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.9467 - val_loss: 12.4988\n",
      "Epoch 126/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.2229 - val_loss: 12.2506\n",
      "Epoch 127/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.3284 - val_loss: 13.1651\n",
      "Epoch 128/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.6376 - val_loss: 12.4480\n",
      "Epoch 129/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.4902 - val_loss: 14.4431\n",
      "Epoch 130/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.8847 - val_loss: 12.7425\n",
      "Epoch 131/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.8624 - val_loss: 12.4904\n",
      "Epoch 132/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.3290 - val_loss: 12.5954\n",
      "Epoch 133/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.6819 - val_loss: 13.4701\n",
      "Epoch 134/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.8007 - val_loss: 13.5152\n",
      "Epoch 135/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.2039 - val_loss: 12.3592\n",
      "Epoch 136/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.7692 - val_loss: 12.2552\n",
      "Epoch 137/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.6122 - val_loss: 12.6903\n",
      "Epoch 138/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.7830 - val_loss: 12.9785\n",
      "Epoch 139/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.2298 - val_loss: 12.4910\n",
      "Epoch 140/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.4126 - val_loss: 12.4111\n",
      "Epoch 141/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.4196 - val_loss: 12.1265\n",
      "Epoch 142/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.9741 - val_loss: 11.8190\n",
      "Epoch 143/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.3930 - val_loss: 12.1775\n",
      "Epoch 144/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.8845 - val_loss: 12.3376\n",
      "Epoch 145/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.4925 - val_loss: 11.9873\n",
      "Epoch 146/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.1783 - val_loss: 12.3110\n",
      "Epoch 147/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.4334 - val_loss: 11.9763\n",
      "Epoch 148/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.3490 - val_loss: 12.9435\n",
      "Epoch 149/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.7939 - val_loss: 15.7805\n",
      "Epoch 150/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 7.0722 - val_loss: 12.3309\n",
      "Epoch 151/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.4047 - val_loss: 13.2097\n",
      "Epoch 152/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.8513 - val_loss: 12.1520\n",
      "Epoch 153/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.2270 - val_loss: 12.3066\n",
      "Epoch 154/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.4734 - val_loss: 13.4181\n",
      "Epoch 155/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.2920 - val_loss: 11.9086\n",
      "Epoch 156/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.3919 - val_loss: 12.3202\n",
      "Epoch 157/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.5347 - val_loss: 12.8954\n",
      "Epoch 158/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.7033 - val_loss: 12.2533\n",
      "Epoch 159/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.0956 - val_loss: 11.5041\n",
      "Epoch 160/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.8941 - val_loss: 12.7078\n",
      "Epoch 161/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.5883 - val_loss: 12.2283\n",
      "Epoch 162/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.9832 - val_loss: 12.7356\n",
      "Epoch 163/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.8064 - val_loss: 13.5556\n",
      "Epoch 164/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.6903 - val_loss: 12.1790\n",
      "Epoch 165/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.8635 - val_loss: 12.2336\n",
      "Epoch 166/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.2363 - val_loss: 11.7387\n",
      "Epoch 167/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.2235 - val_loss: 11.6690\n",
      "Epoch 168/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.9551 - val_loss: 12.6925\n",
      "Epoch 169/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.8509 - val_loss: 13.1194\n",
      "Epoch 170/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.8373 - val_loss: 12.4818\n",
      "Epoch 171/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.1519 - val_loss: 12.1901\n",
      "Epoch 172/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.1141 - val_loss: 12.1727\n",
      "Epoch 173/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.0294 - val_loss: 11.9800\n",
      "Epoch 174/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.2106 - val_loss: 12.0358\n",
      "Epoch 175/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.2544 - val_loss: 11.5098\n",
      "Epoch 176/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.2510 - val_loss: 11.7148\n",
      "Epoch 177/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.3009 - val_loss: 11.6821\n",
      "Epoch 178/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.0131 - val_loss: 13.5293\n",
      "Epoch 179/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.1426 - val_loss: 11.8417\n",
      "Epoch 180/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5174 - val_loss: 11.5599\n",
      "Epoch 181/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.8601 - val_loss: 12.7521\n",
      "Epoch 182/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.7008 - val_loss: 11.7533\n",
      "Epoch 183/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.9707 - val_loss: 12.0111\n",
      "Epoch 184/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5782 - val_loss: 11.5886\n",
      "Epoch 185/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.0222 - val_loss: 11.6941\n",
      "Epoch 186/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.9076 - val_loss: 11.8080\n",
      "Epoch 187/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4363 - val_loss: 11.3846\n",
      "Epoch 188/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.6000 - val_loss: 12.4310\n",
      "Epoch 189/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5882 - val_loss: 12.6123\n",
      "Epoch 190/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.1680 - val_loss: 11.9905\n",
      "Epoch 191/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.6086 - val_loss: 11.7937\n",
      "Epoch 192/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.3078 - val_loss: 11.7225\n",
      "Epoch 193/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5184 - val_loss: 11.2545\n",
      "Epoch 194/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5952 - val_loss: 11.7211\n",
      "Epoch 195/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.3141 - val_loss: 12.2724\n",
      "Epoch 196/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3627 - val_loss: 11.7779\n",
      "Epoch 197/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 6.0958 - val_loss: 11.5078\n",
      "Epoch 198/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3310 - val_loss: 12.8559\n",
      "Epoch 199/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4807 - val_loss: 11.2297\n",
      "Epoch 200/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5724 - val_loss: 11.2186\n",
      "Epoch 201/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.7433 - val_loss: 11.2750\n",
      "Epoch 202/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.7046 - val_loss: 11.8423\n",
      "Epoch 203/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.6173 - val_loss: 11.6144\n",
      "Epoch 204/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.7236 - val_loss: 11.4688\n",
      "Epoch 205/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5910 - val_loss: 11.5064\n",
      "Epoch 206/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3830 - val_loss: 11.7958\n",
      "Epoch 207/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4516 - val_loss: 11.5991\n",
      "Epoch 208/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.2153 - val_loss: 11.5082\n",
      "Epoch 209/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.2878 - val_loss: 11.4752\n",
      "Epoch 210/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.7629 - val_loss: 11.4597\n",
      "Epoch 211/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3298 - val_loss: 12.5008\n",
      "Epoch 212/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.7395 - val_loss: 16.3427\n",
      "Epoch 213/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5305 - val_loss: 12.5322\n",
      "Epoch 214/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4589 - val_loss: 12.8704\n",
      "Epoch 215/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.6362 - val_loss: 11.8611\n",
      "Epoch 216/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5627 - val_loss: 11.1764\n",
      "Epoch 217/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5037 - val_loss: 11.7225\n",
      "Epoch 218/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1084 - val_loss: 12.2987\n",
      "Epoch 219/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5219 - val_loss: 12.1023\n",
      "Epoch 220/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.2057 - val_loss: 11.5271\n",
      "Epoch 221/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.2252 - val_loss: 11.9043\n",
      "Epoch 222/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3390 - val_loss: 11.3208\n",
      "Epoch 223/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4218 - val_loss: 11.4731\n",
      "Epoch 224/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5652 - val_loss: 11.7947\n",
      "Epoch 225/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.6481 - val_loss: 12.6326\n",
      "Epoch 226/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.2636 - val_loss: 11.2249\n",
      "Epoch 227/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0178 - val_loss: 11.3998\n",
      "Epoch 228/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5314 - val_loss: 11.1222\n",
      "Epoch 229/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0271 - val_loss: 14.5270\n",
      "Epoch 230/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3592 - val_loss: 12.4111\n",
      "Epoch 231/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0201 - val_loss: 11.1913\n",
      "Epoch 232/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5625 - val_loss: 11.6810\n",
      "Epoch 233/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.5004 - val_loss: 13.0856\n",
      "Epoch 234/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0468 - val_loss: 11.1509\n",
      "Epoch 235/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1102 - val_loss: 12.4937\n",
      "Epoch 236/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3486 - val_loss: 11.9243\n",
      "Epoch 237/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1966 - val_loss: 11.5948\n",
      "Epoch 238/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3623 - val_loss: 11.7079\n",
      "Epoch 239/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4107 - val_loss: 11.2618\n",
      "Epoch 240/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0051 - val_loss: 11.0282\n",
      "Epoch 241/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3843 - val_loss: 11.2574\n",
      "Epoch 242/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1839 - val_loss: 11.5594\n",
      "Epoch 243/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8405 - val_loss: 11.2133\n",
      "Epoch 244/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8416 - val_loss: 11.8977\n",
      "Epoch 245/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4187 - val_loss: 13.3049\n",
      "Epoch 246/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.9064 - val_loss: 11.1039\n",
      "Epoch 247/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6048 - val_loss: 12.2963\n",
      "Epoch 248/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8589 - val_loss: 11.3665\n",
      "Epoch 249/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0945 - val_loss: 11.6439\n",
      "Epoch 250/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4559 - val_loss: 14.1628\n",
      "Epoch 251/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1093 - val_loss: 11.0943\n",
      "Epoch 252/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8862 - val_loss: 10.8666\n",
      "Epoch 253/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6679 - val_loss: 10.7952\n",
      "Epoch 254/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7666 - val_loss: 11.3242\n",
      "Epoch 255/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1776 - val_loss: 10.8911\n",
      "Epoch 256/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.2537 - val_loss: 11.6238\n",
      "Epoch 257/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6186 - val_loss: 11.0601\n",
      "Epoch 258/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.4090 - val_loss: 11.0901\n",
      "Epoch 259/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6852 - val_loss: 11.0488\n",
      "Epoch 260/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6862 - val_loss: 11.1856\n",
      "Epoch 261/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.8137 - val_loss: 12.4454\n",
      "Epoch 262/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1748 - val_loss: 11.8011\n",
      "Epoch 263/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7856 - val_loss: 10.9283\n",
      "Epoch 264/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5824 - val_loss: 11.0773\n",
      "Epoch 265/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9948 - val_loss: 11.2520\n",
      "Epoch 266/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0892 - val_loss: 11.4824\n",
      "Epoch 267/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9471 - val_loss: 11.4081\n",
      "Epoch 268/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7007 - val_loss: 11.2842\n",
      "Epoch 269/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5679 - val_loss: 11.2224\n",
      "Epoch 270/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1296 - val_loss: 13.4098\n",
      "Epoch 271/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7264 - val_loss: 11.7084\n",
      "Epoch 272/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1031 - val_loss: 12.1240\n",
      "Epoch 273/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9224 - val_loss: 10.8787\n",
      "Epoch 274/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6581 - val_loss: 11.0705\n",
      "Epoch 275/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5104 - val_loss: 10.9756\n",
      "Epoch 276/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.2106 - val_loss: 13.0790\n",
      "Epoch 277/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6530 - val_loss: 11.5828\n",
      "Epoch 278/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9409 - val_loss: 11.6510\n",
      "Epoch 279/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6872 - val_loss: 12.7010\n",
      "Epoch 280/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9484 - val_loss: 10.9700\n",
      "Epoch 281/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0193 - val_loss: 11.6664\n",
      "Epoch 282/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1251 - val_loss: 10.9668\n",
      "Epoch 283/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0978 - val_loss: 11.2549\n",
      "Epoch 284/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.1739 - val_loss: 11.9151\n",
      "Epoch 285/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7575 - val_loss: 11.5185\n",
      "Epoch 286/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5588 - val_loss: 11.6966\n",
      "Epoch 287/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8063 - val_loss: 12.6242\n",
      "Epoch 288/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8181 - val_loss: 10.9718\n",
      "Epoch 289/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7191 - val_loss: 11.2340\n",
      "Epoch 290/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3865 - val_loss: 10.9362\n",
      "Epoch 291/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3772 - val_loss: 12.1228\n",
      "Epoch 292/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9399 - val_loss: 13.7261\n",
      "Epoch 293/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7326 - val_loss: 11.5431\n",
      "Epoch 294/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9480 - val_loss: 11.0603\n",
      "Epoch 295/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.3257 - val_loss: 11.0117\n",
      "Epoch 296/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9016 - val_loss: 10.6589\n",
      "Epoch 297/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5391 - val_loss: 10.8079\n",
      "Epoch 298/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8426 - val_loss: 10.8495\n",
      "Epoch 299/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7830 - val_loss: 11.8981\n",
      "Epoch 300/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3923 - val_loss: 10.7084\n",
      "Epoch 301/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5987 - val_loss: 10.9367\n",
      "Epoch 302/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4995 - val_loss: 11.9018\n",
      "Epoch 303/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7583 - val_loss: 10.9969\n",
      "Epoch 304/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8054 - val_loss: 10.9161\n",
      "Epoch 305/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8432 - val_loss: 11.0077\n",
      "Epoch 306/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6580 - val_loss: 11.1878\n",
      "Epoch 307/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4823 - val_loss: 11.1574\n",
      "Epoch 308/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4464 - val_loss: 10.8605\n",
      "Epoch 309/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5217 - val_loss: 11.8259\n",
      "Epoch 310/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6056 - val_loss: 10.7928\n",
      "Epoch 311/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7919 - val_loss: 11.3934\n",
      "Epoch 312/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5380 - val_loss: 11.6010\n",
      "Epoch 313/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4015 - val_loss: 10.9988\n",
      "Epoch 314/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6784 - val_loss: 11.1824\n",
      "Epoch 315/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6233 - val_loss: 11.0111\n",
      "Epoch 316/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3713 - val_loss: 10.9569\n",
      "Epoch 317/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6194 - val_loss: 11.0572\n",
      "Epoch 318/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3528 - val_loss: 10.7711\n",
      "Epoch 319/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2233 - val_loss: 11.1912\n",
      "Epoch 320/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6947 - val_loss: 11.2135\n",
      "Epoch 321/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9221 - val_loss: 11.0396\n",
      "Epoch 322/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7790 - val_loss: 10.9845\n",
      "Epoch 323/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5077 - val_loss: 12.4574\n",
      "Epoch 324/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6008 - val_loss: 11.5362\n",
      "Epoch 325/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3108 - val_loss: 10.7105\n",
      "Epoch 326/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5589 - val_loss: 12.0096\n",
      "Epoch 327/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3056 - val_loss: 11.1155\n",
      "Epoch 328/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5157 - val_loss: 10.9229\n",
      "Epoch 329/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6834 - val_loss: 11.7390\n",
      "Epoch 330/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7013 - val_loss: 10.6655\n",
      "Epoch 331/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3924 - val_loss: 11.1075\n",
      "Epoch 332/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4574 - val_loss: 10.8221\n",
      "Epoch 333/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0933 - val_loss: 10.8910\n",
      "Epoch 334/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2079 - val_loss: 10.9137\n",
      "Epoch 335/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3720 - val_loss: 12.4893\n",
      "Epoch 336/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 5.0367 - val_loss: 11.7128\n",
      "Epoch 337/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9926 - val_loss: 12.4845\n",
      "Epoch 338/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4358 - val_loss: 11.2006\n",
      "Epoch 339/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0143 - val_loss: 11.0311\n",
      "Epoch 340/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0900 - val_loss: 10.8486\n",
      "Epoch 341/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6394 - val_loss: 10.6988\n",
      "Epoch 342/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1613 - val_loss: 11.0252\n",
      "Epoch 343/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5436 - val_loss: 10.8487\n",
      "Epoch 344/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6460 - val_loss: 11.2116\n",
      "Epoch 345/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8739 - val_loss: 10.7754\n",
      "Epoch 346/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9683 - val_loss: 10.9849\n",
      "Epoch 347/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2006 - val_loss: 10.8019\n",
      "Epoch 348/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2855 - val_loss: 10.8549\n",
      "Epoch 349/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2490 - val_loss: 11.5160\n",
      "Epoch 350/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4084 - val_loss: 11.1850\n",
      "Epoch 351/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4352 - val_loss: 11.1040\n",
      "Epoch 352/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5633 - val_loss: 11.1592\n",
      "Epoch 353/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1320 - val_loss: 10.7376\n",
      "Epoch 354/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1646 - val_loss: 11.1251\n",
      "Epoch 355/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3922 - val_loss: 10.8715\n",
      "Epoch 356/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7150 - val_loss: 11.3787\n",
      "Epoch 357/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4053 - val_loss: 11.3184\n",
      "Epoch 358/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6317 - val_loss: 10.7028\n",
      "Epoch 359/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1469 - val_loss: 10.8951\n",
      "Epoch 360/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1394 - val_loss: 10.6340\n",
      "Epoch 361/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5360 - val_loss: 11.4961\n",
      "Epoch 362/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2328 - val_loss: 10.7190\n",
      "Epoch 363/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1697 - val_loss: 10.7190\n",
      "Epoch 364/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6268 - val_loss: 10.6682\n",
      "Epoch 365/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6480 - val_loss: 10.8215\n",
      "Epoch 366/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3328 - val_loss: 11.3392\n",
      "Epoch 367/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4143 - val_loss: 14.6366\n",
      "Epoch 368/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3826 - val_loss: 10.7010\n",
      "Epoch 369/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0398 - val_loss: 10.8189\n",
      "Epoch 370/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6016 - val_loss: 11.7416\n",
      "Epoch 371/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.8099 - val_loss: 11.7889\n",
      "Epoch 372/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4101 - val_loss: 10.6571\n",
      "Epoch 373/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1544 - val_loss: 11.6284\n",
      "Epoch 374/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4771 - val_loss: 12.3382\n",
      "Epoch 375/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5304 - val_loss: 10.6946\n",
      "Epoch 376/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8596 - val_loss: 10.8573\n",
      "Epoch 377/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2409 - val_loss: 13.4375\n",
      "Epoch 378/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4776 - val_loss: 11.3760\n",
      "Epoch 379/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.6005 - val_loss: 10.7749\n",
      "Epoch 380/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1004 - val_loss: 10.8373\n",
      "Epoch 381/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0200 - val_loss: 11.2753\n",
      "Epoch 382/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2445 - val_loss: 11.9682\n",
      "Epoch 383/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0155 - val_loss: 10.5590\n",
      "Epoch 384/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5986 - val_loss: 10.7984\n",
      "Epoch 385/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0783 - val_loss: 11.9290\n",
      "Epoch 386/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3299 - val_loss: 11.3360\n",
      "Epoch 387/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2877 - val_loss: 10.8992\n",
      "Epoch 388/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0424 - val_loss: 11.1426\n",
      "Epoch 389/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9864 - val_loss: 10.7446\n",
      "Epoch 390/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9979 - val_loss: 10.9222\n",
      "Epoch 391/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3128 - val_loss: 11.0635\n",
      "Epoch 392/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2218 - val_loss: 12.3624\n",
      "Epoch 393/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4654 - val_loss: 10.8909\n",
      "Epoch 394/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8412 - val_loss: 11.1361\n",
      "Epoch 395/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1514 - val_loss: 10.6792\n",
      "Epoch 396/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1787 - val_loss: 10.8813\n",
      "Epoch 397/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4360 - val_loss: 11.6097\n",
      "Epoch 398/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0597 - val_loss: 10.8722\n",
      "Epoch 399/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9306 - val_loss: 10.8164\n",
      "Epoch 400/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9020 - val_loss: 10.5531\n",
      "Epoch 401/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5419 - val_loss: 12.1433\n",
      "Epoch 402/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3958 - val_loss: 10.8786\n",
      "Epoch 403/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9064 - val_loss: 10.8134\n",
      "Epoch 404/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9442 - val_loss: 11.1638\n",
      "Epoch 405/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9919 - val_loss: 10.7968\n",
      "Epoch 406/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4315 - val_loss: 11.4224\n",
      "Epoch 407/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7469 - val_loss: 10.9717\n",
      "Epoch 408/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1192 - val_loss: 10.6992\n",
      "Epoch 409/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6850 - val_loss: 10.9665\n",
      "Epoch 410/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1534 - val_loss: 10.9390\n",
      "Epoch 411/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3199 - val_loss: 11.4625\n",
      "Epoch 412/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4696 - val_loss: 11.4014\n",
      "Epoch 413/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5604 - val_loss: 11.1162\n",
      "Epoch 414/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8567 - val_loss: 10.8807\n",
      "Epoch 415/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2140 - val_loss: 11.7550\n",
      "Epoch 416/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0459 - val_loss: 11.1011\n",
      "Epoch 417/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9223 - val_loss: 10.4884\n",
      "Epoch 418/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8267 - val_loss: 10.9250\n",
      "Epoch 419/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2604 - val_loss: 10.8386\n",
      "Epoch 420/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9094 - val_loss: 10.8148\n",
      "Epoch 421/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4655 - val_loss: 10.5984\n",
      "Epoch 422/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8590 - val_loss: 10.8909\n",
      "Epoch 423/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8901 - val_loss: 10.9617\n",
      "Epoch 424/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1257 - val_loss: 10.8247\n",
      "Epoch 425/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8960 - val_loss: 11.3097\n",
      "Epoch 426/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1941 - val_loss: 11.2325\n",
      "Epoch 427/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9557 - val_loss: 10.7224\n",
      "Epoch 428/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9437 - val_loss: 10.6365\n",
      "Epoch 429/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8008 - val_loss: 10.6589\n",
      "Epoch 430/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0313 - val_loss: 10.6191\n",
      "Epoch 431/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9086 - val_loss: 11.1631\n",
      "Epoch 432/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1796 - val_loss: 11.1764\n",
      "Epoch 433/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.7837 - val_loss: 11.1361\n",
      "Epoch 434/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9579 - val_loss: 11.0996\n",
      "Epoch 435/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0142 - val_loss: 11.2373\n",
      "Epoch 436/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7791 - val_loss: 10.8393\n",
      "Epoch 437/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9490 - val_loss: 10.7089\n",
      "Epoch 438/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1941 - val_loss: 10.8204\n",
      "Epoch 439/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0046 - val_loss: 10.7006\n",
      "Epoch 440/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8550 - val_loss: 10.9225\n",
      "Epoch 441/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1623 - val_loss: 10.7247\n",
      "Epoch 442/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5177 - val_loss: 11.4847\n",
      "Epoch 443/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0872 - val_loss: 11.1067\n",
      "Epoch 444/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8100 - val_loss: 10.5263\n",
      "Epoch 445/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0222 - val_loss: 10.9864\n",
      "Epoch 446/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8249 - val_loss: 10.8669\n",
      "Epoch 447/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8277 - val_loss: 10.7101\n",
      "Epoch 448/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9094 - val_loss: 10.5203\n",
      "Epoch 449/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2319 - val_loss: 10.7234\n",
      "Epoch 450/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9531 - val_loss: 10.8883\n",
      "Epoch 451/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5099 - val_loss: 10.7559\n",
      "Epoch 452/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9059 - val_loss: 10.6608\n",
      "Epoch 453/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6188 - val_loss: 10.8351\n",
      "Epoch 454/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0111 - val_loss: 10.8750\n",
      "Epoch 455/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9444 - val_loss: 10.4963\n",
      "Epoch 456/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8395 - val_loss: 10.9928\n",
      "Epoch 457/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7369 - val_loss: 10.5910\n",
      "Epoch 458/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.4894 - val_loss: 10.7841\n",
      "Epoch 459/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0591 - val_loss: 10.9343\n",
      "Epoch 460/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8392 - val_loss: 10.6950\n",
      "Epoch 461/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7648 - val_loss: 10.6003\n",
      "Epoch 462/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8426 - val_loss: 11.6366\n",
      "Epoch 463/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8971 - val_loss: 10.8736\n",
      "Epoch 464/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7419 - val_loss: 10.6937\n",
      "Epoch 465/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8078 - val_loss: 11.0094\n",
      "Epoch 466/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1897 - val_loss: 11.0765\n",
      "Epoch 467/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1240 - val_loss: 10.8368\n",
      "Epoch 468/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.5804 - val_loss: 10.5731\n",
      "Epoch 469/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2322 - val_loss: 10.7962\n",
      "Epoch 470/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7365 - val_loss: 11.2188\n",
      "Epoch 471/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6251 - val_loss: 10.8643\n",
      "Epoch 472/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6756 - val_loss: 10.8869\n",
      "Epoch 473/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6885 - val_loss: 11.0634\n",
      "Epoch 474/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0397 - val_loss: 10.5612\n",
      "Epoch 475/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0302 - val_loss: 11.0497\n",
      "Epoch 476/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8686 - val_loss: 11.1216\n",
      "Epoch 477/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1746 - val_loss: 10.6991\n",
      "Epoch 478/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8779 - val_loss: 10.5964\n",
      "Epoch 479/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0626 - val_loss: 11.0179\n",
      "Epoch 480/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1294 - val_loss: 11.0027\n",
      "Epoch 481/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8386 - val_loss: 10.4797\n",
      "Epoch 482/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6986 - val_loss: 10.5590\n",
      "Epoch 483/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7100 - val_loss: 10.5957\n",
      "Epoch 484/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9202 - val_loss: 10.9897\n",
      "Epoch 485/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9823 - val_loss: 10.7163\n",
      "Epoch 486/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5796 - val_loss: 10.4287\n",
      "Epoch 487/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8596 - val_loss: 10.9951\n",
      "Epoch 488/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7513 - val_loss: 10.8490\n",
      "Epoch 489/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2094 - val_loss: 10.5479\n",
      "Epoch 490/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8552 - val_loss: 11.8564\n",
      "Epoch 491/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0708 - val_loss: 10.6586\n",
      "Epoch 492/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7569 - val_loss: 10.5161\n",
      "Epoch 493/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7592 - val_loss: 10.7136\n",
      "Epoch 494/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0447 - val_loss: 10.9679\n",
      "Epoch 495/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1316 - val_loss: 10.6983\n",
      "Epoch 496/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8826 - val_loss: 11.1556\n",
      "Epoch 497/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1015 - val_loss: 11.3389\n",
      "Epoch 498/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7044 - val_loss: 10.9107\n",
      "Epoch 499/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7692 - val_loss: 11.6209\n",
      "Epoch 500/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8125 - val_loss: 10.6024\n",
      "Epoch 501/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7482 - val_loss: 10.7511\n",
      "Epoch 502/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9330 - val_loss: 11.0452\n",
      "Epoch 503/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8298 - val_loss: 11.9983\n",
      "Epoch 504/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8226 - val_loss: 11.5122\n",
      "Epoch 505/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9899 - val_loss: 10.6050\n",
      "Epoch 506/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6131 - val_loss: 10.5261\n",
      "Epoch 507/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0490 - val_loss: 10.7841\n",
      "Epoch 508/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2102 - val_loss: 10.9587\n",
      "Epoch 509/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9992 - val_loss: 10.3683\n",
      "Epoch 510/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6628 - val_loss: 10.9761\n",
      "Epoch 511/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7892 - val_loss: 10.6332\n",
      "Epoch 512/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7194 - val_loss: 10.3873\n",
      "Epoch 513/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6684 - val_loss: 10.5272\n",
      "Epoch 514/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7625 - val_loss: 10.5146\n",
      "Epoch 515/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0398 - val_loss: 10.8790\n",
      "Epoch 516/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6165 - val_loss: 10.6232\n",
      "Epoch 517/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6121 - val_loss: 11.3525\n",
      "Epoch 518/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8207 - val_loss: 10.9228\n",
      "Epoch 519/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1519 - val_loss: 10.5928\n",
      "Epoch 520/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9152 - val_loss: 10.5717\n",
      "Epoch 521/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7244 - val_loss: 10.6923\n",
      "Epoch 522/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7439 - val_loss: 10.9258\n",
      "Epoch 523/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7762 - val_loss: 10.7388\n",
      "Epoch 524/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8676 - val_loss: 11.8060\n",
      "Epoch 525/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9287 - val_loss: 11.2514\n",
      "Epoch 526/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7186 - val_loss: 13.0233\n",
      "Epoch 527/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7914 - val_loss: 11.3866\n",
      "Epoch 528/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5576 - val_loss: 10.2984\n",
      "Epoch 529/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8238 - val_loss: 10.6351\n",
      "Epoch 530/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9798 - val_loss: 12.6366\n",
      "Epoch 531/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0007 - val_loss: 10.5874\n",
      "Epoch 532/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6606 - val_loss: 10.7491\n",
      "Epoch 533/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6441 - val_loss: 10.4088\n",
      "Epoch 534/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6176 - val_loss: 11.2701\n",
      "Epoch 535/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9029 - val_loss: 10.6537\n",
      "Epoch 536/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5142 - val_loss: 10.3480\n",
      "Epoch 537/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8271 - val_loss: 10.6695\n",
      "Epoch 538/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6890 - val_loss: 11.1023\n",
      "Epoch 539/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7596 - val_loss: 11.0652\n",
      "Epoch 540/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1543 - val_loss: 10.7342\n",
      "Epoch 541/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9488 - val_loss: 11.2587\n",
      "Epoch 542/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6068 - val_loss: 10.9918\n",
      "Epoch 543/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5857 - val_loss: 10.5942\n",
      "Epoch 544/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4746 - val_loss: 11.0340\n",
      "Epoch 545/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9397 - val_loss: 11.0930\n",
      "Epoch 546/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5219 - val_loss: 10.6285\n",
      "Epoch 547/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5675 - val_loss: 10.4976\n",
      "Epoch 548/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0246 - val_loss: 10.9296\n",
      "Epoch 549/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5000 - val_loss: 10.7301\n",
      "Epoch 550/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6673 - val_loss: 10.3624\n",
      "Epoch 551/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7355 - val_loss: 10.8367\n",
      "Epoch 552/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5329 - val_loss: 10.8074\n",
      "Epoch 553/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8082 - val_loss: 10.6421\n",
      "Epoch 554/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9317 - val_loss: 10.5613\n",
      "Epoch 555/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7034 - val_loss: 11.1431\n",
      "Epoch 556/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6685 - val_loss: 10.4670\n",
      "Epoch 557/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6299 - val_loss: 10.8178\n",
      "Epoch 558/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5571 - val_loss: 10.6630\n",
      "Epoch 559/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6751 - val_loss: 11.2130\n",
      "Epoch 560/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8198 - val_loss: 10.8519\n",
      "Epoch 561/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8942 - val_loss: 11.4170\n",
      "Epoch 562/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7492 - val_loss: 10.4790\n",
      "Epoch 563/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1147 - val_loss: 11.6909\n",
      "Epoch 564/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7814 - val_loss: 10.6482\n",
      "Epoch 565/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4172 - val_loss: 10.7796\n",
      "Epoch 566/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3378 - val_loss: 10.5480\n",
      "Epoch 567/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6196 - val_loss: 11.5352\n",
      "Epoch 568/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4400 - val_loss: 10.5215\n",
      "Epoch 569/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5865 - val_loss: 11.6390\n",
      "Epoch 570/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9431 - val_loss: 10.4521\n",
      "Epoch 571/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5124 - val_loss: 10.9802\n",
      "Epoch 572/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2993 - val_loss: 10.5574\n",
      "Epoch 573/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6097 - val_loss: 10.6536\n",
      "Epoch 574/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5864 - val_loss: 10.5253\n",
      "Epoch 575/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5216 - val_loss: 10.7032\n",
      "Epoch 576/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9219 - val_loss: 10.4922\n",
      "Epoch 577/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4531 - val_loss: 10.5054\n",
      "Epoch 578/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5218 - val_loss: 10.3950\n",
      "Epoch 579/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7310 - val_loss: 10.5885\n",
      "Epoch 580/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6192 - val_loss: 10.6198\n",
      "Epoch 581/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7418 - val_loss: 10.6484\n",
      "Epoch 582/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9214 - val_loss: 11.1409\n",
      "Epoch 583/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4672 - val_loss: 11.0029\n",
      "Epoch 584/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4640 - val_loss: 10.6967\n",
      "Epoch 585/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5253 - val_loss: 10.8693\n",
      "Epoch 586/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9323 - val_loss: 10.5673\n",
      "Epoch 587/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6107 - val_loss: 10.4361\n",
      "Epoch 588/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6539 - val_loss: 10.8235\n",
      "Epoch 589/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7440 - val_loss: 10.4033\n",
      "Epoch 590/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3668 - val_loss: 10.3650\n",
      "Epoch 591/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5337 - val_loss: 10.8796\n",
      "Epoch 592/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5634 - val_loss: 10.7946\n",
      "Epoch 593/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5794 - val_loss: 10.5719\n",
      "Epoch 594/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4958 - val_loss: 10.7525\n",
      "Epoch 595/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2008 - val_loss: 10.4549\n",
      "Epoch 596/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4222 - val_loss: 10.3579\n",
      "Epoch 597/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9462 - val_loss: 10.3922\n",
      "Epoch 598/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3218 - val_loss: 10.4731\n",
      "Epoch 599/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9824 - val_loss: 10.5673\n",
      "Epoch 600/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4890 - val_loss: 10.3485\n",
      "Epoch 601/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8361 - val_loss: 11.0938\n",
      "Epoch 602/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7785 - val_loss: 10.5340\n",
      "Epoch 603/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3212 - val_loss: 10.4503\n",
      "Epoch 604/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7213 - val_loss: 11.7864\n",
      "Epoch 605/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7839 - val_loss: 10.4678\n",
      "Epoch 606/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3721 - val_loss: 10.9001\n",
      "Epoch 607/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6185 - val_loss: 10.5237\n",
      "Epoch 608/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3688 - val_loss: 10.3753\n",
      "Epoch 609/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4769 - val_loss: 10.4742\n",
      "Epoch 610/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4537 - val_loss: 10.5560\n",
      "Epoch 611/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3705 - val_loss: 10.7444\n",
      "Epoch 612/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5163 - val_loss: 10.6154\n",
      "Epoch 613/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4963 - val_loss: 10.7684\n",
      "Epoch 614/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1559 - val_loss: 11.0760\n",
      "Epoch 615/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6524 - val_loss: 10.8483\n",
      "Epoch 616/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4592 - val_loss: 10.3843\n",
      "Epoch 617/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4137 - val_loss: 10.5385\n",
      "Epoch 618/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8488 - val_loss: 10.4214\n",
      "Epoch 619/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5578 - val_loss: 10.8175\n",
      "Epoch 620/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5865 - val_loss: 10.8979\n",
      "Epoch 621/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5768 - val_loss: 11.1236\n",
      "Epoch 622/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8560 - val_loss: 10.5425\n",
      "Epoch 623/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7337 - val_loss: 10.4165\n",
      "Epoch 624/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2688 - val_loss: 10.5417\n",
      "Epoch 625/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5249 - val_loss: 10.9997\n",
      "Epoch 626/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7655 - val_loss: 10.9837\n",
      "Epoch 627/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5389 - val_loss: 11.1568\n",
      "Epoch 628/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5164 - val_loss: 12.1747\n",
      "Epoch 629/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5833 - val_loss: 10.4430\n",
      "Epoch 630/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4539 - val_loss: 10.7138\n",
      "Epoch 631/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6865 - val_loss: 11.7246\n",
      "Epoch 632/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9420 - val_loss: 11.3417\n",
      "Epoch 633/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3412 - val_loss: 10.6484\n",
      "Epoch 634/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8343 - val_loss: 11.1925\n",
      "Epoch 635/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5440 - val_loss: 10.8392\n",
      "Epoch 636/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3109 - val_loss: 10.4405\n",
      "Epoch 637/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6525 - val_loss: 10.8891\n",
      "Epoch 638/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6287 - val_loss: 11.1171\n",
      "Epoch 639/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3715 - val_loss: 10.9393\n",
      "Epoch 640/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7240 - val_loss: 10.5529\n",
      "Epoch 641/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8270 - val_loss: 11.0586\n",
      "Epoch 642/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4017 - val_loss: 10.5312\n",
      "Epoch 643/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4245 - val_loss: 10.3100\n",
      "Epoch 644/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3763 - val_loss: 10.8246\n",
      "Epoch 645/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6668 - val_loss: 10.6606\n",
      "Epoch 646/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7733 - val_loss: 10.6920\n",
      "Epoch 647/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3267 - val_loss: 10.5680\n",
      "Epoch 648/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4545 - val_loss: 10.6683\n",
      "Epoch 649/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3536 - val_loss: 10.4256\n",
      "Epoch 650/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6044 - val_loss: 10.6207\n",
      "Epoch 651/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3431 - val_loss: 10.5753\n",
      "Epoch 652/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.0083 - val_loss: 10.6819\n",
      "Epoch 653/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8812 - val_loss: 10.5233\n",
      "Epoch 654/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2730 - val_loss: 10.2611\n",
      "Epoch 655/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3264 - val_loss: 10.5913\n",
      "Epoch 656/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4420 - val_loss: 10.4276\n",
      "Epoch 657/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1878 - val_loss: 10.4538\n",
      "Epoch 658/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5817 - val_loss: 10.6608\n",
      "Epoch 659/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8745 - val_loss: 10.5080\n",
      "Epoch 660/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3114 - val_loss: 11.2469\n",
      "Epoch 661/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4360 - val_loss: 10.6016\n",
      "Epoch 662/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3591 - val_loss: 11.4524\n",
      "Epoch 663/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6979 - val_loss: 10.9375\n",
      "Epoch 664/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5413 - val_loss: 10.7740\n",
      "Epoch 665/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3932 - val_loss: 10.5835\n",
      "Epoch 666/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7023 - val_loss: 11.1833\n",
      "Epoch 667/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6543 - val_loss: 10.4684\n",
      "Epoch 668/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7741 - val_loss: 11.0785\n",
      "Epoch 669/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3128 - val_loss: 11.1520\n",
      "Epoch 670/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4380 - val_loss: 10.5287\n",
      "Epoch 671/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5668 - val_loss: 10.6787\n",
      "Epoch 672/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4551 - val_loss: 10.5991\n",
      "Epoch 673/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5117 - val_loss: 10.5642\n",
      "Epoch 674/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2652 - val_loss: 10.6398\n",
      "Epoch 675/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6623 - val_loss: 10.5376\n",
      "Epoch 676/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8513 - val_loss: 11.0401\n",
      "Epoch 677/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4818 - val_loss: 10.6251\n",
      "Epoch 678/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5711 - val_loss: 12.1048\n",
      "Epoch 679/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6629 - val_loss: 11.4350\n",
      "Epoch 680/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8383 - val_loss: 10.6818\n",
      "Epoch 681/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3674 - val_loss: 10.7574\n",
      "Epoch 682/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4245 - val_loss: 10.3685\n",
      "Epoch 683/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3436 - val_loss: 11.1892\n",
      "Epoch 684/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3484 - val_loss: 10.6303\n",
      "Epoch 685/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4969 - val_loss: 10.8531\n",
      "Epoch 686/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2719 - val_loss: 10.6237\n",
      "Epoch 687/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8069 - val_loss: 10.4192\n",
      "Epoch 688/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4154 - val_loss: 10.7943\n",
      "Epoch 689/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6112 - val_loss: 10.8106\n",
      "Epoch 690/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4156 - val_loss: 10.3465\n",
      "Epoch 691/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4716 - val_loss: 10.6390\n",
      "Epoch 692/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3095 - val_loss: 10.5177\n",
      "Epoch 693/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8685 - val_loss: 10.6715\n",
      "Epoch 694/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3593 - val_loss: 10.6462\n",
      "Epoch 695/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3614 - val_loss: 10.5474\n",
      "Epoch 696/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6289 - val_loss: 10.4733\n",
      "Epoch 697/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3313 - val_loss: 10.4802\n",
      "Epoch 698/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4598 - val_loss: 10.6465\n",
      "Epoch 699/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4206 - val_loss: 10.8719\n",
      "Epoch 700/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5568 - val_loss: 11.3108\n",
      "Epoch 701/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5209 - val_loss: 11.0793\n",
      "Epoch 702/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3906 - val_loss: 10.5469\n",
      "Epoch 703/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4058 - val_loss: 10.4917\n",
      "Epoch 704/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2842 - val_loss: 10.9963\n",
      "Epoch 705/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2890 - val_loss: 10.5641\n",
      "Epoch 706/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7728 - val_loss: 10.9978\n",
      "Epoch 707/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4384 - val_loss: 10.5108\n",
      "Epoch 708/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3242 - val_loss: 10.6835\n",
      "Epoch 709/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7909 - val_loss: 11.1270\n",
      "Epoch 710/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2456 - val_loss: 11.2148\n",
      "Epoch 711/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3991 - val_loss: 10.4169\n",
      "Epoch 712/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5924 - val_loss: 10.9926\n",
      "Epoch 713/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4628 - val_loss: 10.6294\n",
      "Epoch 714/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6872 - val_loss: 10.6491\n",
      "Epoch 715/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2855 - val_loss: 10.7532\n",
      "Epoch 716/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5622 - val_loss: 11.1831\n",
      "Epoch 717/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4502 - val_loss: 10.6224\n",
      "Epoch 718/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5861 - val_loss: 10.5042\n",
      "Epoch 719/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2507 - val_loss: 10.5046\n",
      "Epoch 720/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2938 - val_loss: 10.2674\n",
      "Epoch 721/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3707 - val_loss: 10.6723\n",
      "Epoch 722/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2902 - val_loss: 10.4931\n",
      "Epoch 723/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2127 - val_loss: 11.4637\n",
      "Epoch 724/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5070 - val_loss: 10.8279\n",
      "Epoch 725/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7135 - val_loss: 10.7987\n",
      "Epoch 726/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9360 - val_loss: 10.8858\n",
      "Epoch 727/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2384 - val_loss: 10.3202\n",
      "Epoch 728/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4262 - val_loss: 11.9092\n",
      "Epoch 729/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5006 - val_loss: 10.7638\n",
      "Epoch 730/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4445 - val_loss: 10.5325\n",
      "Epoch 731/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5137 - val_loss: 11.2434\n",
      "Epoch 732/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0988 - val_loss: 10.8613\n",
      "Epoch 733/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6318 - val_loss: 10.5853\n",
      "Epoch 734/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5433 - val_loss: 10.6668\n",
      "Epoch 735/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2533 - val_loss: 10.7476\n",
      "Epoch 736/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3017 - val_loss: 10.4897\n",
      "Epoch 737/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8078 - val_loss: 10.6663\n",
      "Epoch 738/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4404 - val_loss: 10.9528\n",
      "Epoch 739/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4117 - val_loss: 10.6270\n",
      "Epoch 740/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3665 - val_loss: 10.9795\n",
      "Epoch 741/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5456 - val_loss: 10.6048\n",
      "Epoch 742/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4985 - val_loss: 10.3374\n",
      "Epoch 743/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1672 - val_loss: 10.8713\n",
      "Epoch 744/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1725 - val_loss: 10.3520\n",
      "Epoch 745/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3445 - val_loss: 10.7121\n",
      "Epoch 746/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4636 - val_loss: 10.6641\n",
      "Epoch 747/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4469 - val_loss: 10.5091\n",
      "Epoch 748/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3674 - val_loss: 11.0633\n",
      "Epoch 749/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3609 - val_loss: 10.7348\n",
      "Epoch 750/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5113 - val_loss: 10.9300\n",
      "Epoch 751/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6186 - val_loss: 10.5215\n",
      "Epoch 752/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3610 - val_loss: 10.5344\n",
      "Epoch 753/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3274 - val_loss: 10.7412\n",
      "Epoch 754/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2604 - val_loss: 10.4574\n",
      "Epoch 755/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0397 - val_loss: 10.6322\n",
      "Epoch 756/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3120 - val_loss: 10.6770\n",
      "Epoch 757/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6141 - val_loss: 10.5293\n",
      "Epoch 758/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2218 - val_loss: 13.1612\n",
      "Epoch 759/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4510 - val_loss: 10.4515\n",
      "Epoch 760/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2434 - val_loss: 10.7229\n",
      "Epoch 761/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8128 - val_loss: 10.6806\n",
      "Epoch 762/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4789 - val_loss: 10.9494\n",
      "Epoch 763/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5312 - val_loss: 10.5522\n",
      "Epoch 764/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4887 - val_loss: 11.1633\n",
      "Epoch 765/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1884 - val_loss: 10.6418\n",
      "Epoch 766/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4398 - val_loss: 10.6889\n",
      "Epoch 767/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2951 - val_loss: 10.9332\n",
      "Epoch 768/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3875 - val_loss: 10.8117\n",
      "Epoch 769/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3441 - val_loss: 11.3489\n",
      "Epoch 770/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2052 - val_loss: 11.2153\n",
      "Epoch 771/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2543 - val_loss: 11.2055\n",
      "Epoch 772/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5982 - val_loss: 10.9627\n",
      "Epoch 773/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3962 - val_loss: 10.4381\n",
      "Epoch 774/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2981 - val_loss: 10.3736\n",
      "Epoch 775/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5678 - val_loss: 10.8639\n",
      "Epoch 776/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1412 - val_loss: 10.5149\n",
      "Epoch 777/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4985 - val_loss: 10.5549\n",
      "Epoch 778/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5228 - val_loss: 12.0297\n",
      "Epoch 779/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5174 - val_loss: 10.4656\n",
      "Epoch 780/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2908 - val_loss: 10.7672\n",
      "Epoch 781/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1400 - val_loss: 10.5175\n",
      "Epoch 782/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2999 - val_loss: 10.6849\n",
      "Epoch 783/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3957 - val_loss: 10.6090\n",
      "Epoch 784/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1601 - val_loss: 10.4161\n",
      "Epoch 785/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3777 - val_loss: 10.5161\n",
      "Epoch 786/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5670 - val_loss: 10.7848\n",
      "Epoch 787/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6477 - val_loss: 11.4470\n",
      "Epoch 788/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5793 - val_loss: 10.7091\n",
      "Epoch 789/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0969 - val_loss: 10.5354\n",
      "Epoch 790/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1420 - val_loss: 11.6494\n",
      "Epoch 791/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5603 - val_loss: 10.5416\n",
      "Epoch 792/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4874 - val_loss: 10.7018\n",
      "Epoch 793/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4285 - val_loss: 10.6622\n",
      "Epoch 794/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1753 - val_loss: 10.3654\n",
      "Epoch 795/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2166 - val_loss: 11.0389\n",
      "Epoch 796/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3371 - val_loss: 10.7986\n",
      "Epoch 797/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4364 - val_loss: 10.6774\n",
      "Epoch 798/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2198 - val_loss: 10.7580\n",
      "Epoch 799/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5451 - val_loss: 10.7901\n",
      "Epoch 800/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2899 - val_loss: 10.8774\n",
      "Epoch 801/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1593 - val_loss: 10.7945\n",
      "Epoch 802/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7080 - val_loss: 10.9121\n",
      "Epoch 803/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5204 - val_loss: 11.0620\n",
      "Epoch 804/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2293 - val_loss: 10.8179\n",
      "Epoch 805/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2366 - val_loss: 10.7426\n",
      "Epoch 806/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1705 - val_loss: 10.6659\n",
      "Epoch 807/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1851 - val_loss: 10.8701\n",
      "Epoch 808/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7337 - val_loss: 10.5517\n",
      "Epoch 809/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4404 - val_loss: 10.4486\n",
      "Epoch 810/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2475 - val_loss: 10.3180\n",
      "Epoch 811/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2752 - val_loss: 10.8661\n",
      "Epoch 812/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3017 - val_loss: 10.6384\n",
      "Epoch 813/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3228 - val_loss: 10.4995\n",
      "Epoch 814/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3511 - val_loss: 10.4729\n",
      "Epoch 815/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4815 - val_loss: 10.5839\n",
      "Epoch 816/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1271 - val_loss: 10.4204\n",
      "Epoch 817/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6831 - val_loss: 11.2554\n",
      "Epoch 818/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0380 - val_loss: 10.5169\n",
      "Epoch 819/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0673 - val_loss: 10.8403\n",
      "Epoch 820/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0696 - val_loss: 11.0671\n",
      "Epoch 821/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6451 - val_loss: 10.6270\n",
      "Epoch 822/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4615 - val_loss: 11.1501\n",
      "Epoch 823/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4286 - val_loss: 11.2996\n",
      "Epoch 824/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0330 - val_loss: 10.7141\n",
      "Epoch 825/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1694 - val_loss: 10.5942\n",
      "Epoch 826/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3850 - val_loss: 10.5598\n",
      "Epoch 827/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2713 - val_loss: 10.6292\n",
      "Epoch 828/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1323 - val_loss: 10.6061\n",
      "Epoch 829/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3240 - val_loss: 10.5795\n",
      "Epoch 830/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1378 - val_loss: 10.6321\n",
      "Epoch 831/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1518 - val_loss: 10.5004\n",
      "Epoch 832/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3077 - val_loss: 11.7382\n",
      "Epoch 833/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.8161 - val_loss: 10.8218\n",
      "Epoch 834/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1592 - val_loss: 10.7162\n",
      "Epoch 835/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0147 - val_loss: 10.5679\n",
      "Epoch 836/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0843 - val_loss: 10.9497\n",
      "Epoch 837/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1450 - val_loss: 12.1044\n",
      "Epoch 838/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5121 - val_loss: 10.8675\n",
      "Epoch 839/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1881 - val_loss: 12.2883\n",
      "Epoch 840/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4117 - val_loss: 10.9059\n",
      "Epoch 841/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4812 - val_loss: 10.9690\n",
      "Epoch 842/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1835 - val_loss: 10.6579\n",
      "Epoch 843/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3345 - val_loss: 10.6027\n",
      "Epoch 844/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2342 - val_loss: 10.7954\n",
      "Epoch 845/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1176 - val_loss: 10.5021\n",
      "Epoch 846/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0623 - val_loss: 10.9609\n",
      "Epoch 847/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1316 - val_loss: 10.9456\n",
      "Epoch 848/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1098 - val_loss: 10.9904\n",
      "Epoch 849/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3210 - val_loss: 10.6788\n",
      "Epoch 850/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1253 - val_loss: 10.8349\n",
      "Epoch 851/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1742 - val_loss: 10.8829\n",
      "Epoch 852/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0877 - val_loss: 10.6320\n",
      "Epoch 853/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1209 - val_loss: 10.5808\n",
      "Epoch 854/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1753 - val_loss: 10.8667\n",
      "Epoch 855/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1373 - val_loss: 10.8320\n",
      "Epoch 856/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6783 - val_loss: 10.7146\n",
      "Epoch 857/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3330 - val_loss: 11.1659\n",
      "Epoch 858/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4822 - val_loss: 10.5609\n",
      "Epoch 859/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0871 - val_loss: 10.5475\n",
      "Epoch 860/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2695 - val_loss: 12.5130\n",
      "Epoch 861/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5495 - val_loss: 11.6874\n",
      "Epoch 862/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1883 - val_loss: 10.7211\n",
      "Epoch 863/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0298 - val_loss: 10.7772\n",
      "Epoch 864/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2582 - val_loss: 10.7699\n",
      "Epoch 865/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3317 - val_loss: 10.6066\n",
      "Epoch 866/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0333 - val_loss: 10.4457\n",
      "Epoch 867/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2246 - val_loss: 11.4284\n",
      "Epoch 868/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3232 - val_loss: 10.6099\n",
      "Epoch 869/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1642 - val_loss: 10.6494\n",
      "Epoch 870/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3629 - val_loss: 10.7164\n",
      "Epoch 871/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2061 - val_loss: 10.6508\n",
      "Epoch 872/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2975 - val_loss: 10.8774\n",
      "Epoch 873/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9561 - val_loss: 10.6102\n",
      "Epoch 874/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1654 - val_loss: 10.8437\n",
      "Epoch 875/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0676 - val_loss: 10.6545\n",
      "Epoch 876/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1260 - val_loss: 10.7722\n",
      "Epoch 877/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2416 - val_loss: 10.9006\n",
      "Epoch 878/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2361 - val_loss: 10.7556\n",
      "Epoch 879/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1666 - val_loss: 10.8364\n",
      "Epoch 880/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4479 - val_loss: 10.7161\n",
      "Epoch 881/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0990 - val_loss: 10.7572\n",
      "Epoch 882/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1039 - val_loss: 10.8839\n",
      "Epoch 883/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3017 - val_loss: 10.9185\n",
      "Epoch 884/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2046 - val_loss: 10.7272\n",
      "Epoch 885/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0772 - val_loss: 10.6497\n",
      "Epoch 886/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1830 - val_loss: 10.8729\n",
      "Epoch 887/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3851 - val_loss: 11.0434\n",
      "Epoch 888/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3150 - val_loss: 11.1336\n",
      "Epoch 889/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2582 - val_loss: 10.7572\n",
      "Epoch 890/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2681 - val_loss: 10.7402\n",
      "Epoch 891/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0705 - val_loss: 10.8985\n",
      "Epoch 892/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0204 - val_loss: 10.8126\n",
      "Epoch 893/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1559 - val_loss: 10.6769\n",
      "Epoch 894/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1420 - val_loss: 10.7022\n",
      "Epoch 895/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1543 - val_loss: 10.6910\n",
      "Epoch 896/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1452 - val_loss: 10.9445\n",
      "Epoch 897/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0988 - val_loss: 10.9459\n",
      "Epoch 898/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1222 - val_loss: 10.6505\n",
      "Epoch 899/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3799 - val_loss: 11.0167\n",
      "Epoch 900/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4040 - val_loss: 10.9673\n",
      "Epoch 901/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5415 - val_loss: 10.6328\n",
      "Epoch 902/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1452 - val_loss: 10.7771\n",
      "Epoch 903/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4666 - val_loss: 11.0624\n",
      "Epoch 904/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1210 - val_loss: 10.6873\n",
      "Epoch 905/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0545 - val_loss: 11.2946\n",
      "Epoch 906/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2746 - val_loss: 11.0349\n",
      "Epoch 907/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1751 - val_loss: 10.5855\n",
      "Epoch 908/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9251 - val_loss: 10.4345\n",
      "Epoch 909/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1979 - val_loss: 10.4852\n",
      "Epoch 910/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8897 - val_loss: 10.7347\n",
      "Epoch 911/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3691 - val_loss: 10.9514\n",
      "Epoch 912/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0727 - val_loss: 10.7633\n",
      "Epoch 913/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9724 - val_loss: 10.5971\n",
      "Epoch 914/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6155 - val_loss: 10.9368\n",
      "Epoch 915/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9750 - val_loss: 10.7256\n",
      "Epoch 916/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0240 - val_loss: 10.7085\n",
      "Epoch 917/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0917 - val_loss: 10.9325\n",
      "Epoch 918/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1632 - val_loss: 11.6855\n",
      "Epoch 919/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4134 - val_loss: 10.7503\n",
      "Epoch 920/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2798 - val_loss: 10.6208\n",
      "Epoch 921/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9421 - val_loss: 10.6023\n",
      "Epoch 922/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9994 - val_loss: 11.0009\n",
      "Epoch 923/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1005 - val_loss: 10.8667\n",
      "Epoch 924/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2289 - val_loss: 10.6933\n",
      "Epoch 925/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9749 - val_loss: 10.5210\n",
      "Epoch 926/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2314 - val_loss: 10.7339\n",
      "Epoch 927/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2611 - val_loss: 10.9042\n",
      "Epoch 928/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1089 - val_loss: 10.8814\n",
      "Epoch 929/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2282 - val_loss: 11.0824\n",
      "Epoch 930/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3560 - val_loss: 11.1000\n",
      "Epoch 931/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3179 - val_loss: 10.7381\n",
      "Epoch 932/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0456 - val_loss: 10.6847\n",
      "Epoch 933/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0641 - val_loss: 10.4426\n",
      "Epoch 934/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3658 - val_loss: 11.0669\n",
      "Epoch 935/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9979 - val_loss: 10.5162\n",
      "Epoch 936/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0194 - val_loss: 10.5387\n",
      "Epoch 937/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0071 - val_loss: 10.5550\n",
      "Epoch 938/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1454 - val_loss: 10.6095\n",
      "Epoch 939/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9239 - val_loss: 10.5039\n",
      "Epoch 940/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0540 - val_loss: 11.0064\n",
      "Epoch 941/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1719 - val_loss: 10.7401\n",
      "Epoch 942/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1420 - val_loss: 10.8112\n",
      "Epoch 943/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0174 - val_loss: 10.8397\n",
      "Epoch 944/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3276 - val_loss: 11.1804\n",
      "Epoch 945/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0528 - val_loss: 10.7733\n",
      "Epoch 946/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0866 - val_loss: 10.7741\n",
      "Epoch 947/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0807 - val_loss: 10.7633\n",
      "Epoch 948/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3050 - val_loss: 10.7527\n",
      "Epoch 949/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9459 - val_loss: 10.7623\n",
      "Epoch 950/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2259 - val_loss: 10.6910\n",
      "Epoch 951/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4632 - val_loss: 11.1131\n",
      "Epoch 952/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1153 - val_loss: 10.4397\n",
      "Epoch 953/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1791 - val_loss: 10.8139\n",
      "Epoch 954/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9433 - val_loss: 10.6618\n",
      "Epoch 955/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8619 - val_loss: 10.7513\n",
      "Epoch 956/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3812 - val_loss: 12.1325\n",
      "Epoch 957/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1387 - val_loss: 10.8212\n",
      "Epoch 958/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1597 - val_loss: 11.0168\n",
      "Epoch 959/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9855 - val_loss: 10.9989\n",
      "Epoch 960/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2687 - val_loss: 10.9598\n",
      "Epoch 961/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9778 - val_loss: 10.8772\n",
      "Epoch 962/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9849 - val_loss: 10.6825\n",
      "Epoch 963/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0916 - val_loss: 10.6528\n",
      "Epoch 964/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3055 - val_loss: 10.9290\n",
      "Epoch 965/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9782 - val_loss: 10.8620\n",
      "Epoch 966/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0813 - val_loss: 10.9884\n",
      "Epoch 967/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8671 - val_loss: 10.8388\n",
      "Epoch 968/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1874 - val_loss: 10.7717\n",
      "Epoch 969/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2970 - val_loss: 10.6131\n",
      "Epoch 970/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1816 - val_loss: 10.9825\n",
      "Epoch 971/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2106 - val_loss: 11.2722\n",
      "Epoch 972/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9771 - val_loss: 10.6158\n",
      "Epoch 973/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2394 - val_loss: 10.7555\n",
      "Epoch 974/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1270 - val_loss: 10.6332\n",
      "Epoch 975/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3397 - val_loss: 11.0233\n",
      "Epoch 976/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0942 - val_loss: 10.5911\n",
      "Epoch 977/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9786 - val_loss: 10.7126\n",
      "Epoch 978/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9517 - val_loss: 10.7148\n",
      "Epoch 979/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2664 - val_loss: 11.0652\n",
      "Epoch 980/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3307 - val_loss: 10.9370\n",
      "Epoch 981/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3096 - val_loss: 10.5592\n",
      "Epoch 982/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1181 - val_loss: 10.7308\n",
      "Epoch 983/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0648 - val_loss: 10.7429\n",
      "Epoch 984/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9657 - val_loss: 10.7352\n",
      "Epoch 985/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2944 - val_loss: 11.1237\n",
      "Epoch 986/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0653 - val_loss: 11.1271\n",
      "Epoch 987/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0035 - val_loss: 10.6796\n",
      "Epoch 988/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8989 - val_loss: 10.7113\n",
      "Epoch 989/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0089 - val_loss: 11.7275\n",
      "Epoch 990/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0601 - val_loss: 10.9397\n",
      "Epoch 991/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9736 - val_loss: 11.1215\n",
      "Epoch 992/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5234 - val_loss: 10.4876\n",
      "Epoch 993/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9715 - val_loss: 11.3692\n",
      "Epoch 994/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2401 - val_loss: 10.7645\n",
      "Epoch 995/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8771 - val_loss: 10.5377\n",
      "Epoch 996/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8833 - val_loss: 10.7180\n",
      "Epoch 997/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0201 - val_loss: 10.9776\n",
      "Epoch 998/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8837 - val_loss: 10.7052\n",
      "Epoch 999/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1762 - val_loss: 10.5365\n",
      "Epoch 1000/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0145 - val_loss: 10.8002\n",
      "Epoch 1001/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9178 - val_loss: 10.7157\n",
      "Epoch 1002/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1993 - val_loss: 11.1300\n",
      "Epoch 1003/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0125 - val_loss: 10.7080\n",
      "Epoch 1004/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9108 - val_loss: 10.7364\n",
      "Epoch 1005/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3087 - val_loss: 10.6520\n",
      "Epoch 1006/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9293 - val_loss: 10.9036\n",
      "Epoch 1007/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9034 - val_loss: 10.5652\n",
      "Epoch 1008/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1934 - val_loss: 10.9665\n",
      "Epoch 1009/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0539 - val_loss: 10.7441\n",
      "Epoch 1010/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0243 - val_loss: 11.0550\n",
      "Epoch 1011/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1947 - val_loss: 10.5477\n",
      "Epoch 1012/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2440 - val_loss: 13.1458\n",
      "Epoch 1013/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9244 - val_loss: 10.6975\n",
      "Epoch 1014/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1680 - val_loss: 10.5877\n",
      "Epoch 1015/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0575 - val_loss: 10.6277\n",
      "Epoch 1016/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0717 - val_loss: 11.9296\n",
      "Epoch 1017/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9730 - val_loss: 10.5758\n",
      "Epoch 1018/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1088 - val_loss: 10.8924\n",
      "Epoch 1019/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0094 - val_loss: 10.9057\n",
      "Epoch 1020/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8988 - val_loss: 10.4171\n",
      "Epoch 1021/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0041 - val_loss: 10.6629\n",
      "Epoch 1022/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8925 - val_loss: 10.6015\n",
      "Epoch 1023/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9350 - val_loss: 10.7500\n",
      "Epoch 1024/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2422 - val_loss: 10.8516\n",
      "Epoch 1025/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0909 - val_loss: 10.7084\n",
      "Epoch 1026/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2576 - val_loss: 11.4049\n",
      "Epoch 1027/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4887 - val_loss: 11.0758\n",
      "Epoch 1028/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0741 - val_loss: 11.1381\n",
      "Epoch 1029/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9869 - val_loss: 10.4996\n",
      "Epoch 1030/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9779 - val_loss: 10.5747\n",
      "Epoch 1031/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2061 - val_loss: 10.9688\n",
      "Epoch 1032/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8432 - val_loss: 10.5716\n",
      "Epoch 1033/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0700 - val_loss: 10.8278\n",
      "Epoch 1034/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8903 - val_loss: 11.0676\n",
      "Epoch 1035/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1662 - val_loss: 11.0070\n",
      "Epoch 1036/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9544 - val_loss: 10.3814\n",
      "Epoch 1037/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0452 - val_loss: 10.6380\n",
      "Epoch 1038/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1560 - val_loss: 11.3517\n",
      "Epoch 1039/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9508 - val_loss: 11.0094\n",
      "Epoch 1040/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9243 - val_loss: 10.7693\n",
      "Epoch 1041/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9857 - val_loss: 11.3134\n",
      "Epoch 1042/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1370 - val_loss: 10.6405\n",
      "Epoch 1043/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2344 - val_loss: 11.0261\n",
      "Epoch 1044/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0129 - val_loss: 10.9241\n",
      "Epoch 1045/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1516 - val_loss: 10.6878\n",
      "Epoch 1046/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0378 - val_loss: 10.9601\n",
      "Epoch 1047/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2872 - val_loss: 10.8863\n",
      "Epoch 1048/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9054 - val_loss: 11.0058\n",
      "Epoch 1049/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8130 - val_loss: 10.7372\n",
      "Epoch 1050/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0269 - val_loss: 10.6848\n",
      "Epoch 1051/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0286 - val_loss: 11.0922\n",
      "Epoch 1052/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0413 - val_loss: 10.7971\n",
      "Epoch 1053/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9506 - val_loss: 10.7987\n",
      "Epoch 1054/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0277 - val_loss: 10.4513\n",
      "Epoch 1055/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8568 - val_loss: 10.7435\n",
      "Epoch 1056/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8511 - val_loss: 10.8184\n",
      "Epoch 1057/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2764 - val_loss: 10.5538\n",
      "Epoch 1058/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9792 - val_loss: 10.4538\n",
      "Epoch 1059/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9066 - val_loss: 10.5929\n",
      "Epoch 1060/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0239 - val_loss: 10.6890\n",
      "Epoch 1061/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0205 - val_loss: 10.7856\n",
      "Epoch 1062/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9653 - val_loss: 10.7171\n",
      "Epoch 1063/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2355 - val_loss: 11.4643\n",
      "Epoch 1064/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0383 - val_loss: 10.6355\n",
      "Epoch 1065/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2190 - val_loss: 10.7162\n",
      "Epoch 1066/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0290 - val_loss: 11.4221\n",
      "Epoch 1067/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9515 - val_loss: 10.8792\n",
      "Epoch 1068/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6142 - val_loss: 11.2886\n",
      "Epoch 1069/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4128 - val_loss: 11.5692\n",
      "Epoch 1070/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2620 - val_loss: 10.5107\n",
      "Epoch 1071/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1940 - val_loss: 10.8245\n",
      "Epoch 1072/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7891 - val_loss: 10.6957\n",
      "Epoch 1073/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9648 - val_loss: 11.6628\n",
      "Epoch 1074/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0567 - val_loss: 11.3642\n",
      "Epoch 1075/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9264 - val_loss: 10.8496\n",
      "Epoch 1076/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9931 - val_loss: 10.4827\n",
      "Epoch 1077/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0317 - val_loss: 10.8806\n",
      "Epoch 1078/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8243 - val_loss: 10.7489\n",
      "Epoch 1079/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8432 - val_loss: 10.9842\n",
      "Epoch 1080/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1980 - val_loss: 11.6996\n",
      "Epoch 1081/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8856 - val_loss: 11.0940\n",
      "Epoch 1082/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9181 - val_loss: 10.6502\n",
      "Epoch 1083/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8360 - val_loss: 10.7503\n",
      "Epoch 1084/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9358 - val_loss: 11.2847\n",
      "Epoch 1085/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1507 - val_loss: 11.4247\n",
      "Epoch 1086/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0954 - val_loss: 10.7000\n",
      "Epoch 1087/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1858 - val_loss: 10.9941\n",
      "Epoch 1088/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8811 - val_loss: 10.5786\n",
      "Epoch 1089/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1471 - val_loss: 11.1349\n",
      "Epoch 1090/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8888 - val_loss: 10.6414\n",
      "Epoch 1091/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8938 - val_loss: 10.7205\n",
      "Epoch 1092/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1258 - val_loss: 10.7283\n",
      "Epoch 1093/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8290 - val_loss: 10.6231\n",
      "Epoch 1094/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8897 - val_loss: 10.7325\n",
      "Epoch 1095/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8399 - val_loss: 11.2450\n",
      "Epoch 1096/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0653 - val_loss: 10.7231\n",
      "Epoch 1097/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0329 - val_loss: 10.7815\n",
      "Epoch 1098/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0935 - val_loss: 10.4890\n",
      "Epoch 1099/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8678 - val_loss: 10.6993\n",
      "Epoch 1100/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9102 - val_loss: 10.9297\n",
      "Epoch 1101/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8060 - val_loss: 10.7091\n",
      "Epoch 1102/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8523 - val_loss: 10.8216\n",
      "Epoch 1103/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1477 - val_loss: 11.2848\n",
      "Epoch 1104/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2519 - val_loss: 10.8948\n",
      "Epoch 1105/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9354 - val_loss: 10.5440\n",
      "Epoch 1106/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8424 - val_loss: 10.8114\n",
      "Epoch 1107/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1321 - val_loss: 11.4576\n",
      "Epoch 1108/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9107 - val_loss: 11.0924\n",
      "Epoch 1109/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0458 - val_loss: 10.8588\n",
      "Epoch 1110/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1211 - val_loss: 10.7411\n",
      "Epoch 1111/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2234 - val_loss: 11.2719\n",
      "Epoch 1112/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9611 - val_loss: 11.0074\n",
      "Epoch 1113/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8608 - val_loss: 10.4456\n",
      "Epoch 1114/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9017 - val_loss: 10.7670\n",
      "Epoch 1115/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9892 - val_loss: 10.5351\n",
      "Epoch 1116/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2826 - val_loss: 10.6233\n",
      "Epoch 1117/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2531 - val_loss: 10.7255\n",
      "Epoch 1118/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7907 - val_loss: 10.8305\n",
      "Epoch 1119/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8699 - val_loss: 10.8705\n",
      "Epoch 1120/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8942 - val_loss: 10.8391\n",
      "Epoch 1121/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9527 - val_loss: 10.6248\n",
      "Epoch 1122/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9765 - val_loss: 10.7626\n",
      "Epoch 1123/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1207 - val_loss: 11.1479\n",
      "Epoch 1124/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9715 - val_loss: 11.0293\n",
      "Epoch 1125/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7876 - val_loss: 10.8129\n",
      "Epoch 1126/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1516 - val_loss: 10.7141\n",
      "Epoch 1127/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9446 - val_loss: 10.9607\n",
      "Epoch 1128/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0121 - val_loss: 10.7044\n",
      "Epoch 1129/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8702 - val_loss: 11.0231\n",
      "Epoch 1130/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9002 - val_loss: 10.7255\n",
      "Epoch 1131/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0240 - val_loss: 11.0218\n",
      "Epoch 1132/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1085 - val_loss: 11.0596\n",
      "Epoch 1133/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0160 - val_loss: 10.7629\n",
      "Epoch 1134/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8377 - val_loss: 11.0222\n",
      "Epoch 1135/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1292 - val_loss: 10.8640\n",
      "Epoch 1136/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9842 - val_loss: 11.5814\n",
      "Epoch 1137/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8104 - val_loss: 10.9332\n",
      "Epoch 1138/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0329 - val_loss: 10.8398\n",
      "Epoch 1139/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0076 - val_loss: 10.8377\n",
      "Epoch 1140/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7395 - val_loss: 10.8104\n",
      "Epoch 1141/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9553 - val_loss: 11.2481\n",
      "Epoch 1142/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9207 - val_loss: 10.9702\n",
      "Epoch 1143/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8644 - val_loss: 10.7938\n",
      "Epoch 1144/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2071 - val_loss: 11.5459\n",
      "Epoch 1145/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9056 - val_loss: 10.7571\n",
      "Epoch 1146/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8566 - val_loss: 10.8776\n",
      "Epoch 1147/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2672 - val_loss: 10.7273\n",
      "Epoch 1148/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3260 - val_loss: 10.8464\n",
      "Epoch 1149/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0547 - val_loss: 10.8327\n",
      "Epoch 1150/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9792 - val_loss: 10.7915\n",
      "Epoch 1151/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7396 - val_loss: 10.8548\n",
      "Epoch 1152/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9179 - val_loss: 11.0030\n",
      "Epoch 1153/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9293 - val_loss: 10.8416\n",
      "Epoch 1154/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1157 - val_loss: 11.3587\n",
      "Epoch 1155/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9954 - val_loss: 10.8160\n",
      "Epoch 1156/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7332 - val_loss: 10.6530\n",
      "Epoch 1157/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7457 - val_loss: 10.8859\n",
      "Epoch 1158/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0326 - val_loss: 10.6332\n",
      "Epoch 1159/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9467 - val_loss: 10.9384\n",
      "Epoch 1160/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9201 - val_loss: 10.5567\n",
      "Epoch 1161/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9952 - val_loss: 11.3907\n",
      "Epoch 1162/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9204 - val_loss: 10.5683\n",
      "Epoch 1163/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8480 - val_loss: 11.1347\n",
      "Epoch 1164/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8976 - val_loss: 11.7383\n",
      "Epoch 1165/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1884 - val_loss: 10.8472\n",
      "Epoch 1166/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0841 - val_loss: 10.9915\n",
      "Epoch 1167/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7477 - val_loss: 10.9205\n",
      "Epoch 1168/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9068 - val_loss: 10.7198\n",
      "Epoch 1169/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8144 - val_loss: 10.9008\n",
      "Epoch 1170/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1612 - val_loss: 10.7537\n",
      "Epoch 1171/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8286 - val_loss: 10.7828\n",
      "Epoch 1172/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9628 - val_loss: 10.9186\n",
      "Epoch 1173/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8680 - val_loss: 10.8053\n",
      "Epoch 1174/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1613 - val_loss: 10.5016\n",
      "Epoch 1175/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7739 - val_loss: 11.1136\n",
      "Epoch 1176/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7434 - val_loss: 10.8637\n",
      "Epoch 1177/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8865 - val_loss: 10.9652\n",
      "Epoch 1178/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1671 - val_loss: 11.0448\n",
      "Epoch 1179/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8606 - val_loss: 10.7955\n",
      "Epoch 1180/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9326 - val_loss: 10.4276\n",
      "Epoch 1181/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7820 - val_loss: 10.5440\n",
      "Epoch 1182/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0415 - val_loss: 10.7699\n",
      "Epoch 1183/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9483 - val_loss: 10.9042\n",
      "Epoch 1184/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2545 - val_loss: 11.0899\n",
      "Epoch 1185/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1217 - val_loss: 10.8280\n",
      "Epoch 1186/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8214 - val_loss: 10.7281\n",
      "Epoch 1187/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0064 - val_loss: 11.2816\n",
      "Epoch 1188/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7770 - val_loss: 10.8034\n",
      "Epoch 1189/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7686 - val_loss: 10.7210\n",
      "Epoch 1190/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8295 - val_loss: 11.1557\n",
      "Epoch 1191/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8196 - val_loss: 10.4508\n",
      "Epoch 1192/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7579 - val_loss: 10.7701\n",
      "Epoch 1193/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8315 - val_loss: 10.7435\n",
      "Epoch 1194/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1623 - val_loss: 10.6741\n",
      "Epoch 1195/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9226 - val_loss: 10.7814\n",
      "Epoch 1196/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1750 - val_loss: 10.7992\n",
      "Epoch 1197/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8079 - val_loss: 11.3514\n",
      "Epoch 1198/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7523 - val_loss: 11.0600\n",
      "Epoch 1199/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7838 - val_loss: 10.9522\n",
      "Epoch 1200/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9666 - val_loss: 10.8316\n",
      "Epoch 1201/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9385 - val_loss: 10.8952\n",
      "Epoch 1202/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8260 - val_loss: 11.0069\n",
      "Epoch 1203/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0026 - val_loss: 11.0641\n",
      "Epoch 1204/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8406 - val_loss: 10.6446\n",
      "Epoch 1205/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8569 - val_loss: 11.0193\n",
      "Epoch 1206/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1221 - val_loss: 10.5452\n",
      "Epoch 1207/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9337 - val_loss: 10.5685\n",
      "Epoch 1208/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9449 - val_loss: 10.5858\n",
      "Epoch 1209/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9636 - val_loss: 10.9116\n",
      "Epoch 1210/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7854 - val_loss: 10.6420\n",
      "Epoch 1211/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7178 - val_loss: 10.9725\n",
      "Epoch 1212/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7635 - val_loss: 10.6924\n",
      "Epoch 1213/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8183 - val_loss: 11.4330\n",
      "Epoch 1214/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8754 - val_loss: 10.8756\n",
      "Epoch 1215/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9306 - val_loss: 11.2878\n",
      "Epoch 1216/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2874 - val_loss: 12.6001\n",
      "Epoch 1217/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.3610 - val_loss: 10.7220\n",
      "Epoch 1218/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2363 - val_loss: 10.7580\n",
      "Epoch 1219/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6964 - val_loss: 10.5382\n",
      "Epoch 1220/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0349 - val_loss: 11.3269\n",
      "Epoch 1221/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9288 - val_loss: 10.5257\n",
      "Epoch 1222/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8105 - val_loss: 10.6192\n",
      "Epoch 1223/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6749 - val_loss: 10.5414\n",
      "Epoch 1224/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6746 - val_loss: 10.4424\n",
      "Epoch 1225/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7254 - val_loss: 10.7722\n",
      "Epoch 1226/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7274 - val_loss: 10.5878\n",
      "Epoch 1227/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6497 - val_loss: 10.7317\n",
      "Epoch 1228/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2021 - val_loss: 10.6411\n",
      "Epoch 1229/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9716 - val_loss: 10.7420\n",
      "Epoch 1230/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8651 - val_loss: 10.6623\n",
      "Epoch 1231/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7267 - val_loss: 10.7304\n",
      "Epoch 1232/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8221 - val_loss: 10.6570\n",
      "Epoch 1233/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8981 - val_loss: 10.7819\n",
      "Epoch 1234/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9206 - val_loss: 10.7320\n",
      "Epoch 1235/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7312 - val_loss: 10.6891\n",
      "Epoch 1236/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7546 - val_loss: 10.7205\n",
      "Epoch 1237/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9090 - val_loss: 10.7892\n",
      "Epoch 1238/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0988 - val_loss: 10.9335\n",
      "Epoch 1239/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0639 - val_loss: 11.6561\n",
      "Epoch 1240/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2295 - val_loss: 10.3681\n",
      "Epoch 1241/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8575 - val_loss: 11.0120\n",
      "Epoch 1242/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7463 - val_loss: 10.5987\n",
      "Epoch 1243/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6876 - val_loss: 11.0082\n",
      "Epoch 1244/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0018 - val_loss: 11.7249\n",
      "Epoch 1245/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1926 - val_loss: 10.5304\n",
      "Epoch 1246/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7193 - val_loss: 10.7014\n",
      "Epoch 1247/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9717 - val_loss: 10.7454\n",
      "Epoch 1248/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9181 - val_loss: 11.0617\n",
      "Epoch 1249/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8518 - val_loss: 10.7167\n",
      "Epoch 1250/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9478 - val_loss: 11.0544\n",
      "Epoch 1251/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7812 - val_loss: 10.4027\n",
      "Epoch 1252/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9082 - val_loss: 10.7364\n",
      "Epoch 1253/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8430 - val_loss: 10.7132\n",
      "Epoch 1254/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9286 - val_loss: 10.8077\n",
      "Epoch 1255/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7923 - val_loss: 10.7401\n",
      "Epoch 1256/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8197 - val_loss: 10.7669\n",
      "Epoch 1257/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8705 - val_loss: 10.6190\n",
      "Epoch 1258/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1280 - val_loss: 11.1903\n",
      "Epoch 1259/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9267 - val_loss: 10.6118\n",
      "Epoch 1260/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7255 - val_loss: 10.8505\n",
      "Epoch 1261/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8308 - val_loss: 10.9171\n",
      "Epoch 1262/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1349 - val_loss: 11.1980\n",
      "Epoch 1263/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8546 - val_loss: 10.5188\n",
      "Epoch 1264/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8039 - val_loss: 11.0797\n",
      "Epoch 1265/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0079 - val_loss: 10.9033\n",
      "Epoch 1266/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6971 - val_loss: 10.8628\n",
      "Epoch 1267/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9637 - val_loss: 10.7079\n",
      "Epoch 1268/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8959 - val_loss: 10.7988\n",
      "Epoch 1269/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8524 - val_loss: 10.5872\n",
      "Epoch 1270/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6811 - val_loss: 10.5882\n",
      "Epoch 1271/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7152 - val_loss: 10.5416\n",
      "Epoch 1272/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8004 - val_loss: 11.0015\n",
      "Epoch 1273/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7940 - val_loss: 10.7959\n",
      "Epoch 1274/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8373 - val_loss: 10.7376\n",
      "Epoch 1275/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4455 - val_loss: 11.2793\n",
      "Epoch 1276/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9217 - val_loss: 10.5804\n",
      "Epoch 1277/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6715 - val_loss: 10.6424\n",
      "Epoch 1278/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0671 - val_loss: 10.7095\n",
      "Epoch 1279/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9332 - val_loss: 11.1006\n",
      "Epoch 1280/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8426 - val_loss: 10.7733\n",
      "Epoch 1281/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7417 - val_loss: 10.6629\n",
      "Epoch 1282/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6961 - val_loss: 11.1082\n",
      "Epoch 1283/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8649 - val_loss: 10.8406\n",
      "Epoch 1284/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1701 - val_loss: 10.6735\n",
      "Epoch 1285/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7233 - val_loss: 10.6606\n",
      "Epoch 1286/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7627 - val_loss: 11.1147\n",
      "Epoch 1287/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8996 - val_loss: 11.1361\n",
      "Epoch 1288/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8983 - val_loss: 10.8911\n",
      "Epoch 1289/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9008 - val_loss: 11.0538\n",
      "Epoch 1290/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1276 - val_loss: 10.9291\n",
      "Epoch 1291/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8663 - val_loss: 11.2056\n",
      "Epoch 1292/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9688 - val_loss: 10.7222\n",
      "Epoch 1293/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8455 - val_loss: 10.6878\n",
      "Epoch 1294/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7888 - val_loss: 11.2357\n",
      "Epoch 1295/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.5306 - val_loss: 10.9886\n",
      "Epoch 1296/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0273 - val_loss: 11.4040\n",
      "Epoch 1297/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8581 - val_loss: 10.8063\n",
      "Epoch 1298/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8149 - val_loss: 10.5476\n",
      "Epoch 1299/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7799 - val_loss: 10.6005\n",
      "Epoch 1300/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6724 - val_loss: 10.5102\n",
      "Epoch 1301/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7227 - val_loss: 10.8915\n",
      "Epoch 1302/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7692 - val_loss: 10.7430\n",
      "Epoch 1303/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6943 - val_loss: 10.7482\n",
      "Epoch 1304/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7997 - val_loss: 10.7060\n",
      "Epoch 1305/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6639 - val_loss: 10.5432\n",
      "Epoch 1306/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7944 - val_loss: 10.7575\n",
      "Epoch 1307/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7754 - val_loss: 10.9503\n",
      "Epoch 1308/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8584 - val_loss: 10.7096\n",
      "Epoch 1309/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9380 - val_loss: 10.6439\n",
      "Epoch 1310/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8369 - val_loss: 10.5815\n",
      "Epoch 1311/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7630 - val_loss: 10.6457\n",
      "Epoch 1312/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8641 - val_loss: 11.1526\n",
      "Epoch 1313/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7473 - val_loss: 11.2774\n",
      "Epoch 1314/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9914 - val_loss: 10.7772\n",
      "Epoch 1315/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8171 - val_loss: 10.7924\n",
      "Epoch 1316/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8265 - val_loss: 10.8518\n",
      "Epoch 1317/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0823 - val_loss: 10.8461\n",
      "Epoch 1318/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7034 - val_loss: 10.6952\n",
      "Epoch 1319/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8819 - val_loss: 10.7570\n",
      "Epoch 1320/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7945 - val_loss: 10.6565\n",
      "Epoch 1321/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9779 - val_loss: 10.6684\n",
      "Epoch 1322/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9356 - val_loss: 10.7757\n",
      "Epoch 1323/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7211 - val_loss: 10.9399\n",
      "Epoch 1324/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9179 - val_loss: 10.9558\n",
      "Epoch 1325/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8173 - val_loss: 10.5395\n",
      "Epoch 1326/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9211 - val_loss: 10.7878\n",
      "Epoch 1327/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8339 - val_loss: 10.9409\n",
      "Epoch 1328/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7224 - val_loss: 11.4034\n",
      "Epoch 1329/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1128 - val_loss: 12.0686\n",
      "Epoch 1330/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8398 - val_loss: 10.8821\n",
      "Epoch 1331/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7686 - val_loss: 11.4122\n",
      "Epoch 1332/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8161 - val_loss: 10.8063\n",
      "Epoch 1333/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8287 - val_loss: 10.7315\n",
      "Epoch 1334/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9787 - val_loss: 10.8100\n",
      "Epoch 1335/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1052 - val_loss: 11.1748\n",
      "Epoch 1336/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8297 - val_loss: 10.6159\n",
      "Epoch 1337/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9520 - val_loss: 11.4106\n",
      "Epoch 1338/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8795 - val_loss: 10.6464\n",
      "Epoch 1339/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6586 - val_loss: 10.7969\n",
      "Epoch 1340/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8456 - val_loss: 10.5935\n",
      "Epoch 1341/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1637 - val_loss: 12.3198\n",
      "Epoch 1342/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7228 - val_loss: 11.6651\n",
      "Epoch 1343/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7002 - val_loss: 10.5234\n",
      "Epoch 1344/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8934 - val_loss: 11.3533\n",
      "Epoch 1345/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9303 - val_loss: 10.6751\n",
      "Epoch 1346/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8309 - val_loss: 10.7269\n",
      "Epoch 1347/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8641 - val_loss: 10.8852\n",
      "Epoch 1348/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8664 - val_loss: 10.8079\n",
      "Epoch 1349/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7772 - val_loss: 10.6852\n",
      "Epoch 1350/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8918 - val_loss: 10.8729\n",
      "Epoch 1351/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6178 - val_loss: 10.9199\n",
      "Epoch 1352/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9795 - val_loss: 10.7974\n",
      "Epoch 1353/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6948 - val_loss: 10.7780\n",
      "Epoch 1354/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6567 - val_loss: 10.9012\n",
      "Epoch 1355/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8364 - val_loss: 10.9126\n",
      "Epoch 1356/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8758 - val_loss: 10.8588\n",
      "Epoch 1357/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7316 - val_loss: 10.8372\n",
      "Epoch 1358/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7485 - val_loss: 10.8440\n",
      "Epoch 1359/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8201 - val_loss: 10.8741\n",
      "Epoch 1360/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7281 - val_loss: 10.8566\n",
      "Epoch 1361/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8319 - val_loss: 11.4254\n",
      "Epoch 1362/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0367 - val_loss: 11.4284\n",
      "Epoch 1363/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7800 - val_loss: 11.4615\n",
      "Epoch 1364/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9113 - val_loss: 10.7173\n",
      "Epoch 1365/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7298 - val_loss: 10.6388\n",
      "Epoch 1366/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8516 - val_loss: 11.0151\n",
      "Epoch 1367/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9301 - val_loss: 10.6804\n",
      "Epoch 1368/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6438 - val_loss: 10.7847\n",
      "Epoch 1369/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9258 - val_loss: 11.5048\n",
      "Epoch 1370/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8633 - val_loss: 10.8365\n",
      "Epoch 1371/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7982 - val_loss: 10.8248\n",
      "Epoch 1372/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8042 - val_loss: 10.9110\n",
      "Epoch 1373/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8835 - val_loss: 10.8368\n",
      "Epoch 1374/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9269 - val_loss: 10.8362\n",
      "Epoch 1375/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8977 - val_loss: 10.9316\n",
      "Epoch 1376/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6565 - val_loss: 10.7714\n",
      "Epoch 1377/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8638 - val_loss: 10.5745\n",
      "Epoch 1378/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7740 - val_loss: 11.1451\n",
      "Epoch 1379/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9489 - val_loss: 10.6995\n",
      "Epoch 1380/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0317 - val_loss: 10.6194\n",
      "Epoch 1381/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6978 - val_loss: 10.8343\n",
      "Epoch 1382/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8838 - val_loss: 10.7060\n",
      "Epoch 1383/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0460 - val_loss: 10.7890\n",
      "Epoch 1384/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6792 - val_loss: 10.6123\n",
      "Epoch 1385/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5738 - val_loss: 10.6997\n",
      "Epoch 1386/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6812 - val_loss: 10.6716\n",
      "Epoch 1387/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6243 - val_loss: 10.5461\n",
      "Epoch 1388/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7665 - val_loss: 11.0277\n",
      "Epoch 1389/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6786 - val_loss: 11.2523\n",
      "Epoch 1390/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9328 - val_loss: 11.3273\n",
      "Epoch 1391/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9417 - val_loss: 10.9266\n",
      "Epoch 1392/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7582 - val_loss: 11.1615\n",
      "Epoch 1393/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9596 - val_loss: 11.0516\n",
      "Epoch 1394/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7680 - val_loss: 11.1181\n",
      "Epoch 1395/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8896 - val_loss: 10.7512\n",
      "Epoch 1396/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7082 - val_loss: 10.7124\n",
      "Epoch 1397/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7170 - val_loss: 10.7799\n",
      "Epoch 1398/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7535 - val_loss: 10.9822\n",
      "Epoch 1399/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7578 - val_loss: 10.9000\n",
      "Epoch 1400/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8855 - val_loss: 10.6965\n",
      "Epoch 1401/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7022 - val_loss: 10.6469\n",
      "Epoch 1402/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6973 - val_loss: 11.0442\n",
      "Epoch 1403/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1875 - val_loss: 10.6841\n",
      "Epoch 1404/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8450 - val_loss: 10.9829\n",
      "Epoch 1405/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8037 - val_loss: 10.8242\n",
      "Epoch 1406/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8726 - val_loss: 10.7204\n",
      "Epoch 1407/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7626 - val_loss: 10.7301\n",
      "Epoch 1408/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7309 - val_loss: 10.7112\n",
      "Epoch 1409/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0395 - val_loss: 10.5633\n",
      "Epoch 1410/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9656 - val_loss: 10.8572\n",
      "Epoch 1411/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8603 - val_loss: 10.6714\n",
      "Epoch 1412/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6671 - val_loss: 10.5606\n",
      "Epoch 1413/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6472 - val_loss: 10.5485\n",
      "Epoch 1414/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6410 - val_loss: 10.8653\n",
      "Epoch 1415/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8040 - val_loss: 10.7658\n",
      "Epoch 1416/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7152 - val_loss: 11.0495\n",
      "Epoch 1417/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8660 - val_loss: 11.5399\n",
      "Epoch 1418/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7811 - val_loss: 10.6621\n",
      "Epoch 1419/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0037 - val_loss: 10.9741\n",
      "Epoch 1420/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8253 - val_loss: 10.8036\n",
      "Epoch 1421/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6470 - val_loss: 10.8284\n",
      "Epoch 1422/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7180 - val_loss: 11.3197\n",
      "Epoch 1423/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9574 - val_loss: 10.8081\n",
      "Epoch 1424/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7088 - val_loss: 10.8821\n",
      "Epoch 1425/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8908 - val_loss: 11.0290\n",
      "Epoch 1426/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8239 - val_loss: 10.8551\n",
      "Epoch 1427/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7536 - val_loss: 11.0031\n",
      "Epoch 1428/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9370 - val_loss: 11.7114\n",
      "Epoch 1429/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7105 - val_loss: 10.8144\n",
      "Epoch 1430/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6997 - val_loss: 10.5946\n",
      "Epoch 1431/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8149 - val_loss: 10.8508\n",
      "Epoch 1432/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8247 - val_loss: 10.9377\n",
      "Epoch 1433/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0575 - val_loss: 11.6477\n",
      "Epoch 1434/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8201 - val_loss: 10.8610\n",
      "Epoch 1435/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6716 - val_loss: 11.0313\n",
      "Epoch 1436/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8852 - val_loss: 10.8171\n",
      "Epoch 1437/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5661 - val_loss: 10.6661\n",
      "Epoch 1438/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8788 - val_loss: 10.8215\n",
      "Epoch 1439/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7160 - val_loss: 10.8461\n",
      "Epoch 1440/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8431 - val_loss: 11.0781\n",
      "Epoch 1441/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8613 - val_loss: 11.1663\n",
      "Epoch 1442/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7878 - val_loss: 11.0673\n",
      "Epoch 1443/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6860 - val_loss: 10.8268\n",
      "Epoch 1444/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6109 - val_loss: 10.8481\n",
      "Epoch 1445/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9827 - val_loss: 11.0147\n",
      "Epoch 1446/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7246 - val_loss: 10.8440\n",
      "Epoch 1447/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8783 - val_loss: 10.7964\n",
      "Epoch 1448/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8173 - val_loss: 10.5956\n",
      "Epoch 1449/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9008 - val_loss: 11.1032\n",
      "Epoch 1450/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8247 - val_loss: 10.7268\n",
      "Epoch 1451/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8460 - val_loss: 10.8529\n",
      "Epoch 1452/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7105 - val_loss: 10.7002\n",
      "Epoch 1453/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6963 - val_loss: 10.7976\n",
      "Epoch 1454/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6738 - val_loss: 10.7244\n",
      "Epoch 1455/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2360 - val_loss: 11.3103\n",
      "Epoch 1456/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0588 - val_loss: 10.9002\n",
      "Epoch 1457/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8258 - val_loss: 11.5539\n",
      "Epoch 1458/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7956 - val_loss: 10.7512\n",
      "Epoch 1459/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6134 - val_loss: 11.0629\n",
      "Epoch 1460/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0036 - val_loss: 11.0603\n",
      "Epoch 1461/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6324 - val_loss: 10.7838\n",
      "Epoch 1462/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6908 - val_loss: 10.7967\n",
      "Epoch 1463/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6619 - val_loss: 11.1194\n",
      "Epoch 1464/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9469 - val_loss: 11.0013\n",
      "Epoch 1465/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8780 - val_loss: 11.0444\n",
      "Epoch 1466/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7513 - val_loss: 11.1371\n",
      "Epoch 1467/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8097 - val_loss: 10.8539\n",
      "Epoch 1468/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7241 - val_loss: 10.7662\n",
      "Epoch 1469/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9979 - val_loss: 11.0856\n",
      "Epoch 1470/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6575 - val_loss: 11.1682\n",
      "Epoch 1471/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7655 - val_loss: 10.7024\n",
      "Epoch 1472/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7176 - val_loss: 10.7204\n",
      "Epoch 1473/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6763 - val_loss: 10.9938\n",
      "Epoch 1474/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7674 - val_loss: 11.1603\n",
      "Epoch 1475/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7848 - val_loss: 10.7685\n",
      "Epoch 1476/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6177 - val_loss: 10.6755\n",
      "Epoch 1477/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7899 - val_loss: 10.9832\n",
      "Epoch 1478/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8497 - val_loss: 10.6942\n",
      "Epoch 1479/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7848 - val_loss: 11.1124\n",
      "Epoch 1480/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6837 - val_loss: 11.2341\n",
      "Epoch 1481/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7289 - val_loss: 10.7478\n",
      "Epoch 1482/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6828 - val_loss: 10.8730\n",
      "Epoch 1483/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1996 - val_loss: 11.0865\n",
      "Epoch 1484/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9427 - val_loss: 10.7973\n",
      "Epoch 1485/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8674 - val_loss: 11.0015\n",
      "Epoch 1486/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5915 - val_loss: 10.9403\n",
      "Epoch 1487/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6143 - val_loss: 10.7427\n",
      "Epoch 1488/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8443 - val_loss: 10.8218\n",
      "Epoch 1489/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8503 - val_loss: 11.7839\n",
      "Epoch 1490/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7321 - val_loss: 10.9541\n",
      "Epoch 1491/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6127 - val_loss: 10.7656\n",
      "Epoch 1492/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2108 - val_loss: 11.5261\n",
      "Epoch 1493/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0340 - val_loss: 11.1601\n",
      "Epoch 1494/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6592 - val_loss: 10.6844\n",
      "Epoch 1495/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6207 - val_loss: 10.5456\n",
      "Epoch 1496/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7348 - val_loss: 10.9353\n",
      "Epoch 1497/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6128 - val_loss: 10.8141\n",
      "Epoch 1498/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8626 - val_loss: 10.9403\n",
      "Epoch 1499/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7081 - val_loss: 10.7003\n",
      "Epoch 1500/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5662 - val_loss: 10.6590\n",
      "Epoch 1501/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6827 - val_loss: 11.0922\n",
      "Epoch 1502/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6040 - val_loss: 10.6479\n",
      "Epoch 1503/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7269 - val_loss: 10.8903\n",
      "Epoch 1504/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8004 - val_loss: 10.7704\n",
      "Epoch 1505/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6054 - val_loss: 11.0357\n",
      "Epoch 1506/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6541 - val_loss: 10.7351\n",
      "Epoch 1507/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6730 - val_loss: 10.8291\n",
      "Epoch 1508/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0645 - val_loss: 11.1415\n",
      "Epoch 1509/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7564 - val_loss: 11.0894\n",
      "Epoch 1510/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8532 - val_loss: 10.9212\n",
      "Epoch 1511/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1611 - val_loss: 11.0412\n",
      "Epoch 1512/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8640 - val_loss: 10.7728\n",
      "Epoch 1513/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7860 - val_loss: 10.5282\n",
      "Epoch 1514/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5710 - val_loss: 11.2401\n",
      "Epoch 1515/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6878 - val_loss: 10.7721\n",
      "Epoch 1516/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8121 - val_loss: 10.8609\n",
      "Epoch 1517/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6982 - val_loss: 10.8478\n",
      "Epoch 1518/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0117 - val_loss: 11.3705\n",
      "Epoch 1519/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7048 - val_loss: 11.8797\n",
      "Epoch 1520/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6762 - val_loss: 11.4731\n",
      "Epoch 1521/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8064 - val_loss: 10.8874\n",
      "Epoch 1522/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9129 - val_loss: 10.9546\n",
      "Epoch 1523/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7195 - val_loss: 10.6684\n",
      "Epoch 1524/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7704 - val_loss: 10.8818\n",
      "Epoch 1525/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7448 - val_loss: 10.8099\n",
      "Epoch 1526/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5804 - val_loss: 11.0630\n",
      "Epoch 1527/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0283 - val_loss: 10.7249\n",
      "Epoch 1528/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5618 - val_loss: 10.6613\n",
      "Epoch 1529/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8396 - val_loss: 10.7783\n",
      "Epoch 1530/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6710 - val_loss: 10.8409\n",
      "Epoch 1531/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7553 - val_loss: 11.5235\n",
      "Epoch 1532/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7741 - val_loss: 10.9629\n",
      "Epoch 1533/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7427 - val_loss: 10.6538\n",
      "Epoch 1534/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6490 - val_loss: 10.7574\n",
      "Epoch 1535/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9609 - val_loss: 11.7071\n",
      "Epoch 1536/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5946 - val_loss: 10.8720\n",
      "Epoch 1537/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7455 - val_loss: 11.1707\n",
      "Epoch 1538/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8730 - val_loss: 10.7417\n",
      "Epoch 1539/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6942 - val_loss: 11.1353\n",
      "Epoch 1540/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8304 - val_loss: 10.8420\n",
      "Epoch 1541/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7890 - val_loss: 10.8801\n",
      "Epoch 1542/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6214 - val_loss: 10.8232\n",
      "Epoch 1543/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7016 - val_loss: 10.9656\n",
      "Epoch 1544/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7751 - val_loss: 10.9155\n",
      "Epoch 1545/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7077 - val_loss: 10.7341\n",
      "Epoch 1546/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6670 - val_loss: 10.8006\n",
      "Epoch 1547/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6328 - val_loss: 10.8464\n",
      "Epoch 1548/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8837 - val_loss: 10.6578\n",
      "Epoch 1549/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5376 - val_loss: 11.1830\n",
      "Epoch 1550/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7482 - val_loss: 10.9615\n",
      "Epoch 1551/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6843 - val_loss: 10.9673\n",
      "Epoch 1552/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6656 - val_loss: 10.8565\n",
      "Epoch 1553/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7248 - val_loss: 10.8092\n",
      "Epoch 1554/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6847 - val_loss: 11.0624\n",
      "Epoch 1555/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9905 - val_loss: 11.5224\n",
      "Epoch 1556/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8196 - val_loss: 10.8112\n",
      "Epoch 1557/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6637 - val_loss: 10.8775\n",
      "Epoch 1558/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9262 - val_loss: 10.7284\n",
      "Epoch 1559/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5473 - val_loss: 10.7275\n",
      "Epoch 1560/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7342 - val_loss: 10.8441\n",
      "Epoch 1561/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8921 - val_loss: 11.2902\n",
      "Epoch 1562/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8103 - val_loss: 10.8699\n",
      "Epoch 1563/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8290 - val_loss: 11.1519\n",
      "Epoch 1564/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7882 - val_loss: 10.6294\n",
      "Epoch 1565/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6928 - val_loss: 10.6967\n",
      "Epoch 1566/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8137 - val_loss: 11.0053\n",
      "Epoch 1567/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5006 - val_loss: 10.7159\n",
      "Epoch 1568/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6354 - val_loss: 10.8150\n",
      "Epoch 1569/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8222 - val_loss: 10.8978\n",
      "Epoch 1570/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5996 - val_loss: 10.7367\n",
      "Epoch 1571/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7450 - val_loss: 11.0219\n",
      "Epoch 1572/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6951 - val_loss: 11.1568\n",
      "Epoch 1573/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7833 - val_loss: 11.1627\n",
      "Epoch 1574/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8317 - val_loss: 11.4101\n",
      "Epoch 1575/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7488 - val_loss: 11.4277\n",
      "Epoch 1576/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0706 - val_loss: 11.0775\n",
      "Epoch 1577/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7981 - val_loss: 10.9145\n",
      "Epoch 1578/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6463 - val_loss: 10.9260\n",
      "Epoch 1579/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8162 - val_loss: 10.9966\n",
      "Epoch 1580/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6984 - val_loss: 10.7625\n",
      "Epoch 1581/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7421 - val_loss: 11.2989\n",
      "Epoch 1582/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7845 - val_loss: 10.8291\n",
      "Epoch 1583/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5495 - val_loss: 10.9666\n",
      "Epoch 1584/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6121 - val_loss: 10.6317\n",
      "Epoch 1585/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6826 - val_loss: 10.8574\n",
      "Epoch 1586/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7132 - val_loss: 11.3097\n",
      "Epoch 1587/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7942 - val_loss: 11.4532\n",
      "Epoch 1588/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1204 - val_loss: 11.3734\n",
      "Epoch 1589/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8643 - val_loss: 10.6566\n",
      "Epoch 1590/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5581 - val_loss: 10.6691\n",
      "Epoch 1591/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6697 - val_loss: 10.6382\n",
      "Epoch 1592/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7662 - val_loss: 10.6534\n",
      "Epoch 1593/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6416 - val_loss: 10.6982\n",
      "Epoch 1594/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5772 - val_loss: 10.9468\n",
      "Epoch 1595/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9436 - val_loss: 10.7026\n",
      "Epoch 1596/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7915 - val_loss: 10.8210\n",
      "Epoch 1597/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5366 - val_loss: 10.6399\n",
      "Epoch 1598/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4339 - val_loss: 11.0132\n",
      "Epoch 1599/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0233 - val_loss: 10.8681\n",
      "Epoch 1600/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6502 - val_loss: 10.8630\n",
      "Epoch 1601/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8082 - val_loss: 10.8747\n",
      "Epoch 1602/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5251 - val_loss: 10.7531\n",
      "Epoch 1603/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8181 - val_loss: 10.6623\n",
      "Epoch 1604/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6013 - val_loss: 10.7525\n",
      "Epoch 1605/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5826 - val_loss: 10.9062\n",
      "Epoch 1606/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7684 - val_loss: 10.7763\n",
      "Epoch 1607/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7540 - val_loss: 10.7728\n",
      "Epoch 1608/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7765 - val_loss: 10.7968\n",
      "Epoch 1609/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5200 - val_loss: 10.7465\n",
      "Epoch 1610/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5638 - val_loss: 10.8325\n",
      "Epoch 1611/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7546 - val_loss: 10.7706\n",
      "Epoch 1612/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7820 - val_loss: 11.0480\n",
      "Epoch 1613/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7073 - val_loss: 10.9816\n",
      "Epoch 1614/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5607 - val_loss: 10.7837\n",
      "Epoch 1615/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7474 - val_loss: 10.8801\n",
      "Epoch 1616/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7214 - val_loss: 11.1706\n",
      "Epoch 1617/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6874 - val_loss: 11.3363\n",
      "Epoch 1618/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6496 - val_loss: 10.8984\n",
      "Epoch 1619/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5821 - val_loss: 11.0210\n",
      "Epoch 1620/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6254 - val_loss: 11.0571\n",
      "Epoch 1621/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7355 - val_loss: 10.7229\n",
      "Epoch 1622/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6738 - val_loss: 10.9138\n",
      "Epoch 1623/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8072 - val_loss: 10.8685\n",
      "Epoch 1624/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7202 - val_loss: 11.3345\n",
      "Epoch 1625/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7936 - val_loss: 10.8776\n",
      "Epoch 1626/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7342 - val_loss: 10.8508\n",
      "Epoch 1627/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8956 - val_loss: 10.9624\n",
      "Epoch 1628/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5763 - val_loss: 10.7409\n",
      "Epoch 1629/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6281 - val_loss: 10.7096\n",
      "Epoch 1630/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5798 - val_loss: 10.9402\n",
      "Epoch 1631/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7379 - val_loss: 10.7822\n",
      "Epoch 1632/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5630 - val_loss: 10.8204\n",
      "Epoch 1633/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6084 - val_loss: 10.9214\n",
      "Epoch 1634/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8577 - val_loss: 11.1668\n",
      "Epoch 1635/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0774 - val_loss: 11.0380\n",
      "Epoch 1636/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7650 - val_loss: 10.9876\n",
      "Epoch 1637/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5535 - val_loss: 10.7547\n",
      "Epoch 1638/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5272 - val_loss: 10.6960\n",
      "Epoch 1639/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6398 - val_loss: 10.8761\n",
      "Epoch 1640/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8081 - val_loss: 10.9485\n",
      "Epoch 1641/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6280 - val_loss: 10.9480\n",
      "Epoch 1642/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6852 - val_loss: 10.9382\n",
      "Epoch 1643/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6114 - val_loss: 10.8603\n",
      "Epoch 1644/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7669 - val_loss: 11.0665\n",
      "Epoch 1645/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6826 - val_loss: 11.1654\n",
      "Epoch 1646/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8282 - val_loss: 10.9271\n",
      "Epoch 1647/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6272 - val_loss: 11.1005\n",
      "Epoch 1648/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6317 - val_loss: 11.0124\n",
      "Epoch 1649/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7982 - val_loss: 11.0556\n",
      "Epoch 1650/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6323 - val_loss: 10.9902\n",
      "Epoch 1651/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8177 - val_loss: 10.9298\n",
      "Epoch 1652/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8631 - val_loss: 12.1738\n",
      "Epoch 1653/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6917 - val_loss: 11.0152\n",
      "Epoch 1654/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5613 - val_loss: 11.0508\n",
      "Epoch 1655/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7195 - val_loss: 11.5073\n",
      "Epoch 1656/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8491 - val_loss: 11.5522\n",
      "Epoch 1657/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9053 - val_loss: 10.8922\n",
      "Epoch 1658/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6737 - val_loss: 11.0103\n",
      "Epoch 1659/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6830 - val_loss: 10.8011\n",
      "Epoch 1660/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7310 - val_loss: 12.0236\n",
      "Epoch 1661/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0839 - val_loss: 10.9328\n",
      "Epoch 1662/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7921 - val_loss: 10.9411\n",
      "Epoch 1663/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6112 - val_loss: 10.8614\n",
      "Epoch 1664/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7120 - val_loss: 11.1066\n",
      "Epoch 1665/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5527 - val_loss: 10.8288\n",
      "Epoch 1666/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5023 - val_loss: 10.9243\n",
      "Epoch 1667/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6900 - val_loss: 11.0462\n",
      "Epoch 1668/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8311 - val_loss: 11.0241\n",
      "Epoch 1669/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5878 - val_loss: 11.1993\n",
      "Epoch 1670/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7974 - val_loss: 10.7650\n",
      "Epoch 1671/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6357 - val_loss: 10.9173\n",
      "Epoch 1672/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7535 - val_loss: 10.7535\n",
      "Epoch 1673/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6595 - val_loss: 11.0885\n",
      "Epoch 1674/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6477 - val_loss: 11.1021\n",
      "Epoch 1675/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6581 - val_loss: 11.1102\n",
      "Epoch 1676/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7330 - val_loss: 10.9104\n",
      "Epoch 1677/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5920 - val_loss: 10.7821\n",
      "Epoch 1678/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7486 - val_loss: 10.8432\n",
      "Epoch 1679/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5823 - val_loss: 11.0659\n",
      "Epoch 1680/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5295 - val_loss: 10.8246\n",
      "Epoch 1681/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4658 - val_loss: 12.0272\n",
      "Epoch 1682/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1465 - val_loss: 10.9687\n",
      "Epoch 1683/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6905 - val_loss: 10.7964\n",
      "Epoch 1684/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5358 - val_loss: 10.8402\n",
      "Epoch 1685/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6804 - val_loss: 11.2472\n",
      "Epoch 1686/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7294 - val_loss: 10.6724\n",
      "Epoch 1687/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5751 - val_loss: 10.8049\n",
      "Epoch 1688/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8224 - val_loss: 10.8365\n",
      "Epoch 1689/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6753 - val_loss: 10.6174\n",
      "Epoch 1690/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5570 - val_loss: 11.0046\n",
      "Epoch 1691/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7354 - val_loss: 11.0156\n",
      "Epoch 1692/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6677 - val_loss: 10.8860\n",
      "Epoch 1693/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6578 - val_loss: 10.9850\n",
      "Epoch 1694/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6113 - val_loss: 10.9279\n",
      "Epoch 1695/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6692 - val_loss: 11.2246\n",
      "Epoch 1696/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6810 - val_loss: 11.9582\n",
      "Epoch 1697/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8441 - val_loss: 11.2066\n",
      "Epoch 1698/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8251 - val_loss: 10.8820\n",
      "Epoch 1699/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6770 - val_loss: 10.8581\n",
      "Epoch 1700/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5201 - val_loss: 10.8680\n",
      "Epoch 1701/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6655 - val_loss: 10.8007\n",
      "Epoch 1702/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9272 - val_loss: 11.5647\n",
      "Epoch 1703/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7156 - val_loss: 11.4047\n",
      "Epoch 1704/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7372 - val_loss: 11.2830\n",
      "Epoch 1705/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5336 - val_loss: 11.1338\n",
      "Epoch 1706/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.2240 - val_loss: 11.4931\n",
      "Epoch 1707/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9903 - val_loss: 10.9907\n",
      "Epoch 1708/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5769 - val_loss: 10.8470\n",
      "Epoch 1709/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4927 - val_loss: 10.7851\n",
      "Epoch 1710/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5912 - val_loss: 10.6156\n",
      "Epoch 1711/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5046 - val_loss: 10.6543\n",
      "Epoch 1712/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5874 - val_loss: 10.8830\n",
      "Epoch 1713/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6004 - val_loss: 10.8421\n",
      "Epoch 1714/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8507 - val_loss: 10.6505\n",
      "Epoch 1715/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4738 - val_loss: 10.8888\n",
      "Epoch 1716/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6341 - val_loss: 10.9606\n",
      "Epoch 1717/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9155 - val_loss: 10.8424\n",
      "Epoch 1718/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5511 - val_loss: 11.0655\n",
      "Epoch 1719/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5483 - val_loss: 10.8445\n",
      "Epoch 1720/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7288 - val_loss: 10.8482\n",
      "Epoch 1721/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6453 - val_loss: 10.7669\n",
      "Epoch 1722/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7292 - val_loss: 10.9180\n",
      "Epoch 1723/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7885 - val_loss: 10.7960\n",
      "Epoch 1724/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5996 - val_loss: 11.1178\n",
      "Epoch 1725/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5105 - val_loss: 10.7964\n",
      "Epoch 1726/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6190 - val_loss: 10.9625\n",
      "Epoch 1727/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5963 - val_loss: 10.8598\n",
      "Epoch 1728/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6885 - val_loss: 11.4017\n",
      "Epoch 1729/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1778 - val_loss: 11.0657\n",
      "Epoch 1730/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2124 - val_loss: 10.7623\n",
      "Epoch 1731/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8358 - val_loss: 11.1270\n",
      "Epoch 1732/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5378 - val_loss: 10.7888\n",
      "Epoch 1733/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7838 - val_loss: 10.7317\n",
      "Epoch 1734/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6140 - val_loss: 10.7035\n",
      "Epoch 1735/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5956 - val_loss: 11.4729\n",
      "Epoch 1736/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5811 - val_loss: 10.8414\n",
      "Epoch 1737/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7955 - val_loss: 11.3562\n",
      "Epoch 1738/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6355 - val_loss: 11.2259\n",
      "Epoch 1739/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6482 - val_loss: 10.7257\n",
      "Epoch 1740/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5689 - val_loss: 11.0210\n",
      "Epoch 1741/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6191 - val_loss: 10.9078\n",
      "Epoch 1742/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5605 - val_loss: 10.7964\n",
      "Epoch 1743/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6723 - val_loss: 10.8521\n",
      "Epoch 1744/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5424 - val_loss: 11.2247\n",
      "Epoch 1745/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6225 - val_loss: 10.8016\n",
      "Epoch 1746/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9955 - val_loss: 10.8319\n",
      "Epoch 1747/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8036 - val_loss: 10.6814\n",
      "Epoch 1748/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8315 - val_loss: 10.8912\n",
      "Epoch 1749/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6100 - val_loss: 11.1342\n",
      "Epoch 1750/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6220 - val_loss: 10.6911\n",
      "Epoch 1751/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8891 - val_loss: 10.8023\n",
      "Epoch 1752/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6545 - val_loss: 10.8349\n",
      "Epoch 1753/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6737 - val_loss: 10.6096\n",
      "Epoch 1754/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5144 - val_loss: 11.2419\n",
      "Epoch 1755/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7093 - val_loss: 10.7960\n",
      "Epoch 1756/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6508 - val_loss: 10.7933\n",
      "Epoch 1757/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6924 - val_loss: 10.9116\n",
      "Epoch 1758/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0762 - val_loss: 10.8974\n",
      "Epoch 1759/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7338 - val_loss: 10.7460\n",
      "Epoch 1760/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5419 - val_loss: 11.4500\n",
      "Epoch 1761/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5474 - val_loss: 10.9413\n",
      "Epoch 1762/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5295 - val_loss: 11.1820\n",
      "Epoch 1763/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6176 - val_loss: 10.8288\n",
      "Epoch 1764/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6078 - val_loss: 10.8636\n",
      "Epoch 1765/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4174 - val_loss: 14.0527\n",
      "Epoch 1766/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7844 - val_loss: 10.9738\n",
      "Epoch 1767/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9361 - val_loss: 10.7584\n",
      "Epoch 1768/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5018 - val_loss: 10.7171\n",
      "Epoch 1769/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4278 - val_loss: 10.8767\n",
      "Epoch 1770/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6774 - val_loss: 10.9897\n",
      "Epoch 1771/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6727 - val_loss: 11.4644\n",
      "Epoch 1772/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8647 - val_loss: 11.0045\n",
      "Epoch 1773/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5532 - val_loss: 10.7860\n",
      "Epoch 1774/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4805 - val_loss: 10.9443\n",
      "Epoch 1775/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7003 - val_loss: 10.9251\n",
      "Epoch 1776/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8508 - val_loss: 11.2347\n",
      "Epoch 1777/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5101 - val_loss: 10.9315\n",
      "Epoch 1778/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4619 - val_loss: 10.8937\n",
      "Epoch 1779/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5165 - val_loss: 11.4571\n",
      "Epoch 1780/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6298 - val_loss: 10.7521\n",
      "Epoch 1781/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5363 - val_loss: 11.0448\n",
      "Epoch 1782/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8356 - val_loss: 11.1184\n",
      "Epoch 1783/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5782 - val_loss: 11.1372\n",
      "Epoch 1784/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8194 - val_loss: 11.3209\n",
      "Epoch 1785/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7354 - val_loss: 10.9112\n",
      "Epoch 1786/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5827 - val_loss: 10.9252\n",
      "Epoch 1787/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6794 - val_loss: 10.9356\n",
      "Epoch 1788/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7043 - val_loss: 11.4077\n",
      "Epoch 1789/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5214 - val_loss: 10.8397\n",
      "Epoch 1790/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4827 - val_loss: 10.8898\n",
      "Epoch 1791/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4810 - val_loss: 10.8779\n",
      "Epoch 1792/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6472 - val_loss: 11.1311\n",
      "Epoch 1793/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6131 - val_loss: 10.9362\n",
      "Epoch 1794/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6300 - val_loss: 10.8539\n",
      "Epoch 1795/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8385 - val_loss: 11.4930\n",
      "Epoch 1796/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0231 - val_loss: 10.9350\n",
      "Epoch 1797/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8658 - val_loss: 10.7803\n",
      "Epoch 1798/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6260 - val_loss: 10.9193\n",
      "Epoch 1799/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6254 - val_loss: 10.8360\n",
      "Epoch 1800/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8240 - val_loss: 11.0359\n",
      "Epoch 1801/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7508 - val_loss: 11.0358\n",
      "Epoch 1802/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6650 - val_loss: 10.6833\n",
      "Epoch 1803/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6916 - val_loss: 11.1878\n",
      "Epoch 1804/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6632 - val_loss: 10.9187\n",
      "Epoch 1805/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7019 - val_loss: 10.9202\n",
      "Epoch 1806/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5805 - val_loss: 11.1641\n",
      "Epoch 1807/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5071 - val_loss: 10.9480\n",
      "Epoch 1808/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5537 - val_loss: 10.9398\n",
      "Epoch 1809/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7558 - val_loss: 10.9257\n",
      "Epoch 1810/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5284 - val_loss: 10.9601\n",
      "Epoch 1811/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7579 - val_loss: 11.6664\n",
      "Epoch 1812/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6199 - val_loss: 10.9300\n",
      "Epoch 1813/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7874 - val_loss: 10.8581\n",
      "Epoch 1814/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5257 - val_loss: 10.7795\n",
      "Epoch 1815/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6605 - val_loss: 10.8898\n",
      "Epoch 1816/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6351 - val_loss: 10.8361\n",
      "Epoch 1817/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5823 - val_loss: 10.9111\n",
      "Epoch 1818/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7281 - val_loss: 10.6977\n",
      "Epoch 1819/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6623 - val_loss: 11.1123\n",
      "Epoch 1820/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5953 - val_loss: 10.9726\n",
      "Epoch 1821/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6125 - val_loss: 10.8652\n",
      "Epoch 1822/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5940 - val_loss: 11.1630\n",
      "Epoch 1823/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7013 - val_loss: 10.8834\n",
      "Epoch 1824/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5964 - val_loss: 11.0026\n",
      "Epoch 1825/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5439 - val_loss: 11.6816\n",
      "Epoch 1826/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6419 - val_loss: 11.2997\n",
      "Epoch 1827/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6406 - val_loss: 11.0719\n",
      "Epoch 1828/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6902 - val_loss: 11.0772\n",
      "Epoch 1829/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7672 - val_loss: 10.8413\n",
      "Epoch 1830/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6821 - val_loss: 11.0462\n",
      "Epoch 1831/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5273 - val_loss: 11.3631\n",
      "Epoch 1832/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8046 - val_loss: 11.0666\n",
      "Epoch 1833/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6072 - val_loss: 11.0471\n",
      "Epoch 1834/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6837 - val_loss: 10.9622\n",
      "Epoch 1835/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5095 - val_loss: 11.5643\n",
      "Epoch 1836/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7177 - val_loss: 10.9681\n",
      "Epoch 1837/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5215 - val_loss: 11.1338\n",
      "Epoch 1838/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5996 - val_loss: 11.1175\n",
      "Epoch 1839/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5580 - val_loss: 11.0361\n",
      "Epoch 1840/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6644 - val_loss: 11.0641\n",
      "Epoch 1841/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6696 - val_loss: 11.3152\n",
      "Epoch 1842/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8020 - val_loss: 11.7291\n",
      "Epoch 1843/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6255 - val_loss: 11.1230\n",
      "Epoch 1844/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7443 - val_loss: 10.9808\n",
      "Epoch 1845/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6408 - val_loss: 10.8976\n",
      "Epoch 1846/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6073 - val_loss: 10.9900\n",
      "Epoch 1847/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5326 - val_loss: 11.3289\n",
      "Epoch 1848/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6699 - val_loss: 11.2290\n",
      "Epoch 1849/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5421 - val_loss: 11.1284\n",
      "Epoch 1850/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7988 - val_loss: 10.9426\n",
      "Epoch 1851/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4487 - val_loss: 10.9119\n",
      "Epoch 1852/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7596 - val_loss: 11.6878\n",
      "Epoch 1853/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6821 - val_loss: 11.0527\n",
      "Epoch 1854/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6366 - val_loss: 10.9606\n",
      "Epoch 1855/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8805 - val_loss: 11.2736\n",
      "Epoch 1856/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5222 - val_loss: 10.9624\n",
      "Epoch 1857/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5539 - val_loss: 10.8365\n",
      "Epoch 1858/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6372 - val_loss: 11.2572\n",
      "Epoch 1859/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7374 - val_loss: 11.2017\n",
      "Epoch 1860/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5618 - val_loss: 11.2160\n",
      "Epoch 1861/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5308 - val_loss: 11.0687\n",
      "Epoch 1862/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6113 - val_loss: 10.9940\n",
      "Epoch 1863/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5586 - val_loss: 10.8008\n",
      "Epoch 1864/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6996 - val_loss: 10.9609\n",
      "Epoch 1865/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6570 - val_loss: 11.0237\n",
      "Epoch 1866/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6117 - val_loss: 11.1719\n",
      "Epoch 1867/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5119 - val_loss: 10.9897\n",
      "Epoch 1868/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6837 - val_loss: 11.0929\n",
      "Epoch 1869/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5287 - val_loss: 10.8444\n",
      "Epoch 1870/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4718 - val_loss: 10.8126\n",
      "Epoch 1871/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5536 - val_loss: 11.1592\n",
      "Epoch 1872/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4840 - val_loss: 11.1661\n",
      "Epoch 1873/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8241 - val_loss: 11.5488\n",
      "Epoch 1874/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7830 - val_loss: 11.1842\n",
      "Epoch 1875/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5857 - val_loss: 11.3906\n",
      "Epoch 1876/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5447 - val_loss: 10.9365\n",
      "Epoch 1877/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6345 - val_loss: 11.1330\n",
      "Epoch 1878/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7532 - val_loss: 11.2085\n",
      "Epoch 1879/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7226 - val_loss: 11.6072\n",
      "Epoch 1880/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8538 - val_loss: 10.9936\n",
      "Epoch 1881/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5952 - val_loss: 10.9819\n",
      "Epoch 1882/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7674 - val_loss: 11.4360\n",
      "Epoch 1883/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5132 - val_loss: 11.0283\n",
      "Epoch 1884/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5336 - val_loss: 10.9275\n",
      "Epoch 1885/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5789 - val_loss: 11.0227\n",
      "Epoch 1886/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5612 - val_loss: 11.4034\n",
      "Epoch 1887/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5520 - val_loss: 11.5584\n",
      "Epoch 1888/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6127 - val_loss: 10.8791\n",
      "Epoch 1889/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5934 - val_loss: 10.9743\n",
      "Epoch 1890/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8868 - val_loss: 12.4482\n",
      "Epoch 1891/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.9094 - val_loss: 11.7181\n",
      "Epoch 1892/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8548 - val_loss: 11.1868\n",
      "Epoch 1893/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6037 - val_loss: 10.9252\n",
      "Epoch 1894/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3966 - val_loss: 10.8780\n",
      "Epoch 1895/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5115 - val_loss: 11.5688\n",
      "Epoch 1896/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4656 - val_loss: 10.8602\n",
      "Epoch 1897/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4580 - val_loss: 10.8984\n",
      "Epoch 1898/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4675 - val_loss: 10.8779\n",
      "Epoch 1899/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6934 - val_loss: 11.4302\n",
      "Epoch 1900/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5400 - val_loss: 10.9068\n",
      "Epoch 1901/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5026 - val_loss: 10.8224\n",
      "Epoch 1902/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5689 - val_loss: 10.8849\n",
      "Epoch 1903/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7413 - val_loss: 11.2103\n",
      "Epoch 1904/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5931 - val_loss: 10.9758\n",
      "Epoch 1905/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5336 - val_loss: 11.3423\n",
      "Epoch 1906/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7535 - val_loss: 11.0222\n",
      "Epoch 1907/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8355 - val_loss: 11.0533\n",
      "Epoch 1908/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6457 - val_loss: 10.9615\n",
      "Epoch 1909/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4496 - val_loss: 10.9683\n",
      "Epoch 1910/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5278 - val_loss: 11.2872\n",
      "Epoch 1911/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5531 - val_loss: 11.0716\n",
      "Epoch 1912/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4633 - val_loss: 10.8289\n",
      "Epoch 1913/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6030 - val_loss: 11.0914\n",
      "Epoch 1914/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5200 - val_loss: 11.0841\n",
      "Epoch 1915/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6918 - val_loss: 10.9963\n",
      "Epoch 1916/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6295 - val_loss: 11.0355\n",
      "Epoch 1917/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6266 - val_loss: 10.8630\n",
      "Epoch 1918/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7101 - val_loss: 10.8847\n",
      "Epoch 1919/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5622 - val_loss: 11.0183\n",
      "Epoch 1920/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6758 - val_loss: 11.1145\n",
      "Epoch 1921/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5761 - val_loss: 11.1195\n",
      "Epoch 1922/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5389 - val_loss: 11.1018\n",
      "Epoch 1923/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5980 - val_loss: 11.0000\n",
      "Epoch 1924/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6484 - val_loss: 11.8112\n",
      "Epoch 1925/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6566 - val_loss: 11.0465\n",
      "Epoch 1926/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6771 - val_loss: 11.3267\n",
      "Epoch 1927/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6147 - val_loss: 11.1068\n",
      "Epoch 1928/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7120 - val_loss: 11.2661\n",
      "Epoch 1929/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5636 - val_loss: 11.0948\n",
      "Epoch 1930/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6636 - val_loss: 11.1485\n",
      "Epoch 1931/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5983 - val_loss: 11.0813\n",
      "Epoch 1932/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6452 - val_loss: 11.4685\n",
      "Epoch 1933/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6049 - val_loss: 11.1440\n",
      "Epoch 1934/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5332 - val_loss: 11.1363\n",
      "Epoch 1935/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5217 - val_loss: 11.1478\n",
      "Epoch 1936/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6087 - val_loss: 11.0440\n",
      "Epoch 1937/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6615 - val_loss: 11.0443\n",
      "Epoch 1938/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5484 - val_loss: 10.9805\n",
      "Epoch 1939/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6348 - val_loss: 11.0739\n",
      "Epoch 1940/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7653 - val_loss: 11.0952\n",
      "Epoch 1941/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7843 - val_loss: 10.9989\n",
      "Epoch 1942/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5645 - val_loss: 11.3823\n",
      "Epoch 1943/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4578 - val_loss: 11.1311\n",
      "Epoch 1944/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7614 - val_loss: 11.1673\n",
      "Epoch 1945/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7575 - val_loss: 11.2051\n",
      "Epoch 1946/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5911 - val_loss: 11.0649\n",
      "Epoch 1947/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4556 - val_loss: 11.0600\n",
      "Epoch 1948/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5000 - val_loss: 10.9723\n",
      "Epoch 1949/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6729 - val_loss: 10.9030\n",
      "Epoch 1950/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4636 - val_loss: 11.1666\n",
      "Epoch 1951/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6516 - val_loss: 11.2983\n",
      "Epoch 1952/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7294 - val_loss: 11.0543\n",
      "Epoch 1953/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5029 - val_loss: 11.5272\n",
      "Epoch 1954/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.3606 - val_loss: 11.7933\n",
      "Epoch 1955/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8964 - val_loss: 11.0065\n",
      "Epoch 1956/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5934 - val_loss: 11.6208\n",
      "Epoch 1957/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4458 - val_loss: 10.7748\n",
      "Epoch 1958/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4677 - val_loss: 10.9114\n",
      "Epoch 1959/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4497 - val_loss: 10.8865\n",
      "Epoch 1960/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4842 - val_loss: 11.0061\n",
      "Epoch 1961/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4198 - val_loss: 10.9420\n",
      "Epoch 1962/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6577 - val_loss: 11.1816\n",
      "Epoch 1963/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6456 - val_loss: 11.0707\n",
      "Epoch 1964/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5688 - val_loss: 11.0846\n",
      "Epoch 1965/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6598 - val_loss: 11.0906\n",
      "Epoch 1966/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4735 - val_loss: 11.4172\n",
      "Epoch 1967/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7033 - val_loss: 11.1541\n",
      "Epoch 1968/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6773 - val_loss: 11.2281\n",
      "Epoch 1969/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4884 - val_loss: 10.9263\n",
      "Epoch 1970/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8582 - val_loss: 11.9999\n",
      "Epoch 1971/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9394 - val_loss: 10.9136\n",
      "Epoch 1972/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7570 - val_loss: 11.0713\n",
      "Epoch 1973/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5118 - val_loss: 11.1049\n",
      "Epoch 1974/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6059 - val_loss: 10.9498\n",
      "Epoch 1975/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4797 - val_loss: 10.9554\n",
      "Epoch 1976/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4070 - val_loss: 11.2056\n",
      "Epoch 1977/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5670 - val_loss: 11.5173\n",
      "Epoch 1978/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6827 - val_loss: 10.9413\n",
      "Epoch 1979/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4316 - val_loss: 11.0010\n",
      "Epoch 1980/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5539 - val_loss: 11.1250\n",
      "Epoch 1981/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6002 - val_loss: 11.2220\n",
      "Epoch 1982/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6443 - val_loss: 11.3813\n",
      "Epoch 1983/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7476 - val_loss: 11.4861\n",
      "Epoch 1984/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7124 - val_loss: 10.9668\n",
      "Epoch 1985/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5899 - val_loss: 11.4189\n",
      "Epoch 1986/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5160 - val_loss: 10.9667\n",
      "Epoch 1987/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4566 - val_loss: 11.0036\n",
      "Epoch 1988/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7086 - val_loss: 11.4531\n",
      "Epoch 1989/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6529 - val_loss: 11.2172\n",
      "Epoch 1990/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5264 - val_loss: 10.8921\n",
      "Epoch 1991/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6197 - val_loss: 11.2192\n",
      "Epoch 1992/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5862 - val_loss: 11.5901\n",
      "Epoch 1993/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6446 - val_loss: 10.9759\n",
      "Epoch 1994/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4789 - val_loss: 12.0533\n",
      "Epoch 1995/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0371 - val_loss: 10.7368\n",
      "Epoch 1996/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5167 - val_loss: 10.7300\n",
      "Epoch 1997/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4900 - val_loss: 10.7060\n",
      "Epoch 1998/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4642 - val_loss: 10.9375\n",
      "Epoch 1999/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6064 - val_loss: 10.9200\n",
      "Epoch 2000/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5369 - val_loss: 10.8701\n",
      "Epoch 2001/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4342 - val_loss: 10.9576\n",
      "Epoch 2002/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4650 - val_loss: 10.9133\n",
      "Epoch 2003/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4712 - val_loss: 10.8508\n",
      "Epoch 2004/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8527 - val_loss: 10.9933\n",
      "Epoch 2005/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6209 - val_loss: 10.9069\n",
      "Epoch 2006/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5378 - val_loss: 10.9516\n",
      "Epoch 2007/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4751 - val_loss: 10.7753\n",
      "Epoch 2008/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6980 - val_loss: 10.9219\n",
      "Epoch 2009/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5327 - val_loss: 10.8885\n",
      "Epoch 2010/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5401 - val_loss: 11.0880\n",
      "Epoch 2011/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6531 - val_loss: 11.0624\n",
      "Epoch 2012/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5195 - val_loss: 11.9289\n",
      "Epoch 2013/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8354 - val_loss: 10.9262\n",
      "Epoch 2014/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5145 - val_loss: 10.9376\n",
      "Epoch 2015/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4631 - val_loss: 11.1109\n",
      "Epoch 2016/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4634 - val_loss: 11.0365\n",
      "Epoch 2017/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5697 - val_loss: 10.9256\n",
      "Epoch 2018/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6874 - val_loss: 11.0879\n",
      "Epoch 2019/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6279 - val_loss: 11.2817\n",
      "Epoch 2020/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7736 - val_loss: 11.1232\n",
      "Epoch 2021/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6213 - val_loss: 11.2190\n",
      "Epoch 2022/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5231 - val_loss: 11.2164\n",
      "Epoch 2023/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7108 - val_loss: 11.0933\n",
      "Epoch 2024/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4802 - val_loss: 11.2753\n",
      "Epoch 2025/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5279 - val_loss: 10.9613\n",
      "Epoch 2026/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6735 - val_loss: 11.1247\n",
      "Epoch 2027/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5861 - val_loss: 11.2929\n",
      "Epoch 2028/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6214 - val_loss: 11.2249\n",
      "Epoch 2029/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6664 - val_loss: 10.8493\n",
      "Epoch 2030/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7803 - val_loss: 12.6222\n",
      "Epoch 2031/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9919 - val_loss: 11.0153\n",
      "Epoch 2032/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7049 - val_loss: 11.0841\n",
      "Epoch 2033/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4409 - val_loss: 10.8832\n",
      "Epoch 2034/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4287 - val_loss: 11.0823\n",
      "Epoch 2035/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5862 - val_loss: 10.8585\n",
      "Epoch 2036/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6060 - val_loss: 11.4301\n",
      "Epoch 2037/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7013 - val_loss: 11.1333\n",
      "Epoch 2038/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4673 - val_loss: 12.2598\n",
      "Epoch 2039/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1350 - val_loss: 11.5836\n",
      "Epoch 2040/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6116 - val_loss: 10.8763\n",
      "Epoch 2041/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4132 - val_loss: 10.9610\n",
      "Epoch 2042/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4898 - val_loss: 11.0503\n",
      "Epoch 2043/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5936 - val_loss: 10.7659\n",
      "Epoch 2044/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5640 - val_loss: 10.9315\n",
      "Epoch 2045/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4220 - val_loss: 10.9628\n",
      "Epoch 2046/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5269 - val_loss: 11.0182\n",
      "Epoch 2047/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9889 - val_loss: 11.4165\n",
      "Epoch 2048/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6097 - val_loss: 11.1221\n",
      "Epoch 2049/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4263 - val_loss: 10.9013\n",
      "Epoch 2050/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7960 - val_loss: 11.1900\n",
      "Epoch 2051/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3487 - val_loss: 11.1298\n",
      "Epoch 2052/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4359 - val_loss: 11.1488\n",
      "Epoch 2053/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4117 - val_loss: 11.0334\n",
      "Epoch 2054/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6606 - val_loss: 11.2009\n",
      "Epoch 2055/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5043 - val_loss: 10.8770\n",
      "Epoch 2056/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5919 - val_loss: 11.0893\n",
      "Epoch 2057/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6715 - val_loss: 11.0336\n",
      "Epoch 2058/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6789 - val_loss: 10.9387\n",
      "Epoch 2059/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5241 - val_loss: 10.9409\n",
      "Epoch 2060/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5295 - val_loss: 10.7680\n",
      "Epoch 2061/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5241 - val_loss: 11.0424\n",
      "Epoch 2062/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6075 - val_loss: 11.8236\n",
      "Epoch 2063/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4898 - val_loss: 12.1727\n",
      "Epoch 2064/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7019 - val_loss: 11.1324\n",
      "Epoch 2065/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5693 - val_loss: 11.3921\n",
      "Epoch 2066/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6619 - val_loss: 11.1934\n",
      "Epoch 2067/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6357 - val_loss: 10.8717\n",
      "Epoch 2068/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5098 - val_loss: 10.9292\n",
      "Epoch 2069/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4446 - val_loss: 10.9850\n",
      "Epoch 2070/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7315 - val_loss: 10.9543\n",
      "Epoch 2071/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5694 - val_loss: 11.0278\n",
      "Epoch 2072/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4853 - val_loss: 10.9974\n",
      "Epoch 2073/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4456 - val_loss: 11.0489\n",
      "Epoch 2074/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5217 - val_loss: 10.9035\n",
      "Epoch 2075/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5673 - val_loss: 11.2516\n",
      "Epoch 2076/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5328 - val_loss: 11.3195\n",
      "Epoch 2077/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7690 - val_loss: 11.1650\n",
      "Epoch 2078/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.1684 - val_loss: 12.3361\n",
      "Epoch 2079/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7427 - val_loss: 10.7379\n",
      "Epoch 2080/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5220 - val_loss: 11.3574\n",
      "Epoch 2081/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7128 - val_loss: 10.8295\n",
      "Epoch 2082/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5238 - val_loss: 10.8381\n",
      "Epoch 2083/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4652 - val_loss: 10.7896\n",
      "Epoch 2084/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3857 - val_loss: 10.9222\n",
      "Epoch 2085/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4542 - val_loss: 10.9788\n",
      "Epoch 2086/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4149 - val_loss: 11.1368\n",
      "Epoch 2087/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6906 - val_loss: 10.8927\n",
      "Epoch 2088/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6084 - val_loss: 10.8724\n",
      "Epoch 2089/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5059 - val_loss: 10.8651\n",
      "Epoch 2090/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5822 - val_loss: 11.1656\n",
      "Epoch 2091/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6908 - val_loss: 11.2426\n",
      "Epoch 2092/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5324 - val_loss: 11.0812\n",
      "Epoch 2093/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5519 - val_loss: 11.1691\n",
      "Epoch 2094/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4532 - val_loss: 11.2530\n",
      "Epoch 2095/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8446 - val_loss: 10.8270\n",
      "Epoch 2096/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5995 - val_loss: 11.5541\n",
      "Epoch 2097/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6486 - val_loss: 11.1020\n",
      "Epoch 2098/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4549 - val_loss: 10.8885\n",
      "Epoch 2099/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5104 - val_loss: 11.0085\n",
      "Epoch 2100/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4191 - val_loss: 11.3079\n",
      "Epoch 2101/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7305 - val_loss: 11.5071\n",
      "Epoch 2102/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5254 - val_loss: 11.0491\n",
      "Epoch 2103/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4386 - val_loss: 10.8086\n",
      "Epoch 2104/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4445 - val_loss: 11.0266\n",
      "Epoch 2105/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5121 - val_loss: 10.9980\n",
      "Epoch 2106/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8213 - val_loss: 11.1541\n",
      "Epoch 2107/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8446 - val_loss: 10.9825\n",
      "Epoch 2108/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4754 - val_loss: 11.1354\n",
      "Epoch 2109/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6147 - val_loss: 11.1101\n",
      "Epoch 2110/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5087 - val_loss: 11.1673\n",
      "Epoch 2111/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6383 - val_loss: 11.5760\n",
      "Epoch 2112/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5385 - val_loss: 11.1831\n",
      "Epoch 2113/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6276 - val_loss: 10.9812\n",
      "Epoch 2114/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4730 - val_loss: 10.9654\n",
      "Epoch 2115/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4784 - val_loss: 11.2673\n",
      "Epoch 2116/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4171 - val_loss: 10.9421\n",
      "Epoch 2117/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6561 - val_loss: 12.2349\n",
      "Epoch 2118/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7277 - val_loss: 11.1062\n",
      "Epoch 2119/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4947 - val_loss: 10.9831\n",
      "Epoch 2120/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5673 - val_loss: 11.0819\n",
      "Epoch 2121/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5378 - val_loss: 12.0412\n",
      "Epoch 2122/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5595 - val_loss: 11.0301\n",
      "Epoch 2123/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5895 - val_loss: 11.7234\n",
      "Epoch 2124/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5571 - val_loss: 11.3865\n",
      "Epoch 2125/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5355 - val_loss: 11.0930\n",
      "Epoch 2126/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5903 - val_loss: 11.2968\n",
      "Epoch 2127/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5160 - val_loss: 11.1653\n",
      "Epoch 2128/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5521 - val_loss: 11.4157\n",
      "Epoch 2129/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5491 - val_loss: 10.9752\n",
      "Epoch 2130/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4830 - val_loss: 11.2541\n",
      "Epoch 2131/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5552 - val_loss: 11.1789\n",
      "Epoch 2132/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5670 - val_loss: 11.4726\n",
      "Epoch 2133/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5086 - val_loss: 11.0464\n",
      "Epoch 2134/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5073 - val_loss: 11.5264\n",
      "Epoch 2135/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5911 - val_loss: 11.0253\n",
      "Epoch 2136/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8161 - val_loss: 11.1302\n",
      "Epoch 2137/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5571 - val_loss: 11.1190\n",
      "Epoch 2138/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5102 - val_loss: 10.9779\n",
      "Epoch 2139/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4400 - val_loss: 11.1312\n",
      "Epoch 2140/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9616 - val_loss: 12.2618\n",
      "Epoch 2141/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8683 - val_loss: 11.0258\n",
      "Epoch 2142/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5266 - val_loss: 10.7341\n",
      "Epoch 2143/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4773 - val_loss: 11.1301\n",
      "Epoch 2144/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5449 - val_loss: 11.0748\n",
      "Epoch 2145/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5286 - val_loss: 10.9239\n",
      "Epoch 2146/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5252 - val_loss: 11.1447\n",
      "Epoch 2147/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4707 - val_loss: 11.0986\n",
      "Epoch 2148/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4622 - val_loss: 11.0054\n",
      "Epoch 2149/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6969 - val_loss: 11.4019\n",
      "Epoch 2150/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6388 - val_loss: 10.8884\n",
      "Epoch 2151/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4974 - val_loss: 11.1636\n",
      "Epoch 2152/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5035 - val_loss: 11.1736\n",
      "Epoch 2153/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6743 - val_loss: 11.0168\n",
      "Epoch 2154/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4975 - val_loss: 10.8467\n",
      "Epoch 2155/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6141 - val_loss: 11.8136\n",
      "Epoch 2156/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5365 - val_loss: 11.3105\n",
      "Epoch 2157/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8756 - val_loss: 11.7838\n",
      "Epoch 2158/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5883 - val_loss: 11.1281\n",
      "Epoch 2159/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5788 - val_loss: 10.9860\n",
      "Epoch 2160/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4107 - val_loss: 11.5602\n",
      "Epoch 2161/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5466 - val_loss: 11.1252\n",
      "Epoch 2162/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5613 - val_loss: 11.4040\n",
      "Epoch 2163/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5103 - val_loss: 11.3371\n",
      "Epoch 2164/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4245 - val_loss: 11.3516\n",
      "Epoch 2165/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6062 - val_loss: 11.0152\n",
      "Epoch 2166/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4253 - val_loss: 11.0628\n",
      "Epoch 2167/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5000 - val_loss: 11.2471\n",
      "Epoch 2168/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4425 - val_loss: 10.9200\n",
      "Epoch 2169/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6669 - val_loss: 10.9432\n",
      "Epoch 2170/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4936 - val_loss: 11.1818\n",
      "Epoch 2171/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7642 - val_loss: 11.6710\n",
      "Epoch 2172/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7301 - val_loss: 10.8894\n",
      "Epoch 2173/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4124 - val_loss: 10.8569\n",
      "Epoch 2174/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4556 - val_loss: 10.8181\n",
      "Epoch 2175/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5879 - val_loss: 11.2519\n",
      "Epoch 2176/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6189 - val_loss: 11.0266\n",
      "Epoch 2177/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4393 - val_loss: 10.8959\n",
      "Epoch 2178/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6119 - val_loss: 11.1554\n",
      "Epoch 2179/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5317 - val_loss: 11.3726\n",
      "Epoch 2180/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3766 - val_loss: 11.0066\n",
      "Epoch 2181/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4965 - val_loss: 10.9016\n",
      "Epoch 2182/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5880 - val_loss: 11.2053\n",
      "Epoch 2183/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6195 - val_loss: 11.3634\n",
      "Epoch 2184/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8044 - val_loss: 10.7876\n",
      "Epoch 2185/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6555 - val_loss: 10.9900\n",
      "Epoch 2186/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4307 - val_loss: 10.9594\n",
      "Epoch 2187/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4697 - val_loss: 11.3408\n",
      "Epoch 2188/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5569 - val_loss: 11.1398\n",
      "Epoch 2189/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5226 - val_loss: 11.5322\n",
      "Epoch 2190/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4387 - val_loss: 11.0310\n",
      "Epoch 2191/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5878 - val_loss: 11.1129\n",
      "Epoch 2192/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6135 - val_loss: 10.9324\n",
      "Epoch 2193/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5196 - val_loss: 11.0736\n",
      "Epoch 2194/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4744 - val_loss: 10.9548\n",
      "Epoch 2195/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6343 - val_loss: 11.4401\n",
      "Epoch 2196/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.6792 - val_loss: 11.9670\n",
      "Epoch 2197/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7947 - val_loss: 10.8697\n",
      "Epoch 2198/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5779 - val_loss: 10.8624\n",
      "Epoch 2199/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5254 - val_loss: 11.6085\n",
      "Epoch 2200/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5028 - val_loss: 10.9237\n",
      "Epoch 2201/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4739 - val_loss: 10.9219\n",
      "Epoch 2202/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4273 - val_loss: 11.6017\n",
      "Epoch 2203/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4638 - val_loss: 10.9519\n",
      "Epoch 2204/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3779 - val_loss: 10.9731\n",
      "Epoch 2205/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5400 - val_loss: 12.4203\n",
      "Epoch 2206/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5279 - val_loss: 10.9757\n",
      "Epoch 2207/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5026 - val_loss: 10.8584\n",
      "Epoch 2208/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5282 - val_loss: 11.0574\n",
      "Epoch 2209/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4253 - val_loss: 10.8975\n",
      "Epoch 2210/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4831 - val_loss: 11.2808\n",
      "Epoch 2211/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4373 - val_loss: 10.9255\n",
      "Epoch 2212/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4196 - val_loss: 11.3748\n",
      "Epoch 2213/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5904 - val_loss: 11.2441\n",
      "Epoch 2214/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7141 - val_loss: 11.1580\n",
      "Epoch 2215/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4135 - val_loss: 11.2146\n",
      "Epoch 2216/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.2437 - val_loss: 12.5228\n",
      "Epoch 2217/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.9447 - val_loss: 11.1252\n",
      "Epoch 2218/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6753 - val_loss: 11.0141\n",
      "Epoch 2219/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4015 - val_loss: 11.0257\n",
      "Epoch 2220/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5143 - val_loss: 10.7902\n",
      "Epoch 2221/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4910 - val_loss: 11.1936\n",
      "Epoch 2222/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5913 - val_loss: 11.1908\n",
      "Epoch 2223/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4644 - val_loss: 11.0937\n",
      "Epoch 2224/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3643 - val_loss: 11.1528\n",
      "Epoch 2225/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5612 - val_loss: 11.0822\n",
      "Epoch 2226/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3571 - val_loss: 10.9095\n",
      "Epoch 2227/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4497 - val_loss: 10.9923\n",
      "Epoch 2228/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5195 - val_loss: 11.2612\n",
      "Epoch 2229/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6400 - val_loss: 11.6310\n",
      "Epoch 2230/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4845 - val_loss: 11.0783\n",
      "Epoch 2231/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5639 - val_loss: 11.0309\n",
      "Epoch 2232/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4313 - val_loss: 11.3814\n",
      "Epoch 2233/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4676 - val_loss: 11.3803\n",
      "Epoch 2234/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5078 - val_loss: 11.2678\n",
      "Epoch 2235/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4967 - val_loss: 11.1640\n",
      "Epoch 2236/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5478 - val_loss: 11.2644\n",
      "Epoch 2237/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4994 - val_loss: 11.1916\n",
      "Epoch 2238/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3534 - val_loss: 10.9515\n",
      "Epoch 2239/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6897 - val_loss: 11.5882\n",
      "Epoch 2240/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4116 - val_loss: 11.1564\n",
      "Epoch 2241/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4832 - val_loss: 11.6944\n",
      "Epoch 2242/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6848 - val_loss: 11.2702\n",
      "Epoch 2243/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5631 - val_loss: 11.1442\n",
      "Epoch 2244/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4123 - val_loss: 10.9236\n",
      "Epoch 2245/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5790 - val_loss: 11.5643\n",
      "Epoch 2246/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5542 - val_loss: 10.8163\n",
      "Epoch 2247/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5353 - val_loss: 10.9899\n",
      "Epoch 2248/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4666 - val_loss: 11.3331\n",
      "Epoch 2249/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5127 - val_loss: 11.3635\n",
      "Epoch 2250/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4509 - val_loss: 11.3273\n",
      "Epoch 2251/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4078 - val_loss: 11.2146\n",
      "Epoch 2252/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3814 - val_loss: 11.4802\n",
      "Epoch 2253/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6017 - val_loss: 11.6966\n",
      "Epoch 2254/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4617 - val_loss: 11.3758\n",
      "Epoch 2255/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4963 - val_loss: 11.5998\n",
      "Epoch 2256/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5457 - val_loss: 12.1687\n",
      "Epoch 2257/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9065 - val_loss: 11.5517\n",
      "Epoch 2258/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5688 - val_loss: 11.1798\n",
      "Epoch 2259/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5725 - val_loss: 10.9804\n",
      "Epoch 2260/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6360 - val_loss: 11.1175\n",
      "Epoch 2261/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5314 - val_loss: 11.7209\n",
      "Epoch 2262/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4561 - val_loss: 11.2374\n",
      "Epoch 2263/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3777 - val_loss: 11.1074\n",
      "Epoch 2264/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4393 - val_loss: 11.1538\n",
      "Epoch 2265/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5806 - val_loss: 11.5809\n",
      "Epoch 2266/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4481 - val_loss: 11.0949\n",
      "Epoch 2267/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4327 - val_loss: 11.4469\n",
      "Epoch 2268/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6173 - val_loss: 11.1913\n",
      "Epoch 2269/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6762 - val_loss: 11.0314\n",
      "Epoch 2270/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5250 - val_loss: 10.9610\n",
      "Epoch 2271/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4552 - val_loss: 11.2340\n",
      "Epoch 2272/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4289 - val_loss: 11.3002\n",
      "Epoch 2273/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5203 - val_loss: 11.1451\n",
      "Epoch 2274/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5438 - val_loss: 11.4374\n",
      "Epoch 2275/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4107 - val_loss: 11.1730\n",
      "Epoch 2276/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4507 - val_loss: 10.9597\n",
      "Epoch 2277/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6138 - val_loss: 11.2050\n",
      "Epoch 2278/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4603 - val_loss: 11.9228\n",
      "Epoch 2279/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7753 - val_loss: 11.1848\n",
      "Epoch 2280/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6327 - val_loss: 11.0146\n",
      "Epoch 2281/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4261 - val_loss: 11.2874\n",
      "Epoch 2282/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6808 - val_loss: 11.2219\n",
      "Epoch 2283/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4868 - val_loss: 11.4400\n",
      "Epoch 2284/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4443 - val_loss: 11.3162\n",
      "Epoch 2285/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4456 - val_loss: 10.9684\n",
      "Epoch 2286/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5183 - val_loss: 11.3861\n",
      "Epoch 2287/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5458 - val_loss: 10.9635\n",
      "Epoch 2288/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4802 - val_loss: 11.1642\n",
      "Epoch 2289/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4684 - val_loss: 11.2287\n",
      "Epoch 2290/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6446 - val_loss: 11.3895\n",
      "Epoch 2291/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5037 - val_loss: 10.9341\n",
      "Epoch 2292/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5133 - val_loss: 11.1966\n",
      "Epoch 2293/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4134 - val_loss: 11.0377\n",
      "Epoch 2294/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4323 - val_loss: 11.1976\n",
      "Epoch 2295/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5727 - val_loss: 11.0284\n",
      "Epoch 2296/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6069 - val_loss: 11.2955\n",
      "Epoch 2297/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5945 - val_loss: 11.1209\n",
      "Epoch 2298/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4176 - val_loss: 11.0811\n",
      "Epoch 2299/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5110 - val_loss: 11.2372\n",
      "Epoch 2300/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4837 - val_loss: 11.1516\n",
      "Epoch 2301/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4986 - val_loss: 11.0434\n",
      "Epoch 2302/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4423 - val_loss: 11.0511\n",
      "Epoch 2303/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7654 - val_loss: 14.4921\n",
      "Epoch 2304/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7530 - val_loss: 11.2471\n",
      "Epoch 2305/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4922 - val_loss: 10.8633\n",
      "Epoch 2306/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3359 - val_loss: 11.0391\n",
      "Epoch 2307/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5439 - val_loss: 10.9559\n",
      "Epoch 2308/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4711 - val_loss: 11.0397\n",
      "Epoch 2309/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4111 - val_loss: 10.9416\n",
      "Epoch 2310/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4111 - val_loss: 11.1799\n",
      "Epoch 2311/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5289 - val_loss: 11.3498\n",
      "Epoch 2312/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5351 - val_loss: 11.1337\n",
      "Epoch 2313/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4813 - val_loss: 11.2164\n",
      "Epoch 2314/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4693 - val_loss: 11.1263\n",
      "Epoch 2315/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5237 - val_loss: 11.1058\n",
      "Epoch 2316/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6553 - val_loss: 11.9788\n",
      "Epoch 2317/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7151 - val_loss: 11.1222\n",
      "Epoch 2318/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6805 - val_loss: 11.1563\n",
      "Epoch 2319/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4743 - val_loss: 10.9952\n",
      "Epoch 2320/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4643 - val_loss: 11.2849\n",
      "Epoch 2321/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4297 - val_loss: 11.3067\n",
      "Epoch 2322/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4353 - val_loss: 11.2230\n",
      "Epoch 2323/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5248 - val_loss: 11.1216\n",
      "Epoch 2324/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3385 - val_loss: 11.0186\n",
      "Epoch 2325/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5794 - val_loss: 11.1304\n",
      "Epoch 2326/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5800 - val_loss: 11.1609\n",
      "Epoch 2327/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7183 - val_loss: 11.2989\n",
      "Epoch 2328/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5642 - val_loss: 11.0973\n",
      "Epoch 2329/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4896 - val_loss: 11.5678\n",
      "Epoch 2330/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5663 - val_loss: 11.2513\n",
      "Epoch 2331/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6522 - val_loss: 11.3486\n",
      "Epoch 2332/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4752 - val_loss: 11.0032\n",
      "Epoch 2333/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3882 - val_loss: 11.5621\n",
      "Epoch 2334/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5754 - val_loss: 11.2888\n",
      "Epoch 2335/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3778 - val_loss: 11.2415\n",
      "Epoch 2336/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3921 - val_loss: 11.3000\n",
      "Epoch 2337/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5917 - val_loss: 11.3075\n",
      "Epoch 2338/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7454 - val_loss: 11.3226\n",
      "Epoch 2339/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6520 - val_loss: 11.1372\n",
      "Epoch 2340/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7072 - val_loss: 11.3444\n",
      "Epoch 2341/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4375 - val_loss: 11.0713\n",
      "Epoch 2342/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4017 - val_loss: 11.1623\n",
      "Epoch 2343/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4363 - val_loss: 11.2537\n",
      "Epoch 2344/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4269 - val_loss: 11.0765\n",
      "Epoch 2345/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3857 - val_loss: 11.1682\n",
      "Epoch 2346/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5204 - val_loss: 11.3645\n",
      "Epoch 2347/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4484 - val_loss: 11.3609\n",
      "Epoch 2348/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.0135 - val_loss: 11.5062\n",
      "Epoch 2349/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5648 - val_loss: 11.0207\n",
      "Epoch 2350/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4587 - val_loss: 11.0007\n",
      "Epoch 2351/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4896 - val_loss: 11.4055\n",
      "Epoch 2352/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3924 - val_loss: 11.0847\n",
      "Epoch 2353/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4491 - val_loss: 11.1661\n",
      "Epoch 2354/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4459 - val_loss: 11.0883\n",
      "Epoch 2355/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4691 - val_loss: 10.9512\n",
      "Epoch 2356/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3511 - val_loss: 11.1915\n",
      "Epoch 2357/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4494 - val_loss: 11.0710\n",
      "Epoch 2358/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5618 - val_loss: 11.9336\n",
      "Epoch 2359/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5417 - val_loss: 11.2905\n",
      "Epoch 2360/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5581 - val_loss: 11.1851\n",
      "Epoch 2361/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 4.1434 - val_loss: 12.0000\n",
      "Epoch 2362/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8045 - val_loss: 11.0588\n",
      "Epoch 2363/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6076 - val_loss: 11.1341\n",
      "Epoch 2364/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4793 - val_loss: 10.9472\n",
      "Epoch 2365/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4760 - val_loss: 11.0073\n",
      "Epoch 2366/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3515 - val_loss: 11.1643\n",
      "Epoch 2367/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3317 - val_loss: 11.0243\n",
      "Epoch 2368/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3592 - val_loss: 11.4155\n",
      "Epoch 2369/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4308 - val_loss: 11.2972\n",
      "Epoch 2370/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3838 - val_loss: 11.3141\n",
      "Epoch 2371/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4761 - val_loss: 11.0912\n",
      "Epoch 2372/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5929 - val_loss: 11.2527\n",
      "Epoch 2373/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4357 - val_loss: 11.4316\n",
      "Epoch 2374/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3994 - val_loss: 11.0996\n",
      "Epoch 2375/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4708 - val_loss: 11.0031\n",
      "Epoch 2376/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5297 - val_loss: 11.5247\n",
      "Epoch 2377/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5342 - val_loss: 11.1518\n",
      "Epoch 2378/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5074 - val_loss: 11.1486\n",
      "Epoch 2379/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4694 - val_loss: 11.0546\n",
      "Epoch 2380/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5967 - val_loss: 11.3913\n",
      "Epoch 2381/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6058 - val_loss: 11.3430\n",
      "Epoch 2382/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4763 - val_loss: 11.3407\n",
      "Epoch 2383/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5049 - val_loss: 11.2626\n",
      "Epoch 2384/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6191 - val_loss: 11.3328\n",
      "Epoch 2385/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4928 - val_loss: 11.0200\n",
      "Epoch 2386/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4489 - val_loss: 10.9250\n",
      "Epoch 2387/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4915 - val_loss: 11.2080\n",
      "Epoch 2388/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4872 - val_loss: 11.3477\n",
      "Epoch 2389/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4481 - val_loss: 11.1575\n",
      "Epoch 2390/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5622 - val_loss: 11.2662\n",
      "Epoch 2391/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4029 - val_loss: 11.0665\n",
      "Epoch 2392/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4905 - val_loss: 11.2116\n",
      "Epoch 2393/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6111 - val_loss: 11.5859\n",
      "Epoch 2394/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7040 - val_loss: 11.0889\n",
      "Epoch 2395/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4716 - val_loss: 11.2069\n",
      "Epoch 2396/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3899 - val_loss: 11.4913\n",
      "Epoch 2397/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5294 - val_loss: 11.2021\n",
      "Epoch 2398/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3798 - val_loss: 11.0665\n",
      "Epoch 2399/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6323 - val_loss: 11.4285\n",
      "Epoch 2400/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4852 - val_loss: 11.5180\n",
      "Epoch 2401/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5982 - val_loss: 11.1827\n",
      "Epoch 2402/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4497 - val_loss: 11.4903\n",
      "Epoch 2403/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4515 - val_loss: 11.2111\n",
      "Epoch 2404/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3929 - val_loss: 11.7005\n",
      "Epoch 2405/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4911 - val_loss: 11.4029\n",
      "Epoch 2406/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4235 - val_loss: 12.0751\n",
      "Epoch 2407/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.7075 - val_loss: 11.4256\n",
      "Epoch 2408/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5332 - val_loss: 11.1100\n",
      "Epoch 2409/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6793 - val_loss: 10.9766\n",
      "Epoch 2410/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4543 - val_loss: 10.9362\n",
      "Epoch 2411/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4372 - val_loss: 10.7603\n",
      "Epoch 2412/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3750 - val_loss: 11.1270\n",
      "Epoch 2413/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4136 - val_loss: 11.2764\n",
      "Epoch 2414/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5091 - val_loss: 11.0920\n",
      "Epoch 2415/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5470 - val_loss: 11.1617\n",
      "Epoch 2416/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5888 - val_loss: 11.3598\n",
      "Epoch 2417/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4318 - val_loss: 11.1086\n",
      "Epoch 2418/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3803 - val_loss: 10.8852\n",
      "Epoch 2419/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5629 - val_loss: 11.2076\n",
      "Epoch 2420/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3717 - val_loss: 11.0377\n",
      "Epoch 2421/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4263 - val_loss: 11.1300\n",
      "Epoch 2422/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4380 - val_loss: 11.1329\n",
      "Epoch 2423/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5382 - val_loss: 10.9912\n",
      "Epoch 2424/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5457 - val_loss: 11.3851\n",
      "Epoch 2425/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4865 - val_loss: 11.4377\n",
      "Epoch 2426/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5035 - val_loss: 11.1056\n",
      "Epoch 2427/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5728 - val_loss: 11.4664\n",
      "Epoch 2428/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4481 - val_loss: 11.3124\n",
      "Epoch 2429/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6172 - val_loss: 12.1935\n",
      "Epoch 2430/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4496 - val_loss: 11.1991\n",
      "Epoch 2431/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5561 - val_loss: 12.1681\n",
      "Epoch 2432/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.9901 - val_loss: 11.2594\n",
      "Epoch 2433/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6406 - val_loss: 11.3017\n",
      "Epoch 2434/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4896 - val_loss: 11.2260\n",
      "Epoch 2435/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3467 - val_loss: 11.2257\n",
      "Epoch 2436/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6373 - val_loss: 11.9106\n",
      "Epoch 2437/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4260 - val_loss: 11.2510\n",
      "Epoch 2438/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4689 - val_loss: 12.1257\n",
      "Epoch 2439/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5214 - val_loss: 10.9841\n",
      "Epoch 2440/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4557 - val_loss: 11.0884\n",
      "Epoch 2441/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3646 - val_loss: 11.2970\n",
      "Epoch 2442/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5503 - val_loss: 11.0071\n",
      "Epoch 2443/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3553 - val_loss: 11.1792\n",
      "Epoch 2444/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4335 - val_loss: 11.2698\n",
      "Epoch 2445/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5227 - val_loss: 11.4932\n",
      "Epoch 2446/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3631 - val_loss: 11.0756\n",
      "Epoch 2447/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4444 - val_loss: 11.2130\n",
      "Epoch 2448/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3700 - val_loss: 10.9874\n",
      "Epoch 2449/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.7484 - val_loss: 11.1495\n",
      "Epoch 2450/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5813 - val_loss: 11.1774\n",
      "Epoch 2451/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5503 - val_loss: 11.1925\n",
      "Epoch 2452/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3697 - val_loss: 11.6231\n",
      "Epoch 2453/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6072 - val_loss: 11.4808\n",
      "Epoch 2454/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6230 - val_loss: 11.1817\n",
      "Epoch 2455/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4487 - val_loss: 11.0908\n",
      "Epoch 2456/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3750 - val_loss: 11.3012\n",
      "Epoch 2457/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5232 - val_loss: 11.4464\n",
      "Epoch 2458/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5505 - val_loss: 11.0342\n",
      "Epoch 2459/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4444 - val_loss: 11.2968\n",
      "Epoch 2460/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4536 - val_loss: 11.2075\n",
      "Epoch 2461/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6215 - val_loss: 11.6304\n",
      "Epoch 2462/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5496 - val_loss: 11.4496\n",
      "Epoch 2463/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 3.4254 - val_loss: 12.5526\n",
      "Epoch 2464/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6458 - val_loss: 11.1475\n",
      "Epoch 2465/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3264 - val_loss: 11.3383\n",
      "Epoch 2466/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5486 - val_loss: 11.1862\n",
      "Epoch 2467/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3579 - val_loss: 11.1771\n",
      "Epoch 2468/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3657 - val_loss: 11.2607\n",
      "Epoch 2469/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3883 - val_loss: 10.9367\n",
      "Epoch 2470/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3766 - val_loss: 11.2232\n",
      "Epoch 2471/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4195 - val_loss: 11.1882\n",
      "Epoch 2472/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4248 - val_loss: 11.1870\n",
      "Epoch 2473/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4840 - val_loss: 12.1113\n",
      "Epoch 2474/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5878 - val_loss: 11.4391\n",
      "Epoch 2475/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4885 - val_loss: 12.3519\n",
      "Epoch 2476/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5660 - val_loss: 11.1901\n",
      "Epoch 2477/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5343 - val_loss: 11.2364\n",
      "Epoch 2478/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3919 - val_loss: 11.2133\n",
      "Epoch 2479/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.2970 - val_loss: 11.5260\n",
      "Epoch 2480/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6746 - val_loss: 11.1992\n",
      "Epoch 2481/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6216 - val_loss: 11.1335\n",
      "Epoch 2482/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3824 - val_loss: 11.4953\n",
      "Epoch 2483/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4791 - val_loss: 11.1493\n",
      "Epoch 2484/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4311 - val_loss: 11.6542\n",
      "Epoch 2485/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4615 - val_loss: 11.2836\n",
      "Epoch 2486/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4130 - val_loss: 11.1793\n",
      "Epoch 2487/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6852 - val_loss: 11.9742\n",
      "Epoch 2488/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4908 - val_loss: 11.2731\n",
      "Epoch 2489/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3500 - val_loss: 11.2724\n",
      "Epoch 2490/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.6310 - val_loss: 11.3931\n",
      "Epoch 2491/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4404 - val_loss: 11.1608\n",
      "Epoch 2492/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4255 - val_loss: 11.5603\n",
      "Epoch 2493/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4611 - val_loss: 11.1785\n",
      "Epoch 2494/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.5062 - val_loss: 11.3178\n",
      "Epoch 2495/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4177 - val_loss: 11.6548\n",
      "Epoch 2496/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.3552 - val_loss: 11.6606\n",
      "Epoch 2497/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4863 - val_loss: 11.2693\n",
      "Epoch 2498/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4396 - val_loss: 11.4612\n",
      "Epoch 2499/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.8744 - val_loss: 11.4379\n",
      "Epoch 2500/2500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 2.4571 - val_loss: 11.2933\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=\"mae\",)\n",
    "\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=epoch_size,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, test_labels)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history: dict = history.history\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "l = int(len(loss) / 10)\n",
    "history['loss'] = loss[l:]\n",
    "history['val_loss'] = val_loss[l:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [5.109316825866699, 4.886207103729248, 4.667881965637207, 4.766632556915283, 5.177592754364014, 5.253681659698486, 4.618587017059326, 5.409010887145996, 4.685188293457031, 4.68617582321167, 5.81366491317749, 5.174757480621338, 4.78563117980957, 4.5824384689331055, 4.9947710037231445, 5.089235782623291, 4.947103977203369, 4.7006754875183105, 4.567907810211182, 5.129599571228027, 4.726390838623047, 5.1031012535095215, 4.92239236831665, 4.658093452453613, 4.510387420654297, 5.210574150085449, 4.652991771697998, 4.940923690795898, 4.687182903289795, 4.948368072509766, 5.019304275512695, 5.125141620635986, 5.09778356552124, 5.1739325523376465, 4.757465362548828, 4.558849811553955, 4.806295871734619, 4.818117618560791, 4.719101428985596, 4.3864545822143555, 4.37719202041626, 4.939942359924316, 4.732608795166016, 4.948021411895752, 5.325736045837402, 4.90155029296875, 4.539146900177002, 4.842566967010498, 4.7829909324646, 4.392287254333496, 4.598748207092285, 4.499499320983887, 4.7583088874816895, 4.805360317230225, 4.843163967132568, 4.658045291900635, 4.4822516441345215, 4.446427822113037, 4.521697521209717, 4.605552673339844, 4.7919416427612305, 4.538018703460693, 4.40148401260376, 4.67838191986084, 4.623287677764893, 4.371270179748535, 4.6193695068359375, 4.352802276611328, 4.2233147621154785, 4.6946892738342285, 4.922086715698242, 4.7789788246154785, 4.507721900939941, 4.600780487060547, 4.310849666595459, 4.5589399337768555, 4.305576801300049, 4.515731334686279, 4.683394908905029, 4.7013258934021, 4.3923845291137695, 4.457424163818359, 4.093263149261475, 4.207886695861816, 4.372035980224609, 5.036703109741211, 4.992602348327637, 4.435846328735352, 4.014342784881592, 4.0899977684021, 4.639439582824707, 4.161341190338135, 4.543578147888184, 4.64603328704834, 4.873916149139404, 3.9683163166046143, 4.200587272644043, 4.285515308380127, 4.249000072479248, 4.408350944519043, 4.435213565826416, 4.563267707824707, 4.131974220275879, 4.164644241333008, 4.392238140106201, 4.714964866638184, 4.405282020568848, 4.6317267417907715, 4.146892547607422, 4.139392375946045, 4.536025047302246, 4.232786178588867, 4.169714450836182, 4.626783847808838, 4.6479902267456055, 4.332822799682617, 4.414331436157227, 4.3825860023498535, 4.039837837219238, 4.601625919342041, 4.809872627258301, 4.410114288330078, 4.154408931732178, 4.477069854736328, 4.530389785766602, 3.8596155643463135, 4.24093770980835, 4.477634906768799, 4.600480079650879, 4.100383281707764, 4.019962787628174, 4.244494438171387, 4.015530109405518, 4.598569869995117, 4.078340530395508, 4.32988977432251, 4.287728786468506, 4.042444705963135, 3.9864003658294678, 3.9978692531585693, 4.312823295593262, 4.2217512130737305, 4.465407371520996, 3.8412210941314697, 4.1513872146606445, 4.178732395172119, 4.436009883880615, 4.059686183929443, 3.930574417114258, 3.9020464420318604, 4.541901111602783, 4.395825386047363, 3.906430959701538, 3.9441945552825928, 3.9919402599334717, 4.431509494781494, 4.746903419494629, 4.1192450523376465, 3.6850249767303467, 4.1534037590026855, 4.319902420043945, 4.469601631164551, 4.560433864593506, 3.8567087650299072, 4.214015007019043, 4.0459370613098145, 3.9223086833953857, 3.826704978942871, 4.26042366027832, 3.9093775749206543, 4.465476036071777, 3.858992099761963, 3.890061616897583, 4.125695705413818, 3.895986557006836, 4.19405460357666, 3.955747127532959, 3.943728446960449, 3.800816297531128, 4.031271934509277, 3.908597469329834, 4.179596424102783, 4.783666610717773, 3.9578518867492676, 4.0141754150390625, 3.7791147232055664, 3.9490344524383545, 4.194126605987549, 4.004611015319824, 3.855003595352173, 4.162304401397705, 4.517716407775879, 4.087159633636475, 3.8100247383117676, 4.0221638679504395, 3.8248672485351562, 3.8276619911193848, 3.9094126224517822, 4.2319416999816895, 3.9531195163726807, 4.509926795959473, 3.905850410461426, 3.6187710762023926, 4.011082649230957, 3.944350004196167, 3.839492082595825, 3.7369418144226074, 4.489406108856201, 4.0591139793396, 3.83917498588562, 3.764767646789551, 3.8426015377044678, 3.8971168994903564, 3.7418744564056396, 3.807837963104248, 4.189720630645752, 4.123987197875977, 4.580386161804199, 4.232186317443848, 3.7364728450775146, 3.625072717666626, 3.6755871772766113, 3.6884517669677734, 4.039700031280518, 4.030198574066162, 3.868610382080078, 4.174561977386475, 3.8779067993164062, 4.0625739097595215, 4.129400253295898, 3.8386073112487793, 3.6986188888549805, 3.709975481033325, 3.920159101486206, 3.9823036193847656, 3.5796186923980713, 3.8596367835998535, 3.7512857913970947, 4.209417343139648, 3.8552119731903076, 4.070773124694824, 3.7568740844726562, 3.759248971939087, 4.044729709625244, 4.131555557250977, 3.8825972080230713, 4.101481914520264, 3.7044284343719482, 3.7692067623138428, 3.812490463256836, 3.748225450515747, 3.9330191612243652, 3.829834222793579, 3.8226122856140137, 3.9898674488067627, 3.6131248474121094, 4.048973083496094, 4.210245132446289, 3.9991776943206787, 3.6627721786499023, 3.789215564727783, 3.7193877696990967, 3.668395757675171, 3.7624764442443848, 4.039768695831299, 3.616546630859375, 3.612133502960205, 3.8206961154937744, 4.151939868927002, 3.9151902198791504, 3.7244465351104736, 3.7438831329345703, 3.776245594024658, 3.8675649166107178, 3.928664207458496, 3.718639373779297, 3.7914388179779053, 3.5576064586639404, 3.8238391876220703, 3.9797635078430176, 4.000683307647705, 3.6605513095855713, 3.64411997795105, 3.617579936981201, 3.9029440879821777, 3.5141689777374268, 3.8271443843841553, 3.6889848709106445, 3.759568929672241, 4.154326438903809, 3.9487903118133545, 3.606807231903076, 3.5856680870056152, 3.4745805263519287, 3.939702033996582, 3.521869421005249, 3.567478895187378, 4.024571895599365, 3.5000362396240234, 3.6672914028167725, 3.7354533672332764, 3.53291654586792, 3.808184862136841, 3.9316859245300293, 3.703361988067627, 3.668490171432495, 3.6298987865448, 3.557123899459839, 3.675082206726074, 3.819790840148926, 3.894224166870117, 3.749209403991699, 4.114679336547852, 3.781355857849121, 3.4171879291534424, 3.337766170501709, 3.619586229324341, 3.439995527267456, 3.5865230560302734, 3.9430623054504395, 3.5123581886291504, 4.299273490905762, 3.609707832336426, 3.5863962173461914, 3.5216143131256104, 3.921875, 3.4530792236328125, 3.5217597484588623, 3.7310104370117188, 3.6192331314086914, 3.7417922019958496, 3.921358823776245, 3.4672069549560547, 3.4640235900878906, 3.525315046310425, 3.932307481765747, 3.610673427581787, 3.653905153274536, 3.74396014213562, 3.366804599761963, 3.533665895462036, 3.563443422317505, 3.579393148422241, 3.495781898498535, 4.2008233070373535, 3.4221644401550293, 3.946155071258545, 3.3218088150024414, 3.982372522354126, 3.4890480041503906, 3.836071729660034, 3.7784998416900635, 3.3211991786956787, 3.72131609916687, 3.7838826179504395, 3.3720691204071045, 3.618496894836426, 3.3688273429870605, 3.4768874645233154, 3.4537148475646973, 3.370511770248413, 3.5163135528564453, 3.4962785243988037, 4.155938148498535, 3.6523869037628174, 3.459176778793335, 3.4136557579040527, 3.848829984664917, 3.557818651199341, 3.5864903926849365, 3.5768113136291504, 3.8559610843658447, 3.733675956726074, 3.268766403198242, 3.5248968601226807, 3.7655158042907715, 3.538870334625244, 3.51635479927063, 3.583329677581787, 3.453920602798462, 3.6864585876464844, 3.941988229751587, 3.3411614894866943, 3.8343427181243896, 3.544001340866089, 3.3109166622161865, 3.652454376220703, 3.6287336349487305, 3.3714704513549805, 3.724015951156616, 3.8270013332366943, 3.401698589324951, 3.424475908279419, 3.376316547393799, 3.6667890548706055, 3.7732579708099365, 3.3266654014587402, 3.4545209407806396, 3.353562355041504, 3.6044199466705322, 3.3430888652801514, 4.008256435394287, 3.881200075149536, 3.273008346557617, 3.3263540267944336, 3.442021131515503, 3.187833786010742, 3.581709861755371, 3.8744609355926514, 3.311391592025757, 3.436044216156006, 3.35914945602417, 3.6979408264160156, 3.541301965713501, 3.393198251724243, 3.702285051345825, 3.6542515754699707, 3.774122476577759, 3.3127782344818115, 3.43800950050354, 3.566838026046753, 3.4551050662994385, 3.511702537536621, 3.2651796340942383, 3.662309408187866, 3.8512980937957764, 3.481813907623291, 3.5711405277252197, 3.6629245281219482, 3.838275909423828, 3.3673882484436035, 3.4244701862335205, 3.343601703643799, 3.348402976989746, 3.4968976974487305, 3.2718749046325684, 3.8069305419921875, 3.415388584136963, 3.6111960411071777, 3.4155588150024414, 3.4716131687164307, 3.3094534873962402, 3.868453025817871, 3.359344720840454, 3.3614351749420166, 3.628868579864502, 3.33128023147583, 3.4598233699798584, 3.4205994606018066, 3.5567550659179688, 3.520873546600342, 3.390563726425171, 3.4058122634887695, 3.2842020988464355, 3.2889857292175293, 3.772771120071411, 3.438417911529541, 3.3241610527038574, 3.7909252643585205, 3.245600461959839, 3.399120330810547, 3.592419385910034, 3.4628095626831055, 3.6871776580810547, 3.2855162620544434, 3.5621509552001953, 3.450234889984131, 3.5860533714294434, 3.250694513320923, 3.293776035308838, 3.3707468509674072, 3.2901647090911865, 3.2127349376678467, 3.5070369243621826, 3.713460922241211, 3.936049699783325, 3.238434076309204, 3.4261813163757324, 3.500640630722046, 3.444498062133789, 3.513698101043701, 3.0987510681152344, 3.6318359375, 3.5432562828063965, 3.253323554992676, 3.3016724586486816, 3.8077523708343506, 3.440373420715332, 3.4116599559783936, 3.3664844036102295, 3.5455939769744873, 3.4985058307647705, 3.1671676635742188, 3.1725168228149414, 3.344547986984253, 3.4636166095733643, 3.4469144344329834, 3.3674075603485107, 3.360936403274536, 3.511336088180542, 3.618614912033081, 3.3610427379608154, 3.3274166584014893, 3.260425567626953, 3.039707899093628, 3.311983108520508, 3.6141068935394287, 3.2218105792999268, 3.450960159301758, 3.2434027194976807, 3.8127593994140625, 3.478867292404175, 3.5311951637268066, 3.4886891841888428, 3.1884195804595947, 3.439789295196533, 3.295060634613037, 3.387463092803955, 3.3441152572631836, 3.205212116241455, 3.2542643547058105, 3.598165988922119, 3.3961846828460693, 3.298065662384033, 3.5677802562713623, 3.1411657333374023, 3.498540163040161, 3.5228474140167236, 3.5174262523651123, 3.2907707691192627, 3.1400187015533447, 3.299887180328369, 3.395674228668213, 3.160144567489624, 3.3777332305908203, 3.5670053958892822, 3.647742748260498, 3.579256772994995, 3.0969295501708984, 3.142024040222168, 3.5602879524230957, 3.487356662750244, 3.4284558296203613, 3.1752512454986572, 3.216585397720337, 3.3371078968048096, 3.4363553524017334, 3.2198269367218018, 3.545140027999878, 3.2899250984191895, 3.1592559814453125, 3.707951545715332, 3.5204405784606934, 3.2292656898498535, 3.2366464138031006, 3.170487642288208, 3.1850531101226807, 3.7336864471435547, 3.4403862953186035, 3.247488260269165, 3.2751951217651367, 3.301663637161255, 3.3228063583374023, 3.351099729537964, 3.4815425872802734, 3.1271297931671143, 3.683115005493164, 3.038015604019165, 3.0673136711120605, 3.0695700645446777, 3.645143508911133, 3.4615020751953125, 3.4286251068115234, 3.0329813957214355, 3.169386625289917, 3.385040044784546, 3.271299362182617, 3.13232159614563, 3.3239827156066895, 3.1378235816955566, 3.1517598628997803, 3.3077118396759033, 3.816056728363037, 3.1591732501983643, 3.014652967453003, 3.0843253135681152, 3.145005702972412, 3.5120699405670166, 3.188108205795288, 3.4117369651794434, 3.4812381267547607, 3.1834702491760254, 3.3345346450805664, 3.2342066764831543, 3.117563009262085, 3.062321901321411, 3.1315910816192627, 3.109762668609619, 3.32102370262146, 3.1252822875976562, 3.1741790771484375, 3.0876564979553223, 3.120943784713745, 3.1752994060516357, 3.1372640132904053, 3.6782541275024414, 4.333041191101074, 3.4821937084198, 3.0871450901031494, 3.2694830894470215, 3.5494954586029053, 3.1882708072662354, 3.0297772884368896, 3.258183002471924, 3.3316562175750732, 3.033330202102661, 3.2245900630950928, 3.32320499420166, 3.164158821105957, 3.362903118133545, 3.206129312515259, 3.2975289821624756, 2.956057548522949, 3.1653621196746826, 3.067587375640869, 3.1260428428649902, 3.2415690422058105, 3.2360987663269043, 3.166557788848877, 3.4478919506073, 3.0990099906921387, 3.103936195373535, 3.3017489910125732, 3.204636573791504, 3.0771660804748535, 3.1830196380615234, 3.3851161003112793, 3.3149807453155518, 3.2582101821899414, 3.2680504322052, 3.070462703704834, 3.0203945636749268, 3.155869960784912, 3.141951322555542, 3.154299020767212, 3.145151376724243, 3.0988199710845947, 3.1222083568573, 3.3798792362213135, 3.403951644897461, 3.541548728942871, 3.145214080810547, 3.4666337966918945, 3.1209781169891357, 3.0544590950012207, 3.27459716796875, 3.1750905513763428, 2.9251275062561035, 3.197864055633545, 2.8897039890289307, 3.369054079055786, 3.0726592540740967, 2.972400188446045, 3.6155261993408203, 2.9750266075134277, 3.0239524841308594, 3.091697931289673, 3.1631548404693604, 3.413440704345703, 3.279770612716675, 2.9421350955963135, 2.999352216720581, 3.1004672050476074, 3.2289319038391113, 2.9749386310577393, 3.2314348220825195, 3.26112961769104, 3.1088550090789795, 3.2282049655914307, 3.3560376167297363, 3.3178820610046387, 3.0456392765045166, 3.0641489028930664, 3.3657703399658203, 2.9979376792907715, 3.0193874835968018, 3.007129669189453, 3.145425319671631, 2.923877000808716, 3.053985834121704, 3.1718733310699463, 3.1420159339904785, 3.017408609390259, 3.3275961875915527, 3.052778720855713, 3.086615562438965, 3.080721855163574, 3.3050177097320557, 2.9459400177001953, 3.2258715629577637, 3.463223934173584, 3.1152918338775635, 3.1790661811828613, 2.9432647228240967, 2.861929416656494, 3.3811862468719482, 3.1386566162109375, 3.1597321033477783, 2.9854896068573, 3.268749475479126, 2.977771282196045, 2.9848837852478027, 3.0916342735290527, 3.3054888248443604, 2.9781763553619385, 3.081303358078003, 2.8671414852142334, 3.1873843669891357, 3.297001838684082, 3.1815667152404785, 3.210590362548828, 2.977123498916626, 3.2393763065338135, 3.12701678276062, 3.3397133350372314, 3.0942342281341553, 2.9786007404327393, 2.9517223834991455, 3.266421318054199, 3.330669403076172, 3.3095736503601074, 3.118055820465088, 3.064786911010742, 2.9657139778137207, 3.2944388389587402, 3.0652577877044678, 3.0035383701324463, 2.8988537788391113, 3.0089268684387207, 3.06007719039917, 2.97355055809021, 3.5233561992645264, 2.971482992172241, 3.2400779724121094, 2.87713360786438, 2.8832550048828125, 3.0201385021209717, 2.883652687072754, 3.1762044429779053, 3.014510154724121, 2.917836904525757, 3.1993062496185303, 3.012491226196289, 2.9107580184936523, 3.3086724281311035, 2.929314374923706, 2.903441905975342, 3.1934010982513428, 3.053877115249634, 3.0243380069732666, 3.1946756839752197, 3.2439656257629395, 3.9244377613067627, 3.168020725250244, 3.057461977005005, 3.0716769695281982, 2.973015308380127, 3.1087567806243896, 3.009366273880005, 2.8988094329833984, 3.0041167736053467, 2.8925182819366455, 2.935032844543457, 3.2421634197235107, 3.090883255004883, 3.2576076984405518, 3.4887301921844482, 3.074064254760742, 2.986879587173462, 2.977940082550049, 3.206094980239868, 2.8431930541992188, 3.070035219192505, 2.8902828693389893, 3.166226625442505, 2.9543938636779785, 3.04522442817688, 3.1560287475585938, 2.950773239135742, 2.924281358718872, 2.985692262649536, 3.1369683742523193, 3.234379291534424, 3.0129482746124268, 3.1516220569610596, 3.037799119949341, 3.2872073650360107, 2.9054112434387207, 2.812987804412842, 3.0269250869750977, 3.028611898422241, 3.041283369064331, 2.950552463531494, 3.0276904106140137, 2.856801986694336, 2.8510775566101074, 3.2763750553131104, 2.979170560836792, 2.906613349914551, 3.0238983631134033, 3.020470380783081, 2.965254545211792, 3.2354941368103027, 3.0382766723632812, 3.2190423011779785, 3.0290164947509766, 2.9515023231506348, 3.6142449378967285, 3.4127888679504395, 3.2620370388031006, 3.19398832321167, 2.789137125015259, 2.9648118019104004, 3.0567116737365723, 2.9263882637023926, 2.9931178092956543, 3.0316803455352783, 2.824268341064453, 2.8431596755981445, 3.197951555252075, 2.8855772018432617, 2.918073892593384, 2.8359742164611816, 2.9357857704162598, 3.15069842338562, 3.0953946113586426, 3.185842990875244, 2.8811275959014893, 3.1471219062805176, 2.888760566711426, 2.8938446044921875, 3.125805139541626, 2.829014539718628, 2.8896708488464355, 2.839853286743164, 3.0652823448181152, 3.0328664779663086, 3.0935451984405518, 2.8677802085876465, 2.9101991653442383, 2.8059725761413574, 2.852285385131836, 3.1477293968200684, 3.251852035522461, 2.9354286193847656, 2.8424246311187744, 3.132086753845215, 2.910693883895874, 3.0457913875579834, 3.121058464050293, 3.223407030105591, 2.961066246032715, 2.8607664108276367, 2.9016973972320557, 2.9891843795776367, 3.2825562953948975, 3.253140687942505, 2.7907369136810303, 2.869887351989746, 2.8941516876220703, 2.952700614929199, 2.976461887359619, 3.1206698417663574, 2.971522808074951, 2.787569046020508, 3.151630163192749, 2.944612741470337, 3.012141227722168, 2.870161771774292, 2.900242567062378, 3.023984909057617, 3.108492136001587, 3.015984296798706, 2.837697744369507, 3.1291630268096924, 2.9842300415039062, 2.810426712036133, 3.032860040664673, 3.0075650215148926, 2.7394914627075195, 2.95534348487854, 2.9206840991973877, 2.8643834590911865, 3.2071263790130615, 2.905557632446289, 2.856628894805908, 3.267238140106201, 3.32598876953125, 3.0547268390655518, 2.9792282581329346, 2.7395877838134766, 2.917935371398926, 2.929299831390381, 3.11569881439209, 2.9953811168670654, 2.733182191848755, 2.7456533908843994, 3.0325677394866943, 2.9466898441314697, 2.920078754425049, 2.995246648788452, 2.9203763008117676, 2.8479506969451904, 2.8976073265075684, 3.1883785724639893, 3.0841004848480225, 2.7477495670318604, 2.9068005084991455, 2.814418077468872, 3.161170721054077, 2.8285915851593018, 2.962803363800049, 2.8680269718170166, 3.161271810531616, 2.7739336490631104, 2.7433583736419678, 2.886476993560791, 3.1671111583709717, 2.8606369495391846, 2.9326250553131104, 2.7819747924804688, 3.041533946990967, 2.948312520980835, 3.254495620727539, 3.1217215061187744, 2.8214094638824463, 3.0063648223876953, 2.776996612548828, 2.768601417541504, 2.829531192779541, 2.8195950984954834, 2.7579147815704346, 2.83152437210083, 3.162317991256714, 2.922640085220337, 3.17497181892395, 2.807889461517334, 2.752328634262085, 2.783750534057617, 2.9665589332580566, 2.938504457473755, 2.826014518737793, 3.0025634765625, 2.8406448364257812, 2.8568508625030518, 3.1220595836639404, 2.9336509704589844, 2.9449305534362793, 2.9635586738586426, 2.7854185104370117, 2.717785596847534, 2.763484477996826, 2.8182992935180664, 2.875361919403076, 2.930626630783081, 4.287379741668701, 3.361025810241699, 3.2363474369049072, 2.6964128017425537, 3.0348503589630127, 2.9288108348846436, 2.8104677200317383, 2.67492938041687, 2.6745986938476562, 2.7254154682159424, 2.727370023727417, 2.649716854095459, 3.2021234035491943, 2.9716269969940186, 2.8650662899017334, 2.726680040359497, 2.8221168518066406, 2.8980674743652344, 2.9205946922302246, 2.731168270111084, 2.7545652389526367, 2.9089953899383545, 3.0988266468048096, 3.0638508796691895, 3.229526996612549, 2.857481002807617, 2.746324300765991, 2.6875710487365723, 3.001800298690796, 3.1926021575927734, 2.719306230545044, 2.971675395965576, 2.9181411266326904, 2.851815938949585, 2.9478342533111572, 2.781224489212036, 2.908234119415283, 2.8429601192474365, 2.9286081790924072, 2.792341470718384, 2.819715738296509, 2.8704636096954346, 3.127974271774292, 2.926677703857422, 2.725491523742676, 2.8307647705078125, 3.1348717212677, 2.854560136795044, 2.80387020111084, 3.007936477661133, 2.6970863342285156, 2.963707208633423, 2.895946741104126, 2.8523876667022705, 2.681102752685547, 2.715226411819458, 2.800434112548828, 2.7940261363983154, 2.837280511856079, 3.4455337524414062, 2.9217324256896973, 2.671466112136841, 3.0671374797821045, 2.9331905841827393, 2.842646598815918, 2.741724729537964, 2.6961112022399902, 2.864924430847168, 3.1701040267944336, 2.723306894302368, 2.762718439102173, 2.899604558944702, 2.898267984390259, 2.9008073806762695, 3.127552032470703, 2.866318464279175, 2.968843460083008, 2.845534086227417, 2.788768768310547, 3.530609369277954, 3.027275323867798, 2.858093500137329, 2.814936637878418, 2.779907464981079, 2.6723904609680176, 2.722715139389038, 2.7692484855651855, 2.6942639350891113, 2.799652099609375, 2.6639292240142822, 2.7944366931915283, 2.7754461765289307, 2.858363628387451, 2.9380078315734863, 2.8368687629699707, 2.762998104095459, 2.8641324043273926, 2.7473137378692627, 2.9914276599884033, 2.817063570022583, 2.8264806270599365, 3.0822594165802, 2.7034261226654053, 2.881882429122925, 2.7944958209991455, 2.9779069423675537, 2.9356346130371094, 2.7211086750030518, 2.9178950786590576, 2.817258596420288, 2.9211175441741943, 2.833890199661255, 2.722428798675537, 3.112760543823242, 2.839775323867798, 2.7685887813568115, 2.816101551055908, 2.828716278076172, 2.9787399768829346, 3.105243444442749, 2.829650640487671, 2.951988697052002, 2.8794569969177246, 2.6586453914642334, 2.8456146717071533, 3.1637072563171387, 3.722842216491699, 2.7002053260803223, 2.8933706283569336, 2.9302639961242676, 2.830866813659668, 2.8641064167022705, 2.8663904666900635, 2.7772014141082764, 2.891772747039795, 2.617833137512207, 2.9795334339141846, 2.6948482990264893, 2.6566784381866455, 2.8363945484161377, 2.875779390335083, 2.7315833568573, 2.7485263347625732, 2.820068836212158, 2.7281453609466553, 2.8319058418273926, 3.0367074012756348, 2.780017614364624, 2.911327600479126, 2.729799270629883, 2.851557731628418, 2.930067777633667, 2.643761396408081, 2.9258453845977783, 2.863274335861206, 2.7982306480407715, 2.8042490482330322, 2.883535623550415, 2.926898241043091, 2.8976504802703857, 2.6565191745758057, 2.8638322353363037, 2.774040699005127, 2.948927640914917, 3.031740188598633, 2.69781494140625, 2.8838260173797607, 3.046030044555664, 2.679152250289917, 2.5737719535827637, 2.681161403656006, 2.624282121658325, 2.7664530277252197, 2.678626298904419, 2.9327714443206787, 2.941659927368164, 2.7582168579101562, 2.9596164226531982, 2.767991304397583, 2.889557361602783, 2.708178758621216, 2.7170190811157227, 2.7534706592559814, 2.7578048706054688, 2.885528087615967, 2.7022197246551514, 2.6972625255584717, 3.187504768371582, 2.8449504375457764, 2.803746461868286, 2.8725948333740234, 2.762631893157959, 2.7309064865112305, 3.039462089538574, 2.9655771255493164, 2.8603150844573975, 2.667090654373169, 2.647226095199585, 2.640976667404175, 2.8039941787719727, 2.7151899337768555, 2.865987539291382, 2.7810721397399902, 3.0036861896514893, 2.8253185749053955, 2.6469907760620117, 2.7180440425872803, 2.9574332237243652, 2.708787441253662, 2.890793800354004, 2.823859691619873, 2.753558397293091, 2.9369747638702393, 2.710488796234131, 2.699692726135254, 2.8149123191833496, 2.8247385025024414, 3.057466983795166, 2.820089817047119, 2.6716456413269043, 2.8852484226226807, 2.5661356449127197, 2.8787879943847656, 2.7159619331359863, 2.8431308269500732, 2.8612709045410156, 2.7878479957580566, 2.686044692993164, 2.6108975410461426, 2.982679605484009, 2.7246482372283936, 2.878265857696533, 2.8173000812530518, 2.9007606506347656, 2.824720859527588, 2.8459975719451904, 2.710477590560913, 2.6963188648223877, 2.673750400543213, 3.2359774112701416, 3.0587964057922363, 2.8258378505706787, 2.7955679893493652, 2.6133854389190674, 3.0036392211914062, 2.632364511489868, 2.6907541751861572, 2.6618738174438477, 2.9469306468963623, 2.8780126571655273, 2.7512638568878174, 2.8097047805786133, 2.724095106124878, 2.997905731201172, 2.657482385635376, 2.7654597759246826, 2.717611074447632, 2.676283359527588, 2.7673678398132324, 2.784761667251587, 2.6176915168762207, 2.789947986602783, 2.8497161865234375, 2.7847931385040283, 2.683722734451294, 2.7289373874664307, 2.68284273147583, 3.1996452808380127, 2.9427402019500732, 2.8673925399780273, 2.591470241546631, 2.6142802238464355, 2.8443164825439453, 2.8503103256225586, 2.7320892810821533, 2.6127259731292725, 4.210782051086426, 3.03397274017334, 2.6591508388519287, 2.620657205581665, 2.7348108291625977, 2.6127657890319824, 2.8626341819763184, 2.7081098556518555, 2.5662331581115723, 2.682681083679199, 2.6039984226226807, 2.726876735687256, 2.8003780841827393, 2.6053647994995117, 2.6540632247924805, 2.673004627227783, 3.064497709274292, 2.756406545639038, 2.8531923294067383, 3.1611104011535645, 2.86397385597229, 2.786031484603882, 2.570984363555908, 2.687807083129883, 2.812135696411133, 2.698248863220215, 3.011681318283081, 2.704822540283203, 2.6762421131134033, 2.8064396381378174, 2.9128708839416504, 2.719485282897949, 2.770390033721924, 2.7447638511657715, 2.580409288406372, 3.0282559394836426, 2.5617568492889404, 2.8395769596099854, 2.6709890365600586, 2.75531005859375, 2.774101495742798, 2.7426857948303223, 2.6490001678466797, 2.960949420928955, 2.5946359634399414, 2.745537519454956, 2.8729896545410156, 2.694164276123047, 2.8303847312927246, 2.78896427154541, 2.6213643550872803, 2.7015771865844727, 2.775117874145508, 2.707733392715454, 2.6669862270355225, 2.632755994796753, 2.8836982250213623, 2.5375547409057617, 2.748168468475342, 2.6842777729034424, 2.6655595302581787, 2.7248432636260986, 2.684708595275879, 2.9905383586883545, 2.8195784091949463, 2.6636993885040283, 2.9261577129364014, 2.5473363399505615, 2.7341556549072266, 2.8921217918395996, 2.8103396892547607, 2.8290295600891113, 2.7882094383239746, 2.6927993297576904, 2.8136565685272217, 2.5006308555603027, 2.6354243755340576, 2.8221535682678223, 2.599616289138794, 2.745030164718628, 2.695094108581543, 2.7832891941070557, 2.831697702407837, 3.7488315105438232, 3.070605993270874, 2.798119306564331, 2.6462700366973877, 2.816218137741089, 2.6984314918518066, 2.742062568664551, 2.7844581604003906, 2.5495381355285645, 2.612114429473877, 2.6826257705688477, 2.713228464126587, 2.794210433959961, 3.1203863620758057, 2.864309310913086, 2.558074474334717, 2.669677972793579, 2.766247034072876, 2.6415631771087646, 2.5771543979644775, 2.943605661392212, 2.7915074825286865, 2.5365991592407227, 3.4339118003845215, 3.023261308670044, 2.6501688957214355, 2.80820631980896, 2.525078058242798, 2.8181416988372803, 2.6012802124023438, 2.582606315612793, 2.768425226211548, 2.754042863845825, 2.776475429534912, 2.5200021266937256, 2.5638461112976074, 2.7546279430389404, 2.782036781311035, 2.7072577476501465, 2.560655355453491, 2.7473580837249756, 2.7213895320892334, 2.6874070167541504, 2.649644374847412, 2.5821375846862793, 2.625417709350586, 2.7355144023895264, 2.6738128662109375, 2.807218551635742, 2.7202014923095703, 2.7935800552368164, 2.7342169284820557, 2.895616054534912, 2.5763251781463623, 2.6281089782714844, 2.5797836780548096, 2.737882375717163, 2.5629568099975586, 2.6084492206573486, 2.857698917388916, 3.077390432357788, 2.7649636268615723, 2.55353045463562, 2.527233123779297, 2.639819622039795, 2.808128595352173, 2.628019094467163, 2.6852312088012695, 2.611403703689575, 2.7669262886047363, 2.6825807094573975, 2.828176259994507, 2.627241611480713, 2.6316606998443604, 2.7982258796691895, 2.632258653640747, 2.8177316188812256, 2.8631255626678467, 2.691739082336426, 2.5612995624542236, 2.7194621562957764, 2.849093437194824, 2.905311346054077, 2.6737287044525146, 2.6830382347106934, 2.7309863567352295, 3.0839133262634277, 2.7921128273010254, 2.611163377761841, 2.711972713470459, 2.5526700019836426, 2.5023391246795654, 2.690000295639038, 2.831115484237671, 2.5878145694732666, 2.797372817993164, 2.6356725692749023, 2.7535369396209717, 2.6595101356506348, 2.6476962566375732, 2.658149242401123, 2.7329964637756348, 2.5920028686523438, 2.74861216545105, 2.5823092460632324, 2.5295181274414062, 3.4658212661743164, 3.1464672088623047, 2.6904802322387695, 2.5358314514160156, 2.6804263591766357, 2.72941517829895, 2.5751218795776367, 2.8223743438720703, 2.6752512454986572, 2.5570452213287354, 2.735410451889038, 2.66774320602417, 2.6578457355499268, 2.6112990379333496, 2.6692497730255127, 2.681046962738037, 2.844060182571411, 2.8250768184661865, 2.6770458221435547, 2.5201120376586914, 2.6654603481292725, 2.927232503890991, 2.7155942916870117, 2.7372043132781982, 2.533628225326538, 4.22402286529541, 2.9903154373168945, 2.57694149017334, 2.4927024841308594, 2.5911524295806885, 2.504566192626953, 2.5874481201171875, 2.6003692150115967, 2.850656747817993, 2.473827838897705, 2.634101629257202, 2.915465831756592, 2.5510799884796143, 2.5482559204101562, 2.728823184967041, 2.6453237533569336, 2.7291741371154785, 2.7884960174560547, 2.5995922088623047, 2.5104966163635254, 2.619011163711548, 2.5963051319122314, 2.688530206680298, 3.177769899368286, 3.2123513221740723, 2.835813522338867, 2.5378143787384033, 2.7838361263275146, 2.6139917373657227, 2.595632314682007, 2.5811073780059814, 2.795503616333008, 2.6355180740356445, 2.648223876953125, 2.5689198970794678, 2.619128465652466, 2.5605409145355225, 2.67226243019104, 2.5424280166625977, 2.622532844543457, 2.9955432415008545, 2.8035948276519775, 2.8314876556396484, 2.609973907470703, 2.6219513416290283, 2.8890838623046875, 2.6545350551605225, 2.673656702041626, 2.5143871307373047, 2.709299325942993, 2.650779962539673, 2.692439079284668, 3.0761969089508057, 2.733829975128174, 2.541898250579834, 2.547400951385498, 2.5294911861419678, 2.617567539215088, 2.607755422592163, 3.417384386062622, 3.7844104766845703, 2.936063289642334, 2.5018417835235596, 2.4278244972229004, 2.6773691177368164, 2.67273211479187, 2.8646678924560547, 2.5531740188598633, 2.480455160140991, 2.700268507003784, 2.8507587909698486, 2.510080337524414, 2.4619438648223877, 2.5164878368377686, 2.6297569274902344, 2.536264419555664, 2.8356196880340576, 2.578166961669922, 2.819424629211426, 2.735407590866089, 2.5827043056488037, 2.6794066429138184, 2.7043039798736572, 2.5213558673858643, 2.4826743602752686, 2.480966329574585, 2.647169828414917, 2.613112449645996, 2.6300013065338135, 2.8384644985198975, 3.02309250831604, 2.865846633911133, 2.626049757003784, 2.6253833770751953, 2.8239755630493164, 2.750779151916504, 2.6649839878082275, 2.691563367843628, 2.6631996631622314, 2.7019171714782715, 2.5805296897888184, 2.5070645809173584, 2.5536651611328125, 2.7557883262634277, 2.528414011001587, 2.7579236030578613, 2.6199285984039307, 2.787360668182373, 2.52573561668396, 2.6605379581451416, 2.6350631713867188, 2.5823423862457275, 2.7281196117401123, 2.6623077392578125, 2.5953354835510254, 2.6125450134277344, 2.593963861465454, 2.7013132572174072, 2.59641170501709, 2.5438544750213623, 2.641906261444092, 2.6405904293060303, 2.6902337074279785, 2.7671709060668945, 2.682077407836914, 2.527312755584717, 2.8045663833618164, 2.6072170734405518, 2.683697462081909, 2.509483575820923, 2.7177040576934814, 2.5215275287628174, 2.5995888710021973, 2.5580148696899414, 2.6643829345703125, 2.6696274280548096, 2.8020408153533936, 2.6255297660827637, 2.7443490028381348, 2.6408121585845947, 2.6072967052459717, 2.532575845718384, 2.669936180114746, 2.5420682430267334, 2.7987539768218994, 2.4486677646636963, 2.759570360183716, 2.682056427001953, 2.636638641357422, 2.8805456161499023, 2.5222089290618896, 2.553887128829956, 2.6372427940368652, 2.737382173538208, 2.561814546585083, 2.530799388885498, 2.6113104820251465, 2.558591604232788, 2.6995558738708496, 2.657048225402832, 2.6117122173309326, 2.511941432952881, 2.6836800575256348, 2.5286672115325928, 2.4718494415283203, 2.553571939468384, 2.484037160873413, 2.8240795135498047, 2.783015727996826, 2.585683822631836, 2.5446817874908447, 2.63454008102417, 2.7531821727752686, 2.7225565910339355, 2.853769540786743, 2.595170259475708, 2.7673861980438232, 2.513153314590454, 2.5336227416992188, 2.5788614749908447, 2.5612075328826904, 2.5519907474517822, 2.6127350330352783, 2.593437910079956, 2.886838912963867, 4.909399509429932, 2.85475492477417, 2.6036651134490967, 2.396552324295044, 2.5115365982055664, 2.465582847595215, 2.457975387573242, 2.4675023555755615, 2.6934168338775635, 2.539961814880371, 2.5025625228881836, 2.568891763687134, 2.7412924766540527, 2.593144416809082, 2.533552885055542, 2.7534677982330322, 2.8354742527008057, 2.645721435546875, 2.4496188163757324, 2.5278127193450928, 2.5531046390533447, 2.4632623195648193, 2.603024959564209, 2.52001690864563, 2.6918466091156006, 2.6294703483581543, 2.6266438961029053, 2.710099220275879, 2.5621957778930664, 2.6757826805114746, 2.5760984420776367, 2.538933038711548, 2.5979576110839844, 2.648425340652466, 2.656578540802002, 2.677135705947876, 2.6147451400756836, 2.7120161056518555, 2.5635886192321777, 2.6636178493499756, 2.598325729370117, 2.645195960998535, 2.604912757873535, 2.533188819885254, 2.5216619968414307, 2.608732223510742, 2.6615469455718994, 2.548403739929199, 2.6347694396972656, 2.765338659286499, 2.784348487854004, 2.564502716064453, 2.4577903747558594, 2.761404514312744, 2.757539987564087, 2.5911035537719727, 2.455638885498047, 2.4999608993530273, 2.6729185581207275, 2.4635751247406006, 2.651623487472534, 2.729410409927368, 2.5028719902038574, 4.360595703125, 2.896362543106079, 2.5933804512023926, 2.4458425045013428, 2.467670202255249, 2.44972825050354, 2.484233856201172, 2.4197590351104736, 2.6577420234680176, 2.6455512046813965, 2.568760633468628, 2.659818172454834, 2.473494529724121, 2.703305721282959, 2.677313804626465, 2.4883646965026855, 2.8582353591918945, 2.93935489654541, 2.757014513015747, 2.5117909908294678, 2.6059417724609375, 2.4796547889709473, 2.4070444107055664, 2.5669620037078857, 2.6826863288879395, 2.4316329956054688, 2.553906202316284, 2.600207805633545, 2.644334316253662, 2.7476463317871094, 2.712350606918335, 2.5898799896240234, 2.5160319805145264, 2.4565608501434326, 2.708620309829712, 2.6528666019439697, 2.5264337062835693, 2.619699478149414, 2.586242914199829, 2.6446125507354736, 3.478902816772461, 3.0371124744415283, 2.516735792160034, 2.490046501159668, 2.4641764163970947, 2.606435775756836, 2.5368881225585938, 2.4341723918914795, 2.465028762817383, 2.471184253692627, 2.8526761531829834, 2.62090802192688, 2.537761926651001, 2.4750750064849854, 2.6980271339416504, 2.532679796218872, 2.540055274963379, 2.6531214714050293, 2.5195093154907227, 2.835362672805786, 2.5144765377044678, 2.4630837440490723, 2.4634408950805664, 2.569685459136963, 2.687375783920288, 2.6278836727142334, 2.773635149002075, 2.621300458908081, 2.523134469985962, 2.7108137607574463, 2.480245351791382, 2.5278878211975098, 2.673457384109497, 2.5860707759857178, 2.621406078338623, 2.666382074356079, 2.7802865505218506, 2.991935968399048, 2.704939126968384, 2.4408552646636963, 2.4286556243896484, 2.5862157344818115, 2.6059725284576416, 2.701333522796631, 3.4672794342041016, 3.1350297927856445, 2.6115732192993164, 2.4131765365600586, 2.4898173809051514, 2.593550682067871, 2.563962697982788, 2.421967029571533, 2.5268585681915283, 2.9888744354248047, 2.6097025871276855, 2.426286220550537, 2.796016216278076, 2.348721504211426, 2.4358725547790527, 2.411717653274536, 2.6605563163757324, 2.504310369491577, 2.5918991565704346, 2.671466827392578, 2.6788628101348877, 2.5241036415100098, 2.529517412185669, 2.5241167545318604, 2.6074881553649902, 2.4897758960723877, 2.7018561363220215, 2.569270133972168, 2.6618852615356445, 2.63565993309021, 2.5098254680633545, 2.444575309753418, 2.731518507003784, 2.5693843364715576, 2.4853334426879883, 2.4456379413604736, 2.5216891765594482, 2.567304849624634, 2.5328376293182373, 3.7689952850341797, 3.168447732925415, 2.7427196502685547, 2.52203631401062, 2.7127914428710938, 2.5238218307495117, 2.465202569961548, 2.385715961456299, 2.4541709423065186, 2.4149107933044434, 2.690584897994995, 2.6084258556365967, 2.5058858394622803, 2.5822324752807617, 2.6907708644866943, 2.5324478149414062, 2.5518529415130615, 2.4531736373901367, 2.844550371170044, 2.5995404720306396, 2.648573398590088, 2.4549055099487305, 2.5103938579559326, 2.419128179550171, 2.7304904460906982, 2.525379180908203, 2.438581705093384, 2.444457769393921, 2.5121147632598877, 2.8212931156158447, 2.8446438312530518, 2.4753925800323486, 2.6147072315216064, 2.5086793899536133, 2.6382896900177, 2.5384647846221924, 2.627594232559204, 2.472987174987793, 2.47835111618042, 2.417055130004883, 2.6560704708099365, 2.7276771068573, 2.4946653842926025, 2.5672683715820312, 2.53779673576355, 2.5595016479492188, 2.5894877910614014, 2.5570833683013916, 2.5354862213134766, 2.5902814865112305, 2.515965223312378, 2.5520598888397217, 2.549100399017334, 2.48301362991333, 2.5551812648773193, 2.566967010498047, 2.5085697174072266, 2.5073349475860596, 2.591125249862671, 2.8160746097564697, 2.5570731163024902, 2.5101606845855713, 2.4400222301483154, 2.961592435836792, 2.8682940006256104, 2.5266048908233643, 2.477289915084839, 2.5448546409606934, 2.528623580932617, 2.5252418518066406, 2.470653772354126, 2.4621963500976562, 2.696918487548828, 2.6388399600982666, 2.497408866882324, 2.5035243034362793, 2.6742823123931885, 2.4974992275238037, 2.6141207218170166, 2.5365235805511475, 2.8756303787231445, 2.588266372680664, 2.5787951946258545, 2.41072678565979, 2.546569347381592, 2.561265707015991, 2.5102667808532715, 2.4244544506073, 2.6062028408050537, 2.425252676010132, 2.5000462532043457, 2.442498207092285, 2.666910171508789, 2.4935731887817383, 2.7641782760620117, 2.730130195617676, 2.412360191345215, 2.4556400775909424, 2.587871551513672, 2.6189234256744385, 2.439338445663452, 2.6118931770324707, 2.5316834449768066, 2.3766183853149414, 2.4965152740478516, 2.5880253314971924, 2.6195061206817627, 2.8043782711029053, 2.6554927825927734, 2.4307329654693604, 2.469693422317505, 2.5568928718566895, 2.522627592086792, 2.4387123584747314, 2.587800979614258, 2.6135270595550537, 2.5195975303649902, 2.4744014739990234, 2.6342782974243164, 3.6791770458221436, 2.794703483581543, 2.577854871749878, 2.525444984436035, 2.5028467178344727, 2.473891258239746, 2.4273431301116943, 2.463766574859619, 2.3779234886169434, 2.5399675369262695, 2.5278704166412354, 2.5026094913482666, 2.528179407119751, 2.4253029823303223, 2.4830737113952637, 2.4373011589050293, 2.4196419715881348, 2.5904183387756348, 2.7140965461730957, 2.4135491847991943, 3.2437243461608887, 3.944709539413452, 2.6752824783325195, 2.4014968872070312, 2.514314651489258, 2.491032361984253, 2.5913496017456055, 2.4644296169281006, 2.364332675933838, 2.5612294673919678, 2.357062578201294, 2.4496610164642334, 2.519516706466675, 2.6400058269500732, 2.484503746032715, 2.563852071762085, 2.4313015937805176, 2.4676175117492676, 2.507777452468872, 2.496687412261963, 2.547762155532837, 2.4994237422943115, 2.3534481525421143, 2.6896867752075195, 2.4115636348724365, 2.483180522918701, 2.684783458709717, 2.563077926635742, 2.4122793674468994, 2.5789530277252197, 2.554245710372925, 2.535318374633789, 2.466646671295166, 2.5127384662628174, 2.4508767127990723, 2.4078195095062256, 2.3814263343811035, 2.601665735244751, 2.461670398712158, 2.4963014125823975, 2.545671224594116, 2.906491994857788, 2.5688133239746094, 2.572490692138672, 2.6359620094299316, 2.5314342975616455, 2.4560627937316895, 2.3777079582214355, 2.4392666816711426, 2.5806374549865723, 2.448122262954712, 2.432680368423462, 2.6173408031463623, 2.676224946975708, 2.525009870529175, 2.4551944732666016, 2.428896903991699, 2.520308256149292, 2.5437777042388916, 2.41070294380188, 2.450678825378418, 2.613849639892578, 2.4602694511413574, 2.775279998779297, 2.6327149868011475, 2.4261202812194824, 2.6807968616485596, 2.4868462085723877, 2.4443464279174805, 2.445627450942993, 2.5183300971984863, 2.5457510948181152, 2.4801993370056152, 2.4683685302734375, 2.6445534229278564, 2.503709554672241, 2.51334810256958, 2.4133918285369873, 2.432264566421509, 2.572730779647827, 2.6069247722625732, 2.594517469406128, 2.4175631999969482, 2.5110437870025635, 2.483706474304199, 2.498609781265259, 2.4422953128814697, 2.7653651237487793, 3.753035068511963, 2.4922237396240234, 2.3358888626098633, 2.5439205169677734, 2.4710965156555176, 2.4111433029174805, 2.4110851287841797, 2.5289173126220703, 2.5350677967071533, 2.48129940032959, 2.4692542552948, 2.523749828338623, 2.6553494930267334, 2.715085506439209, 2.6804518699645996, 2.4742507934570312, 2.4642746448516846, 2.4297423362731934, 2.4352850914001465, 2.524761199951172, 2.338454008102417, 2.579444646835327, 2.5799708366394043, 2.7183377742767334, 2.5641610622406006, 2.489579200744629, 2.56626558303833, 2.6522138118743896, 2.4752068519592285, 2.3882229328155518, 2.575411319732666, 2.3777594566345215, 2.3920929431915283, 2.5916929244995117, 2.7453930377960205, 2.652024507522583, 2.7071917057037354, 2.437518358230591, 2.4017441272735596, 2.43630051612854, 2.4268722534179688, 2.385704755783081, 2.5204102993011475, 2.448361396789551, 3.0134658813476562, 2.564836025238037, 2.4586589336395264, 2.48958683013916, 2.392411470413208, 2.4490740299224854, 2.4458553791046143, 2.4690957069396973, 2.351069211959839, 2.449376344680786, 2.5617544651031494, 2.5416953563690186, 2.558077096939087, 4.143385887145996, 2.804522752761841, 2.607609748840332, 2.4792542457580566, 2.4760384559631348, 2.351531744003296, 2.331695556640625, 2.3591864109039307, 2.4308204650878906, 2.3837878704071045, 2.4761133193969727, 2.592877149581909, 2.4357306957244873, 2.3993594646453857, 2.4707930088043213, 2.5296785831451416, 2.5341811180114746, 2.5074148178100586, 2.4693973064422607, 2.5967366695404053, 2.6058125495910645, 2.4762563705444336, 2.5049264430999756, 2.6190853118896484, 2.492769718170166, 2.44891619682312, 2.4915449619293213, 2.487212657928467, 2.4480957984924316, 2.562220573425293, 2.402883529663086, 2.4904966354370117, 2.611149787902832, 2.704019069671631, 2.471647262573242, 2.3898746967315674, 2.5293753147125244, 2.3797566890716553, 2.632294178009033, 2.48524808883667, 2.5981743335723877, 2.449676036834717, 2.4515433311462402, 2.392866611480713, 2.49108624458313, 2.423476219177246, 3.7075209617614746, 2.5331759452819824, 2.6793432235717773, 2.454292058944702, 2.437239408493042, 2.3749992847442627, 2.4136016368865967, 2.5091426372528076, 2.5469584465026855, 2.5887656211853027, 2.4318459033966064, 2.380300998687744, 2.56288743019104, 2.3716928958892822, 2.4263103008270264, 2.437974691390991, 2.5382022857666016, 2.545698404312134, 2.4865005016326904, 2.5035059452056885, 2.5728442668914795, 2.448065757751465, 2.617152214050293, 2.4495861530303955, 2.556142568588257, 2.990061044692993, 2.640601873397827, 2.489591598510742, 2.3467445373535156, 2.637277126312256, 2.426044225692749, 2.4688720703125, 2.5213913917541504, 2.4556620121002197, 2.3646206855773926, 2.5502705574035645, 2.3552799224853516, 2.4334774017333984, 2.522658109664917, 2.363112688064575, 2.4443721771240234, 2.3700106143951416, 2.7483625411987305, 2.5812549591064453, 2.5503199100494385, 2.3696982860565186, 2.607154369354248, 2.623049259185791, 2.448678493499756, 2.3749754428863525, 2.5232043266296387, 2.5504555702209473, 2.4444212913513184, 2.453622341156006, 2.6214873790740967, 2.549571990966797, 3.425381898880005, 2.6457712650299072, 2.326380968093872, 2.548586845397949, 2.35786509513855, 2.36568284034729, 2.3882970809936523, 2.376596450805664, 2.419468402862549, 2.424837589263916, 2.4839532375335693, 2.587827444076538, 2.4885387420654297, 2.5659756660461426, 2.5342960357666016, 2.391902446746826, 2.297001838684082, 2.674557685852051, 2.621551275253296, 2.382448196411133, 2.4791362285614014, 2.43110728263855, 2.4615297317504883, 2.4130022525787354, 2.6851654052734375, 2.4907538890838623, 2.3500027656555176, 2.6310083866119385, 2.4403650760650635, 2.425515651702881, 2.4611289501190186, 2.5061731338500977, 2.417686939239502, 2.3551721572875977, 2.4863462448120117, 2.4396214485168457, 2.8743767738342285, 2.4570770263671875], 'val_loss': [11.094253540039062, 10.866572380065918, 10.79522705078125, 11.32420539855957, 10.891130447387695, 11.623802185058594, 11.060066223144531, 11.090091705322266, 11.048754692077637, 11.185588836669922, 12.445364952087402, 11.801128387451172, 10.928333282470703, 11.077280044555664, 11.25203800201416, 11.482369422912598, 11.408127784729004, 11.284202575683594, 11.222430229187012, 13.409847259521484, 11.708425521850586, 12.12398624420166, 10.878693580627441, 11.070478439331055, 10.975576400756836, 13.078956604003906, 11.582779884338379, 11.651037216186523, 12.700994491577148, 10.970039367675781, 11.666418075561523, 10.966753959655762, 11.254890441894531, 11.915064811706543, 11.518482208251953, 11.696576118469238, 12.624156951904297, 10.971796035766602, 11.233999252319336, 10.936168670654297, 12.122764587402344, 13.726096153259277, 11.543134689331055, 11.060308456420898, 11.011675834655762, 10.658921241760254, 10.807887077331543, 10.849459648132324, 11.898077964782715, 10.708444595336914, 10.936678886413574, 11.901751518249512, 10.996871948242188, 10.916050910949707, 11.007722854614258, 11.18784236907959, 11.157368659973145, 10.860498428344727, 11.825919151306152, 10.792847633361816, 11.393377304077148, 11.600980758666992, 10.998775482177734, 11.182442665100098, 11.011101722717285, 10.956937789916992, 11.05715560913086, 10.771065711975098, 11.191189765930176, 11.213468551635742, 11.039634704589844, 10.984513282775879, 12.457387924194336, 11.536178588867188, 10.710480690002441, 12.009600639343262, 11.115523338317871, 10.922861099243164, 11.738975524902344, 10.665484428405762, 11.107474327087402, 10.822096824645996, 10.891000747680664, 10.913700103759766, 12.489328384399414, 11.712847709655762, 12.484464645385742, 11.200572967529297, 11.031057357788086, 10.848648071289062, 10.698777198791504, 11.025238990783691, 10.848701477050781, 11.211554527282715, 10.77543830871582, 10.984931945800781, 10.801908493041992, 10.854853630065918, 11.516005516052246, 11.184962272644043, 11.104024887084961, 11.159202575683594, 10.737630844116211, 11.125052452087402, 10.871475219726562, 11.37869930267334, 11.318400382995605, 10.702789306640625, 10.895099639892578, 10.634020805358887, 11.496074676513672, 10.718952178955078, 10.719003677368164, 10.668214797973633, 10.821548461914062, 11.339226722717285, 14.636556625366211, 10.700950622558594, 10.818854331970215, 11.74162769317627, 11.788893699645996, 10.657072067260742, 11.628436088562012, 12.338191032409668, 10.694604873657227, 10.857266426086426, 13.43746566772461, 11.376045227050781, 10.774850845336914, 10.837320327758789, 11.275309562683105, 11.968244552612305, 10.558959007263184, 10.798401832580566, 11.928955078125, 11.335987091064453, 10.899212837219238, 11.142563819885254, 10.744573593139648, 10.922200202941895, 11.063450813293457, 12.3624267578125, 10.890949249267578, 11.136112213134766, 10.67917537689209, 10.881315231323242, 11.609731674194336, 10.872214317321777, 10.816407203674316, 10.553095817565918, 12.143349647521973, 10.878623008728027, 10.813399314880371, 11.163799285888672, 10.79680347442627, 11.422389030456543, 10.971674919128418, 10.699155807495117, 10.966512680053711, 10.939024925231934, 11.4625244140625, 11.401371002197266, 11.11619758605957, 10.880738258361816, 11.755029678344727, 11.101086616516113, 10.48835563659668, 10.924962997436523, 10.838629722595215, 10.81484603881836, 10.598427772521973, 10.890923500061035, 10.96170425415039, 10.82465934753418, 11.309675216674805, 11.232460975646973, 10.722354888916016, 10.63645076751709, 10.658855438232422, 10.61906909942627, 11.16307258605957, 11.176447868347168, 11.136054992675781, 11.099617004394531, 11.237321853637695, 10.839333534240723, 10.708900451660156, 10.820378303527832, 10.700642585754395, 10.922515869140625, 10.72474193572998, 11.484720230102539, 11.106678009033203, 10.52628231048584, 10.986391067504883, 10.866934776306152, 10.710073471069336, 10.520286560058594, 10.723356246948242, 10.88827133178711, 10.755884170532227, 10.660809516906738, 10.835147857666016, 10.875008583068848, 10.496275901794434, 10.992842674255371, 10.590970993041992, 10.784138679504395, 10.934348106384277, 10.694976806640625, 10.600337028503418, 11.636645317077637, 10.873635292053223, 10.693716049194336, 11.00940227508545, 11.076452255249023, 10.836752891540527, 10.573112487792969, 10.796159744262695, 11.21881103515625, 10.86434268951416, 10.886863708496094, 11.063371658325195, 10.561179161071777, 11.0497407913208, 11.121552467346191, 10.699145317077637, 10.596382141113281, 11.017914772033691, 11.002738952636719, 10.479697227478027, 10.558988571166992, 10.595664024353027, 10.989662170410156, 10.716299057006836, 10.428730010986328, 10.995144844055176, 10.849005699157715, 10.547892570495605, 11.856377601623535, 10.658616065979004, 10.51605224609375, 10.713589668273926, 10.96790599822998, 10.698302268981934, 11.155601501464844, 11.338915824890137, 10.910706520080566, 11.620868682861328, 10.6024169921875, 10.751065254211426, 11.045209884643555, 11.998343467712402, 11.512201309204102, 10.604988098144531, 10.526061058044434, 10.784092903137207, 10.958662033081055, 10.368256568908691, 10.976078033447266, 10.633249282836914, 10.387298583984375, 10.527200698852539, 10.514591217041016, 10.878990173339844, 10.623235702514648, 11.352530479431152, 10.922767639160156, 10.592759132385254, 10.571739196777344, 10.692268371582031, 10.925820350646973, 10.738822937011719, 11.806029319763184, 11.251435279846191, 13.02331256866455, 11.386556625366211, 10.298401832580566, 10.63514518737793, 12.636561393737793, 10.58735466003418, 10.749085426330566, 10.40876293182373, 11.270066261291504, 10.653725624084473, 10.347966194152832, 10.669541358947754, 11.102254867553711, 11.06518268585205, 10.734241485595703, 11.258729934692383, 10.991780281066895, 10.594189643859863, 11.03404712677002, 11.09303092956543, 10.628509521484375, 10.497603416442871, 10.929593086242676, 10.73006534576416, 10.362405776977539, 10.836663246154785, 10.807377815246582, 10.642107009887695, 10.561259269714355, 11.143094062805176, 10.467044830322266, 10.817771911621094, 10.662984848022461, 11.213043212890625, 10.851888656616211, 11.416959762573242, 10.478961944580078, 11.690884590148926, 10.648221015930176, 10.7795991897583, 10.548040390014648, 11.53520679473877, 10.521519660949707, 11.639007568359375, 10.45205307006836, 10.980231285095215, 10.55744457244873, 10.653634071350098, 10.525315284729004, 10.703177452087402, 10.492158889770508, 10.505396842956543, 10.3949556350708, 10.588484764099121, 10.619826316833496, 10.648448944091797, 11.140922546386719, 11.00288200378418, 10.696706771850586, 10.869309425354004, 10.56733226776123, 10.436141014099121, 10.82352352142334, 10.40334415435791, 10.36497688293457, 10.879611015319824, 10.794609069824219, 10.571934700012207, 10.75252628326416, 10.454902648925781, 10.357882499694824, 10.392189025878906, 10.473099708557129, 10.567323684692383, 10.348483085632324, 11.09384536743164, 10.533974647521973, 10.450323104858398, 11.786360740661621, 10.46784496307373, 10.9000883102417, 10.52366828918457, 10.37525749206543, 10.474212646484375, 10.55599308013916, 10.744354248046875, 10.61544418334961, 10.768441200256348, 11.075997352600098, 10.848250389099121, 10.384321212768555, 10.538536071777344, 10.421408653259277, 10.817523002624512, 10.89786148071289, 11.123640060424805, 10.542547225952148, 10.416500091552734, 10.541723251342773, 10.99970817565918, 10.983733177185059, 11.156755447387695, 12.174670219421387, 10.442977905273438, 10.71375560760498, 11.724620819091797, 11.3417329788208, 10.648353576660156, 11.192543983459473, 10.839221000671387, 10.440464973449707, 10.889057159423828, 11.11710262298584, 10.939266204833984, 10.552912712097168, 11.058627128601074, 10.5311918258667, 10.310035705566406, 10.824601173400879, 10.660639762878418, 10.69198226928711, 10.568042755126953, 10.668278694152832, 10.425554275512695, 10.620704650878906, 10.57526683807373, 10.681924819946289, 10.523268699645996, 10.261106491088867, 10.591279029846191, 10.427613258361816, 10.453771591186523, 10.66077995300293, 10.507974624633789, 11.246869087219238, 10.601594924926758, 11.452449798583984, 10.937458038330078, 10.774008750915527, 10.583457946777344, 11.183348655700684, 10.46838665008545, 11.078492164611816, 11.151995658874512, 10.528664588928223, 10.678677558898926, 10.59909725189209, 10.564188003540039, 10.639830589294434, 10.537580490112305, 11.0401029586792, 10.625052452087402, 12.104804992675781, 11.435016632080078, 10.68184757232666, 10.757376670837402, 10.368467330932617, 11.1891508102417, 10.630288124084473, 10.853092193603516, 10.623746871948242, 10.419222831726074, 10.79430866241455, 10.810579299926758, 10.346478462219238, 10.638978004455566, 10.517671585083008, 10.671494483947754, 10.646172523498535, 10.547412872314453, 10.473335266113281, 10.480180740356445, 10.646461486816406, 10.8718843460083, 11.31084156036377, 11.079266548156738, 10.546936988830566, 10.491690635681152, 10.996336936950684, 10.564096450805664, 10.997818946838379, 10.510801315307617, 10.683475494384766, 11.126964569091797, 11.214838981628418, 10.416860580444336, 10.992620468139648, 10.62940502166748, 10.649145126342773, 10.75322437286377, 11.183107376098633, 10.622413635253906, 10.504181861877441, 10.504600524902344, 10.267400741577148, 10.67226505279541, 10.493077278137207, 11.463696479797363, 10.827929496765137, 10.798662185668945, 10.88583755493164, 10.32020092010498, 11.909211158752441, 10.763809204101562, 10.53248405456543, 11.243350982666016, 10.861323356628418, 10.585309028625488, 10.666828155517578, 10.747618675231934, 10.489709854125977, 10.666277885437012, 10.95277214050293, 10.627028465270996, 10.979456901550293, 10.604764938354492, 10.337430953979492, 10.871275901794434, 10.352027893066406, 10.712100982666016, 10.664109230041504, 10.509134292602539, 11.0632963180542, 10.7348051071167, 10.930002212524414, 10.521527290344238, 10.534372329711914, 10.741230964660645, 10.457379341125488, 10.632210731506348, 10.676972389221191, 10.529302597045898, 13.1611967086792, 10.451498985290527, 10.722854614257812, 10.680607795715332, 10.949389457702637, 10.55223274230957, 11.163287162780762, 10.64178466796875, 10.688899993896484, 10.933246612548828, 10.811697006225586, 11.34887409210205, 11.21534252166748, 11.205525398254395, 10.962745666503906, 10.438126564025879, 10.373620986938477, 10.863899230957031, 10.514897346496582, 10.554938316345215, 12.029749870300293, 10.465612411499023, 10.767168045043945, 10.517463684082031, 10.684930801391602, 10.609012603759766, 10.41608715057373, 10.516141891479492, 10.784780502319336, 11.447031021118164, 10.709146499633789, 10.535392761230469, 11.64938735961914, 10.541556358337402, 10.701849937438965, 10.66222858428955, 10.365371704101562, 11.038935661315918, 10.798588752746582, 10.677366256713867, 10.758047103881836, 10.790144920349121, 10.877354621887207, 10.794488906860352, 10.912057876586914, 11.061992645263672, 10.817931175231934, 10.742573738098145, 10.66588306427002, 10.870051383972168, 10.551743507385254, 10.448577880859375, 10.318042755126953, 10.866135597229004, 10.638407707214355, 10.499463081359863, 10.47292709350586, 10.58393383026123, 10.4203519821167, 11.255352973937988, 10.516921997070312, 10.840291976928711, 11.067112922668457, 10.627005577087402, 11.150099754333496, 11.299562454223633, 10.71409797668457, 10.594160079956055, 10.559788703918457, 10.629227638244629, 10.606080055236816, 10.57951545715332, 10.632063865661621, 10.500419616699219, 11.738232612609863, 10.821762084960938, 10.716208457946777, 10.567853927612305, 10.949665069580078, 12.104408264160156, 10.8674898147583, 12.288314819335938, 10.905862808227539, 10.969024658203125, 10.657917976379395, 10.602723121643066, 10.795371055603027, 10.502135276794434, 10.960948944091797, 10.945631980895996, 10.990406036376953, 10.67876148223877, 10.834930419921875, 10.882872581481934, 10.631990432739258, 10.580753326416016, 10.866734504699707, 10.831997871398926, 10.714576721191406, 11.165926933288574, 10.560908317565918, 10.547478675842285, 12.513023376464844, 11.687442779541016, 10.721104621887207, 10.7771577835083, 10.769852638244629, 10.606614112854004, 10.445670127868652, 11.428369522094727, 10.609919548034668, 10.649354934692383, 10.716408729553223, 10.65083122253418, 10.877360343933105, 10.610201835632324, 10.843707084655762, 10.65445327758789, 10.77223014831543, 10.900577545166016, 10.755599975585938, 10.836418151855469, 10.716093063354492, 10.757165908813477, 10.883898735046387, 10.918481826782227, 10.727173805236816, 10.64971923828125, 10.872943878173828, 11.043364524841309, 11.133612632751465, 10.75715446472168, 10.740181922912598, 10.898494720458984, 10.812623023986816, 10.676878929138184, 10.702204704284668, 10.690958023071289, 10.944491386413574, 10.945919036865234, 10.650480270385742, 11.016708374023438, 10.967331886291504, 10.632781028747559, 10.77711009979248, 11.062434196472168, 10.68728256225586, 11.294600486755371, 11.03486156463623, 10.585530281066895, 10.4345064163208, 10.485187530517578, 10.73469352722168, 10.951416015625, 10.763318061828613, 10.597122192382812, 10.936758995056152, 10.72557544708252, 10.708502769470215, 10.932477951049805, 11.685541152954102, 10.750258445739746, 10.620840072631836, 10.602307319641113, 11.000935554504395, 10.866742134094238, 10.6932954788208, 10.521025657653809, 10.733880043029785, 10.904152870178223, 10.88137149810791, 11.082388877868652, 11.100021362304688, 10.738114356994629, 10.684664726257324, 10.442561149597168, 11.06694507598877, 10.516155242919922, 10.538653373718262, 10.555047035217285, 10.609545707702637, 10.503899574279785, 11.006368637084961, 10.740060806274414, 10.811199188232422, 10.83974552154541, 11.180403709411621, 10.77331256866455, 10.774145126342773, 10.763340950012207, 10.752665519714355, 10.762262344360352, 10.690977096557617, 11.11312484741211, 10.439655303955078, 10.813922882080078, 10.661827087402344, 10.751301765441895, 12.1325044631958, 10.821209907531738, 11.016839981079102, 10.998912811279297, 10.959793090820312, 10.877209663391113, 10.682452201843262, 10.652848243713379, 10.928994178771973, 10.861977577209473, 10.988351821899414, 10.838805198669434, 10.771732330322266, 10.613083839416504, 10.982504844665527, 11.27217960357666, 10.615756034851074, 10.755534172058105, 10.633196830749512, 11.023344993591309, 10.591127395629883, 10.712576866149902, 10.714800834655762, 11.065215110778809, 10.937013626098633, 10.559213638305664, 10.730798721313477, 10.742934226989746, 10.735151290893555, 11.123672485351562, 11.127054214477539, 10.679601669311523, 10.711326599121094, 11.727548599243164, 10.939684867858887, 11.121519088745117, 10.487617492675781, 11.369221687316895, 10.764531135559082, 10.537683486938477, 10.718001365661621, 10.977632522583008, 10.70519733428955, 10.536456108093262, 10.800168991088867, 10.715651512145996, 11.12999439239502, 10.708001136779785, 10.73643970489502, 10.652013778686523, 10.903621673583984, 10.565176963806152, 10.966548919677734, 10.74409294128418, 11.055033683776855, 10.547746658325195, 13.145792007446289, 10.69751262664795, 10.587664604187012, 10.62765884399414, 11.929615020751953, 10.575809478759766, 10.89240550994873, 10.905692100524902, 10.417060852050781, 10.662933349609375, 10.601462364196777, 10.749971389770508, 10.851611137390137, 10.708385467529297, 11.404918670654297, 11.075828552246094, 11.138105392456055, 10.499566078186035, 10.574740409851074, 10.968767166137695, 10.571571350097656, 10.827836990356445, 11.067574501037598, 11.006953239440918, 10.381448745727539, 10.638025283813477, 11.351689338684082, 11.009407997131348, 10.769251823425293, 11.313380241394043, 10.640503883361816, 11.02607536315918, 10.924067497253418, 10.687759399414062, 10.96008586883545, 10.886270523071289, 11.00575065612793, 10.737168312072754, 10.684798240661621, 11.092249870300293, 10.797115325927734, 10.798661231994629, 10.451278686523438, 10.743500709533691, 10.81836223602295, 10.553787231445312, 10.453819274902344, 10.592883110046387, 10.689033508300781, 10.78562068939209, 10.717060089111328, 11.464325904846191, 10.635542869567871, 10.716182708740234, 11.422139167785645, 10.879190444946289, 11.288623809814453, 11.569221496582031, 10.51069450378418, 10.824528694152832, 10.69566535949707, 11.662836074829102, 11.364212036132812, 10.849611282348633, 10.482696533203125, 10.880575180053711, 10.748873710632324, 10.984166145324707, 11.699580192565918, 11.094047546386719, 10.650186538696289, 10.750286102294922, 11.28466796875, 11.424681663513184, 10.70002269744873, 10.994129180908203, 10.578573226928711, 11.134876251220703, 10.641407012939453, 10.720520973205566, 10.728294372558594, 10.623132705688477, 10.732548713684082, 11.244998931884766, 10.723102569580078, 10.781458854675293, 10.488975524902344, 10.699257850646973, 10.92967414855957, 10.70907211303711, 10.82162094116211, 11.284757614135742, 10.894761085510254, 10.543954849243164, 10.811379432678223, 11.457602500915527, 11.092422485351562, 10.85876750946045, 10.741104125976562, 11.2719144821167, 11.007392883300781, 10.445637702941895, 10.767008781433105, 10.535094261169434, 10.623303413391113, 10.725481986999512, 10.830490112304688, 10.870471954345703, 10.839058876037598, 10.624813079833984, 10.762618064880371, 11.147915840148926, 11.029338836669922, 10.8128662109375, 10.714095115661621, 10.960671424865723, 10.704370498657227, 11.023092269897461, 10.72547721862793, 11.021805763244629, 11.059581756591797, 10.762893676757812, 11.022200584411621, 10.86401653289795, 11.58139705657959, 10.933197975158691, 10.839755058288574, 10.837722778320312, 10.81039047241211, 11.24808120727539, 10.970245361328125, 10.79384994506836, 11.545921325683594, 10.75705623626709, 10.877646446228027, 10.727349281311035, 10.846445083618164, 10.832749366760254, 10.791474342346191, 10.854787826538086, 11.003003120422363, 10.8416166305542, 11.358667373657227, 10.815975189208984, 10.653034210205078, 10.885924339294434, 10.633221626281738, 10.938419342041016, 10.556663513183594, 11.390653610229492, 10.568264961242676, 11.134745597839355, 11.738277435302734, 10.847217559814453, 10.991493225097656, 10.92049503326416, 10.71978759765625, 10.900842666625977, 10.7537202835083, 10.782764434814453, 10.918617248535156, 10.805295944213867, 10.5015869140625, 11.113643646240234, 10.863661766052246, 10.96522331237793, 11.044787406921387, 10.795544624328613, 10.42756462097168, 10.54395580291748, 10.769948959350586, 10.904151916503906, 11.089882850646973, 10.828043937683105, 10.728134155273438, 11.281634330749512, 10.803357124328613, 10.721002578735352, 11.155706405639648, 10.450767517089844, 10.77011489868164, 10.743475914001465, 10.674099922180176, 10.78140926361084, 10.799219131469727, 11.351380348205566, 11.060003280639648, 10.952248573303223, 10.831561088562012, 10.895169258117676, 11.00691032409668, 11.06412410736084, 10.644570350646973, 11.01931095123291, 10.545220375061035, 10.568495750427246, 10.585805892944336, 10.911556243896484, 10.642047882080078, 10.97246265411377, 10.692416191101074, 11.433021545410156, 10.875618934631348, 11.287845611572266, 12.600146293640137, 10.721953392028809, 10.758017539978027, 10.538215637207031, 11.32690715789795, 10.52570915222168, 10.619159698486328, 10.541367530822754, 10.442399024963379, 10.772185325622559, 10.587785720825195, 10.73169994354248, 10.64110279083252, 10.742044448852539, 10.662308692932129, 10.73044490814209, 10.656957626342773, 10.781885147094727, 10.731992721557617, 10.689064025878906, 10.720451354980469, 10.78919506072998, 10.93349838256836, 11.656146049499512, 10.368132591247559, 11.01202392578125, 10.598700523376465, 11.00815200805664, 11.724918365478516, 10.530418395996094, 10.701423645019531, 10.745416641235352, 11.061722755432129, 10.716651916503906, 11.05443000793457, 10.40267276763916, 10.736444473266602, 10.713215827941895, 10.807659149169922, 10.740097999572754, 10.766937255859375, 10.618978500366211, 11.190287590026855, 10.611811637878418, 10.85045051574707, 10.917097091674805, 11.197992324829102, 10.518805503845215, 11.079684257507324, 10.90333366394043, 10.862845420837402, 10.707907676696777, 10.798845291137695, 10.587234497070312, 10.588189125061035, 10.541568756103516, 11.001533508300781, 10.795866966247559, 10.737643241882324, 11.279314041137695, 10.580366134643555, 10.64236831665039, 10.709466934204102, 11.100597381591797, 10.773316383361816, 10.662924766540527, 11.10823917388916, 10.84057903289795, 10.673513412475586, 10.660649299621582, 11.114742279052734, 11.136093139648438, 10.891096115112305, 11.053811073303223, 10.929128646850586, 11.205574989318848, 10.722229957580566, 10.687767028808594, 11.23570442199707, 10.988553047180176, 11.403953552246094, 10.806281089782715, 10.547571182250977, 10.600489616394043, 10.51024055480957, 10.891489028930664, 10.742966651916504, 10.748209953308105, 10.705979347229004, 10.5431547164917, 10.75750732421875, 10.950316429138184, 10.709558486938477, 10.643939971923828, 10.58150577545166, 10.645686149597168, 11.152584075927734, 11.277382850646973, 10.777174949645996, 10.792386054992676, 10.85175609588623, 10.846115112304688, 10.695184707641602, 10.757027626037598, 10.656487464904785, 10.66838550567627, 10.775656700134277, 10.939919471740723, 10.955849647521973, 10.539474487304688, 10.787789344787598, 10.940857887268066, 11.403392791748047, 12.06861686706543, 10.882098197937012, 11.41223430633545, 10.806264877319336, 10.731492042541504, 10.809971809387207, 11.174764633178711, 10.61587905883789, 11.410584449768066, 10.646394729614258, 10.796945571899414, 10.593547821044922, 12.31983470916748, 11.665122032165527, 10.523394584655762, 11.353290557861328, 10.675122261047363, 10.726853370666504, 10.885180473327637, 10.807897567749023, 10.685198783874512, 10.872899055480957, 10.919919967651367, 10.79744815826416, 10.777982711791992, 10.901158332824707, 10.912562370300293, 10.858817100524902, 10.837220191955566, 10.844005584716797, 10.874120712280273, 10.85656452178955, 11.42540454864502, 11.428442001342773, 11.461507797241211, 10.717253684997559, 10.63882064819336, 11.015111923217773, 10.680416107177734, 10.784719467163086, 11.504817008972168, 10.836516380310059, 10.82480239868164, 10.910968780517578, 10.836780548095703, 10.836249351501465, 10.93159008026123, 10.771363258361816, 10.574459075927734, 11.145092010498047, 10.699480056762695, 10.619429588317871, 10.834306716918945, 10.706049919128418, 10.788970947265625, 10.612303733825684, 10.699662208557129, 10.671586036682129, 10.546148300170898, 11.027742385864258, 11.252336502075195, 11.327347755432129, 10.926600456237793, 11.161469459533691, 11.051627159118652, 11.118136405944824, 10.751168251037598, 10.712448120117188, 10.77985668182373, 10.982236862182617, 10.899985313415527, 10.696475982666016, 10.646882057189941, 11.044238090515137, 10.684099197387695, 10.982882499694824, 10.824226379394531, 10.720430374145508, 10.730095863342285, 10.711220741271973, 10.563322067260742, 10.857216835021973, 10.671351432800293, 10.560571670532227, 10.54853630065918, 10.865251541137695, 10.76578426361084, 11.049549102783203, 11.539900779724121, 10.662134170532227, 10.974082946777344, 10.803604125976562, 10.828417778015137, 11.319668769836426, 10.808109283447266, 10.882105827331543, 11.029048919677734, 10.855061531066895, 11.003132820129395, 11.711417198181152, 10.814401626586914, 10.594593048095703, 10.850809097290039, 10.937663078308105, 11.647711753845215, 10.861029624938965, 11.03128433227539, 10.817120552062988, 10.666069030761719, 10.82153606414795, 10.846089363098145, 11.078142166137695, 11.166250228881836, 11.067310333251953, 10.826796531677246, 10.848124504089355, 11.01474380493164, 10.844011306762695, 10.796368598937988, 10.595621109008789, 11.103231430053711, 10.726754188537598, 10.852879524230957, 10.700226783752441, 10.79764175415039, 10.724356651306152, 11.310253143310547, 10.90019416809082, 11.553898811340332, 10.751201629638672, 11.062944412231445, 11.060263633728027, 10.783848762512207, 10.796672821044922, 11.119416236877441, 11.001253128051758, 11.044447898864746, 11.137125968933105, 10.853912353515625, 10.766167640686035, 11.085626602172852, 11.168157577514648, 10.70238971710205, 10.720362663269043, 10.99375057220459, 11.160258293151855, 10.76853084564209, 10.675484657287598, 10.983184814453125, 10.694233894348145, 11.112414360046387, 11.234138488769531, 10.747757911682129, 10.8729829788208, 11.086531639099121, 10.797256469726562, 11.001482963562012, 10.940263748168945, 10.742681503295898, 10.821794509887695, 11.78388786315918, 10.954123497009277, 10.765582084655762, 11.526135444641113, 11.160133361816406, 10.684429168701172, 10.54560661315918, 10.935251235961914, 10.814117431640625, 10.940301895141602, 10.700299263000488, 10.658981323242188, 11.09219741821289, 10.647941589355469, 10.890254974365234, 10.770394325256348, 11.035737037658691, 10.735103607177734, 10.829139709472656, 11.1415433883667, 11.089439392089844, 10.921199798583984, 11.041163444519043, 10.772838592529297, 10.528237342834473, 11.240104675292969, 10.772099494934082, 10.860864639282227, 10.847845077514648, 11.370494842529297, 11.879659652709961, 11.473132133483887, 10.887386322021484, 10.95456600189209, 10.668411254882812, 10.881824493408203, 10.809890747070312, 11.063048362731934, 10.724882125854492, 10.661258697509766, 10.778267860412598, 10.840907096862793, 11.52346134185791, 10.962864875793457, 10.653836250305176, 10.757372856140137, 11.707100868225098, 10.872044563293457, 11.170694351196289, 10.741747856140137, 11.13530158996582, 10.84199047088623, 10.880109786987305, 10.82320499420166, 10.965625762939453, 10.915457725524902, 10.734082221984863, 10.800638198852539, 10.846389770507812, 10.657769203186035, 11.182978630065918, 10.961548805236816, 10.967291831970215, 10.856494903564453, 10.809215545654297, 11.06242847442627, 11.52238655090332, 10.811203002929688, 10.877486228942871, 10.728353500366211, 10.727546691894531, 10.844138145446777, 11.290163040161133, 10.86990737915039, 11.151893615722656, 10.629396438598633, 10.696717262268066, 11.005270004272461, 10.715892791748047, 10.814982414245605, 10.897785186767578, 10.736717224121094, 11.021895408630371, 11.156779289245605, 11.16268539428711, 11.410137176513672, 11.427736282348633, 11.077479362487793, 10.914518356323242, 10.926003456115723, 10.996551513671875, 10.762490272521973, 11.298871994018555, 10.829096794128418, 10.966641426086426, 10.631745338439941, 10.857354164123535, 11.309687614440918, 11.453155517578125, 11.37339973449707, 10.656599998474121, 10.669146537780762, 10.638236045837402, 10.653393745422363, 10.698160171508789, 10.946781158447266, 10.702610969543457, 10.82103443145752, 10.639948844909668, 11.01318359375, 10.868053436279297, 10.86303997039795, 10.874737739562988, 10.75311279296875, 10.662313461303711, 10.752497673034668, 10.906174659729004, 10.776314735412598, 10.772796630859375, 10.79675006866455, 10.746465682983398, 10.832508087158203, 10.770562171936035, 11.048013687133789, 10.981634140014648, 10.783723831176758, 10.880105018615723, 11.170554161071777, 11.33626937866211, 10.898359298706055, 11.020984649658203, 11.057138442993164, 10.722865104675293, 10.913799285888672, 10.868461608886719, 11.334526062011719, 10.8776216506958, 10.850807189941406, 10.962394714355469, 10.740941047668457, 10.709610939025879, 10.940194129943848, 10.78215217590332, 10.820426940917969, 10.921381950378418, 11.16679859161377, 11.03803539276123, 10.987588882446289, 10.754684448242188, 10.695992469787598, 10.876092910766602, 10.948450088500977, 10.948012351989746, 10.938157081604004, 10.860342025756836, 11.066481590270996, 11.16535758972168, 10.927122116088867, 11.100485801696777, 11.012430191040039, 11.055622100830078, 10.990150451660156, 10.929819107055664, 12.173810005187988, 11.015212059020996, 11.05084228515625, 11.507268905639648, 11.552207946777344, 10.892186164855957, 11.010279655456543, 10.80107307434082, 12.023612976074219, 10.932844161987305, 10.941084861755371, 10.861385345458984, 11.106637001037598, 10.828782081604004, 10.924269676208496, 11.046192169189453, 11.024053573608398, 11.199295997619629, 10.76502799987793, 10.917341232299805, 10.753475189208984, 11.08847427368164, 11.102058410644531, 11.110246658325195, 10.910411834716797, 10.782106399536133, 10.843155860900879, 11.06593132019043, 10.824552536010742, 12.027154922485352, 10.968743324279785, 10.796431541442871, 10.840174674987793, 11.247200965881348, 10.672441482543945, 10.80494213104248, 10.8365478515625, 10.617430686950684, 11.004557609558105, 11.015646934509277, 10.886019706726074, 10.984952926635742, 10.927946090698242, 11.224552154541016, 11.958176612854004, 11.206620216369629, 10.881991386413574, 10.858087539672852, 10.868049621582031, 10.80067253112793, 11.564746856689453, 11.404706954956055, 11.283032417297363, 11.133808135986328, 11.493145942687988, 10.990723609924316, 10.846973419189453, 10.78506851196289, 10.61559009552002, 10.654313087463379, 10.882989883422852, 10.842052459716797, 10.650450706481934, 10.888840675354004, 10.960580825805664, 10.842427253723145, 11.065494537353516, 10.844470977783203, 10.848209381103516, 10.76685905456543, 10.918034553527832, 10.795976638793945, 11.117780685424805, 10.796380043029785, 10.962522506713867, 10.859768867492676, 11.401744842529297, 11.065715789794922, 10.762340545654297, 11.127049446105957, 10.788786888122559, 10.731710433959961, 10.70345401763916, 11.472906112670898, 10.841424942016602, 11.356180191040039, 11.225875854492188, 10.725686073303223, 11.020951271057129, 10.907835006713867, 10.796359062194824, 10.852113723754883, 11.22472095489502, 10.801642417907715, 10.831910133361816, 10.681437492370605, 10.891205787658691, 11.134174346923828, 10.691145896911621, 10.802298545837402, 10.834935188293457, 10.60955810546875, 11.241944313049316, 10.79601764678955, 10.79328441619873, 10.911619186401367, 10.897449493408203, 10.746041297912598, 11.449957847595215, 10.941254615783691, 11.18199634552002, 10.82880973815918, 10.863630294799805, 14.052706718444824, 10.973760604858398, 10.75840950012207, 10.717132568359375, 10.876654624938965, 10.989742279052734, 11.464423179626465, 11.00450325012207, 10.786023139953613, 10.944334030151367, 10.925076484680176, 11.234708786010742, 10.931458473205566, 10.893717765808105, 11.45712947845459, 10.7521333694458, 11.044757843017578, 11.118423461914062, 11.137179374694824, 11.320940971374512, 10.91122055053711, 10.925224304199219, 10.935617446899414, 11.407654762268066, 10.839681625366211, 10.889772415161133, 10.877900123596191, 11.131102561950684, 10.936193466186523, 10.853851318359375, 11.493016242980957, 10.934996604919434, 10.780311584472656, 10.919273376464844, 10.83603572845459, 11.035876274108887, 11.035822868347168, 10.683349609375, 11.187777519226074, 10.918667793273926, 10.920233726501465, 11.164080619812012, 10.948040962219238, 10.93975830078125, 10.925704002380371, 10.960119247436523, 11.66641902923584, 10.930039405822754, 10.858081817626953, 10.779499053955078, 10.889761924743652, 10.836118698120117, 10.911096572875977, 10.697672843933105, 11.112286567687988, 10.97256851196289, 10.865195274353027, 11.163008689880371, 10.883404731750488, 11.002593040466309, 11.681591033935547, 11.299686431884766, 11.07194709777832, 11.077189445495605, 10.841313362121582, 11.046234130859375, 11.363094329833984, 11.066587448120117, 11.047113418579102, 10.962225914001465, 11.564277648925781, 10.968050003051758, 11.133780479431152, 11.117548942565918, 11.036145210266113, 11.064135551452637, 11.315241813659668, 11.729110717773438, 11.122954368591309, 10.980767250061035, 10.89755916595459, 10.990032196044922, 11.32889175415039, 11.228965759277344, 11.128353118896484, 10.942631721496582, 10.911866188049316, 11.687793731689453, 11.05272102355957, 10.960593223571777, 11.273555755615234, 10.962360382080078, 10.83654499053955, 11.257174491882324, 11.201741218566895, 11.216021537780762, 11.06866455078125, 10.993961334228516, 10.800764083862305, 10.960939407348633, 11.023680686950684, 11.171889305114746, 10.989666938781738, 11.092896461486816, 10.844366073608398, 10.812566757202148, 11.159238815307617, 11.16614055633545, 11.548848152160645, 11.184194564819336, 11.390567779541016, 10.936500549316406, 11.133004188537598, 11.20854663848877, 11.607245445251465, 10.993610382080078, 10.981905937194824, 11.435953140258789, 11.028311729431152, 10.927492141723633, 11.022747993469238, 11.403356552124023, 11.558436393737793, 10.879084587097168, 10.974287033081055, 12.448164939880371, 11.718137741088867, 11.18676471710205, 10.92520523071289, 10.878003120422363, 11.568812370300293, 10.860203742980957, 10.898407936096191, 10.877878189086914, 11.430171966552734, 10.906806945800781, 10.822400093078613, 10.884880065917969, 11.210251808166504, 10.975756645202637, 11.342331886291504, 11.022237777709961, 11.053282737731934, 10.961481094360352, 10.968321800231934, 11.287242889404297, 11.071624755859375, 10.828859329223633, 11.091403007507324, 11.084054946899414, 10.99630069732666, 11.03551197052002, 10.86295223236084, 10.884749412536621, 11.018284797668457, 11.114509582519531, 11.1195068359375, 11.101781845092773, 11.000011444091797, 11.81115436553955, 11.04647445678711, 11.326722145080566, 11.106800079345703, 11.266081809997559, 11.094842910766602, 11.148459434509277, 11.08125114440918, 11.468460083007812, 11.144014358520508, 11.136336326599121, 11.147822380065918, 11.04399299621582, 11.044286727905273, 10.980497360229492, 11.073881149291992, 11.095234870910645, 10.99891471862793, 11.382347106933594, 11.131149291992188, 11.167335510253906, 11.205089569091797, 11.064851760864258, 11.059999465942383, 10.97232723236084, 10.903010368347168, 11.166613578796387, 11.298255920410156, 11.05426025390625, 11.527192115783691, 11.793296813964844, 11.006547927856445, 11.620783805847168, 10.774755477905273, 10.911420822143555, 10.886513710021973, 11.006094932556152, 10.942008018493652, 11.181562423706055, 11.070655822753906, 11.084555625915527, 11.090561866760254, 11.417189598083496, 11.154082298278809, 11.228116035461426, 10.92634105682373, 11.999910354614258, 10.913558959960938, 11.071270942687988, 11.104887008666992, 10.949807167053223, 10.955354690551758, 11.205570220947266, 11.517269134521484, 10.941319465637207, 11.001004219055176, 11.125003814697266, 11.22197151184082, 11.381261825561523, 11.486108779907227, 10.966774940490723, 11.418902397155762, 10.966651916503906, 11.003580093383789, 11.453149795532227, 11.217222213745117, 10.892068862915039, 11.219207763671875, 11.590121269226074, 10.9758882522583, 12.053289413452148, 10.736822128295898, 10.730005264282227, 10.706002235412598, 10.93753433227539, 10.920038223266602, 10.870149612426758, 10.957599639892578, 10.913349151611328, 10.850798606872559, 10.993345260620117, 10.906928062438965, 10.95157527923584, 10.775308609008789, 10.921869277954102, 10.888498306274414, 11.088029861450195, 11.062427520751953, 11.928906440734863, 10.926156997680664, 10.937631607055664, 11.110905647277832, 11.036542892456055, 10.925599098205566, 11.087894439697266, 11.281734466552734, 11.123201370239258, 11.218955039978027, 11.216377258300781, 11.093280792236328, 11.275346755981445, 10.961292266845703, 11.124719619750977, 11.292898178100586, 11.224905014038086, 10.849287033081055, 12.622208595275879, 11.015276908874512, 11.084105491638184, 10.883216857910156, 11.082270622253418, 10.858461380004883, 11.430120468139648, 11.13328742980957, 12.259786605834961, 11.583635330200195, 10.87633228302002, 10.960999488830566, 11.050271987915039, 10.765918731689453, 10.931517601013184, 10.962756156921387, 11.018177032470703, 11.416459083557129, 11.122053146362305, 10.901275634765625, 11.189973831176758, 11.129786491394043, 11.148785591125488, 11.033350944519043, 11.200935363769531, 10.876985549926758, 11.089273452758789, 11.033612251281738, 10.938684463500977, 10.940866470336914, 10.767976760864258, 11.042444229125977, 11.823585510253906, 12.172677040100098, 11.132424354553223, 11.392121315002441, 11.193389892578125, 10.871726989746094, 10.929176330566406, 10.985028266906738, 10.954324722290039, 11.02783489227295, 10.997432708740234, 11.048910140991211, 10.903532981872559, 11.251636505126953, 11.319539070129395, 11.164965629577637, 12.336101531982422, 10.737895011901855, 11.357423782348633, 10.829541206359863, 10.83813190460205, 10.789605140686035, 10.922222137451172, 10.978763580322266, 11.136780738830566, 10.892724990844727, 10.872406005859375, 10.865065574645996, 11.165610313415527, 11.242587089538574, 11.081168174743652, 11.169085502624512, 11.253035545349121, 10.826992988586426, 11.55412483215332, 11.10201644897461, 10.888524055480957, 11.008511543273926, 11.307915687561035, 11.507128715515137, 11.049121856689453, 10.80860424041748, 11.026590347290039, 10.998028755187988, 11.154112815856934, 10.982512474060059, 11.135380744934082, 11.110067367553711, 11.167271614074707, 11.5759916305542, 11.183059692382812, 10.981213569641113, 10.965383529663086, 11.26726245880127, 10.942137718200684, 12.234850883483887, 11.106152534484863, 10.983070373535156, 11.081920623779297, 12.041176795959473, 11.030144691467285, 11.723441123962402, 11.3865385055542, 11.09296703338623, 11.296751022338867, 11.16529369354248, 11.415699005126953, 10.975226402282715, 11.254096984863281, 11.178934097290039, 11.472633361816406, 11.046369552612305, 11.52640438079834, 11.025300025939941, 11.13021183013916, 11.118966102600098, 10.977888107299805, 11.131160736083984, 12.261788368225098, 11.025776863098145, 10.734137535095215, 11.130094528198242, 11.074763298034668, 10.923921585083008, 11.144739151000977, 11.09859561920166, 11.005434036254883, 11.401896476745605, 10.888419151306152, 11.163566589355469, 11.1736478805542, 11.016770362854004, 10.84669017791748, 11.813557624816895, 11.310455322265625, 11.783805847167969, 11.128093719482422, 10.986042976379395, 11.560175895690918, 11.125221252441406, 11.404013633728027, 11.33712100982666, 11.351633071899414, 11.0152006149292, 11.06277084350586, 11.24712085723877, 10.92002010345459, 10.943188667297363, 11.181790351867676, 11.670982360839844, 10.889443397521973, 10.856882095336914, 10.818083763122559, 11.251925468444824, 11.026598930358887, 10.895905494689941, 11.155360221862793, 11.372607231140137, 11.006558418273926, 10.901640892028809, 11.205265998840332, 11.363391876220703, 10.787575721740723, 10.990038871765137, 10.95935344696045, 11.340751647949219, 11.139762878417969, 11.532248497009277, 11.031010627746582, 11.1128511428833, 10.932365417480469, 11.073619842529297, 10.954808235168457, 11.440078735351562, 11.966952323913574, 10.869651794433594, 10.86244010925293, 11.608543395996094, 10.923698425292969, 10.921881675720215, 11.601655960083008, 10.951905250549316, 10.973138809204102, 12.420266151428223, 10.975686073303223, 10.858414649963379, 11.057398796081543, 10.897477149963379, 11.280824661254883, 10.9254732131958, 11.37479019165039, 11.244112968444824, 11.158025741577148, 11.214581489562988, 12.522753715515137, 11.125192642211914, 11.01414966583252, 11.025702476501465, 10.79024887084961, 11.193585395812988, 11.190799713134766, 11.093745231628418, 11.152787208557129, 11.08221435546875, 10.909505844116211, 10.992344856262207, 11.261207580566406, 11.63102912902832, 11.07830810546875, 11.030876159667969, 11.381353378295898, 11.380295753479004, 11.267820358276367, 11.164002418518066, 11.26435661315918, 11.191584587097168, 10.951488494873047, 11.588184356689453, 11.156432151794434, 11.694378852844238, 11.270212173461914, 11.144214630126953, 10.923604011535645, 11.564284324645996, 10.816271781921387, 10.98985481262207, 11.333117485046387, 11.363470077514648, 11.32732105255127, 11.214627265930176, 11.480167388916016, 11.696571350097656, 11.375753402709961, 11.5997896194458, 12.168656349182129, 11.551666259765625, 11.17981243133545, 10.980429649353027, 11.117476463317871, 11.72088623046875, 11.237363815307617, 11.107405662536621, 11.153824806213379, 11.580941200256348, 11.094886779785156, 11.446867942810059, 11.191319465637207, 11.031404495239258, 10.961018562316895, 11.23401927947998, 11.300230026245117, 11.145125389099121, 11.437383651733398, 11.173027038574219, 10.959673881530762, 11.205007553100586, 11.922833442687988, 11.184846878051758, 11.01455307006836, 11.28735065460205, 11.221861839294434, 11.43998908996582, 11.316242218017578, 10.968364715576172, 11.386086463928223, 10.963543891906738, 11.164202690124512, 11.228681564331055, 11.38953971862793, 10.934059143066406, 11.196643829345703, 11.037688255310059, 11.197583198547363, 11.028404235839844, 11.295480728149414, 11.120916366577148, 11.081084251403809, 11.237213134765625, 11.151557922363281, 11.043395042419434, 11.05109691619873, 14.49205207824707, 11.247147560119629, 10.863334655761719, 11.039053916931152, 10.955933570861816, 11.039689064025879, 10.941559791564941, 11.179898262023926, 11.349821090698242, 11.133749008178711, 11.216411590576172, 11.126264572143555, 11.105831146240234, 11.978813171386719, 11.122176170349121, 11.156327247619629, 10.995220184326172, 11.284852027893066, 11.306700706481934, 11.222999572753906, 11.12156867980957, 11.018576622009277, 11.130377769470215, 11.160881042480469, 11.298912048339844, 11.09730339050293, 11.5678129196167, 11.251272201538086, 11.348607063293457, 11.003240585327148, 11.562065124511719, 11.288848876953125, 11.241525650024414, 11.300026893615723, 11.30754566192627, 11.322590827941895, 11.137205123901367, 11.344389915466309, 11.071271896362305, 11.162264823913574, 11.253654479980469, 11.076454162597656, 11.168203353881836, 11.364508628845215, 11.360867500305176, 11.506194114685059, 11.020702362060547, 11.000690460205078, 11.405533790588379, 11.084705352783203, 11.166069984436035, 11.08827018737793, 10.95123291015625, 11.191506385803223, 11.070952415466309, 11.93362045288086, 11.290526390075684, 11.18514633178711, 12.000041007995605, 11.05881118774414, 11.134052276611328, 10.947242736816406, 11.007304191589355, 11.164310455322266, 11.024327278137207, 11.415475845336914, 11.297249794006348, 11.314069747924805, 11.091156005859375, 11.252655029296875, 11.431588172912598, 11.099589347839355, 11.003143310546875, 11.524733543395996, 11.15182876586914, 11.148649215698242, 11.054570198059082, 11.3912935256958, 11.342951774597168, 11.340666770935059, 11.262554168701172, 11.332761764526367, 11.019984245300293, 10.925009727478027, 11.207991600036621, 11.347671508789062, 11.157472610473633, 11.266161918640137, 11.06653118133545, 11.2116117477417, 11.585864067077637, 11.088869094848633, 11.206854820251465, 11.491256713867188, 11.20205020904541, 11.066518783569336, 11.428470611572266, 11.51795768737793, 11.182671546936035, 11.490287780761719, 11.211053848266602, 11.70053768157959, 11.40285873413086, 12.075092315673828, 11.425561904907227, 11.110008239746094, 10.976625442504883, 10.936192512512207, 10.760315895080566, 11.127039909362793, 11.276412963867188, 11.091986656188965, 11.161680221557617, 11.359760284423828, 11.108556747436523, 10.885198593139648, 11.20763874053955, 11.037699699401855, 11.130003929138184, 11.13292121887207, 10.991194725036621, 11.385096549987793, 11.437653541564941, 11.105606079101562, 11.466428756713867, 11.312410354614258, 12.1935453414917, 11.199122428894043, 12.168102264404297, 11.259422302246094, 11.30173397064209, 11.225958824157715, 11.225737571716309, 11.910642623901367, 11.25097370147705, 12.125663757324219, 10.984146118164062, 11.088379859924316, 11.296960830688477, 11.007105827331543, 11.179228782653809, 11.26980972290039, 11.493245124816895, 11.075591087341309, 11.212971687316895, 10.987381935119629, 11.149529457092285, 11.177410125732422, 11.192514419555664, 11.623148918151855, 11.48077392578125, 11.181658744812012, 11.090802192687988, 11.301154136657715, 11.446431159973145, 11.034174919128418, 11.296792030334473, 11.207476615905762, 11.63041877746582, 11.449629783630371, 12.552573204040527, 11.14754867553711, 11.338340759277344, 11.186177253723145, 11.17708969116211, 11.260741233825684, 10.936689376831055, 11.223228454589844, 11.188166618347168, 11.186995506286621, 12.111326217651367, 11.439132690429688, 12.351851463317871, 11.190092086791992, 11.23644733428955, 11.213278770446777, 11.52596378326416, 11.199153900146484, 11.13354206085205, 11.495291709899902, 11.149306297302246, 11.654240608215332, 11.283581733703613, 11.179335594177246, 11.974190711975098, 11.273078918457031, 11.272359848022461, 11.39309310913086, 11.160774230957031, 11.560280799865723, 11.178486824035645, 11.317815780639648, 11.654827117919922, 11.660634994506836, 11.26927375793457, 11.461180686950684, 11.437878608703613, 11.29328727722168]}\n",
      "Minimum validation loss: 10.261106491088867\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB2g0lEQVR4nO3dd3QUVRsG8GfTE0ijEwhVQJqhI6ICgnQECxYQwS5iQayoKNgQxC4fCipYaCJNqdJC6D10AoGEACEJBNL77nx/THZ3dne2ZrYk+/zOycnu7OzM3Tbzzr3vvVclCIIAIiIiIhfxcXcBiIiIyLsw+CAiIiKXYvBBRERELsXgg4iIiFyKwQcRERG5FIMPIiIicikGH0RERORSDD6IiIjIpfzcXQBjGo0GqampCA0NhUqlcndxiIiIyAaCICA3NxdRUVHw8bFct+FxwUdqaiqio6PdXQwiIiJywKVLl9CwYUOL63hc8BEaGgpALHxYWJibS0NERES2yMnJQXR0tO48bonHBR/appawsDAGH0RERJWMLSkTTDglIiIil2LwQURERC7F4IOIiIhcisEHERERuRSDDyIiInIpBh9ERETkUgw+iIiIyKUYfBAREZFLMfggIiIil2LwQURERC7F4IOIiIhcisEHERERuRSDD6Vc3A0cnO/uUhAREXk8j5vVttKaP0j8X6sF0ORO95aFiIjIg7HmQ2k3k91dAiIiIo/G4IOIiIhcisEHERGRkq4cBrZ+ApQWurskHos5H0REREqa10f8LwhA3ynuLYuHYs0HERGRM2SccncJPBaDDyIiInIpBh9ERETOIAjuLoHHYvBBRERELsXgg4iIyBlUKneXwGMx+CAiIiKXYvBBRERELsXgQ3GsZiMiIrKEwQcRERG5lN3BR1xcHIYNG4aoqCioVCqsWrXK7LovvPACVCoVvvnmmwoUkYiIqBJiV1uz7A4+8vPzERMTg9mzZ1tcb+XKldi7dy+ioqIcLhwREQG4dhZY/w6Qm+bukhApwu65XQYNGoRBgwZZXOfKlSt4+eWXsXHjRgwZMsThwhEREcS5QkrygLTjwJNr3V0aogpTfGI5jUaDMWPG4M0330Tbtm2trl9cXIzi4mLd/ZycHKWLRERUuZXkif9TD7u3HEQKUTzhdMaMGfDz88Mrr7xi0/rTp09HeHi47i86OlrpIhEREbkeBxkzS9Hg49ChQ/j222+xYMECqGx80ydPnozs7Gzd36VLl5Qskuvxy0ZERGSRosHHjh07kJGRgUaNGsHPzw9+fn64ePEiXn/9dTRp0kT2OYGBgQgLCzP4IyIioqpL0ZyPMWPGoF+/fgbLBgwYgDFjxuDJJ59UcldERESejV1tzbI7+MjLy0NiYqLuflJSEuLj41GjRg00atQINWvWNFjf398f9erVQ6tWrSpeWiIib8aTGVURdgcfBw8eRJ8+fXT3J02aBAAYO3YsFixYoFjBiIiIqGqyO/jo3bs3BDui7+TkZHt34T775wEZp4EhXzJxlIg8D49LVEUoPs5HpbbuDfF/2xFA07vdWhQiIqKqihPLySnOdXcJiIiIqiwGH7JYtUlEHogJp1RFMPggIiIil2LwIYdJXURERE7D4IOIiIhcisGHLNZ8EJEHYq0sVRHeHXwUZskncFXkB75tOjB/CFBW7Pg2iIjkMOGUqgjvDT4uHQBmNAaWP63sdrNTgIs7gZMrld0uERFRFeG9wceub8T/J5Y7Z/vqEudsl4iIqJLz3uDDIrarEhEROQuDD2dh2ywRKY0Jp1RFMPiQwx84EXkiXtRQFcHgQ8vgR83gg4iIyFkYfGgpfkXBKxSblBUDZ/8DSvLdXRIiInIRBh86DBbcYsM7wKKRwLIn3V2Sijv7H/BNe+DibneXhIjIozH4AAB1GSBo9PfZ6uI6B38V/5/b6N5yKGHRSCArBfjtPneXhKoq5qNRFeG9wYf0R/xDFyZykXI0pe4uAVVVPE5RFeG9wYe0euNmEgybXRS4unD1QeLoEuDgfNfuk4iIyAF+7i6Ax5A2u1Q26jJg5fPi7VuHANXruLc8REREFnhxzYcRaU1FZWtXFdT628W57isHERGRDRh86Cjc7OLSAKaSBUtEROTVGHxoKd3s4tKcDyahERFR5cHgQ6siwYK7M9DdvX8iIiI7eG/wYdIsUolzPljzQURElYj3Bh/GKtLsIlvz4MKAQFp21oIQEZGHY/ChVZknlmPAQURElQiDD60rh/S37W52cffJvzI3GRERVVXuPjd4Li8OPoxO0gsfcnxT7q55kO7f3WUhIiKywouDDydzV1fbRQ+7cL9UKZUWubsERED2FWD1BCDtuLtL4kSsiTaHwYccuwMHD6r5uHHefeUgz7f1U+DTukDSDneXhLzd308CR/4EfrzT3SUhN/De4MNibkRla7qobOUlt4mbKf7f8I57y0GUftLdJZBXFZquz28D/nwIyEpxd0nM8t7gwxJLX76yEvvWdwV379/TxH0B7PjS3aUgosqmtBCY3R3491V3l6Ri/hgBJG4CVr3o7pKYxeDDFho1kHUJuJ4IfFIbmBoOlBW7u1Qkp+AGsPUTYMtHQFGOu0tDRGZ5YD7E6X+B6wnAoQXuLokycq+6uwRmeV/woVEDC0cCJ1daWMmoJmHJKOCbdsCv/fXLEtaZX9/VWPOhpy7V39aUua8cHs8DD/zkXue3AV+3F/97o+wrTjhmuPvY7Lm/cz93F8DlkuKAc/9ZXsf4ZH52g/i/IFO/TKM2v77LuXv/HkSay+P2z4WoEvljhP7/1Gx3lsT1zqwVLzLJZbyv5kN6ZWyWDSctq4N5CcC5TWL+gbNPgjzJSkg/F74vZnnuBRF5DQ/6fe782t0lcJ7LB4G8a+4uhQnvCz5sGQHUpt+EDSe5hQ+J+QfWaloqzIN+xO7Gmg/3y78udqEsyXd3SagqiJsF/NyP3ydHZJ4Dfu4LfNnS3SUx4X3Bh02XfErUfEjkXLF9XUdUZFK8yk6jBrbPBJJ3lS9gzYdtnFj18fsIcfCo9W85bx9UBdj4Hdz6MXD5AHDoN+cWxyk8pIrRA88R3hd82BR72HLS8qArbHfv352OLgG2fQosGCzeN6j58LwfnFdILx+x8uQqtxaDKpHFj8kPYyClNtPDsLQQKCnQ3888D8ztLfZcIY/lfcGHUpGotZoPdw2v7m2MR3TlPDeeg8Ef2SphHXBsqf3P06iBGU2A6Q31+XwrXwBSjwBLH1e0iB5JU3l/Y94XfNjUXGJnzYfV9Z1c9eaMk2z+dWDfT+K4GZWKYOY2GXDF7MfeFnwIAhC/GEg/5e6SVE4lefrbGWdkas5kvrMleUBZESCogfzypMrCm84qoSj1iB0jhzrxGJS8E/goEoid4bx9OJH3dbW1JRCw5WRuT2Lj3jni+p3HWd+uQ5zwBV/0CHDloJgs+/hy5bfvLErXfOSmA9fPAk3udM0Juyrxtpqn0/8Cq14Qb3tbV1UlSL8v/+su/g/+R7/ME35/Ny6ITTqA+z/jBUPE/7GfAZ3HAqH13FseO9ld8xEXF4dhw4YhKioKKpUKq1at0j1WWlqKt99+G+3bt0e1atUQFRWFJ554AqmpqUqWuWKcUvNhxfUEcbhe6dggSnLGQf7KQfF/4mbL62VfATZPA7IvK18GW1w/a7RAGnwocOX9dRvgt6HW34dKhzUfirsa74KdVKGAzuRYLPPa7Jnx1hXBrqfOwPtlK+DUP9bX8yB2Bx/5+fmIiYnB7NmzTR4rKCjA4cOHMWXKFBw+fBgrVqxAQkIC7rvvPkUKqwwn1HzYekBw2o/DjQekRQ8DO78SR411B+OkMkHh4EM74uH5rRXfVlUXvwg4OF+yoAqdKMkxGrXtFyZyx0eVp2UGuLH2Ze3r4tAN5sROd11ZFGB3s8ugQYMwaNAg2cfCw8OxadMmg2U//PADunXrhpSUFDRq1MixUipJsZoPRzhpu+6s3k4/If7PUKidOz8TOPQrcNujQES0AxtQOPioqpSuwi4rAVaNN1zmze//mbVAi/6Ar784PsXRJUCrwUBY/Qpu2AOaHuzx95PAqdXAw38AbaxdhMoFH3a83l3fAoNn2lW8SiPzPHDgZ/F2n/fMvC+V67vh9LAyOzsbKpUKERERso8XFxcjJyfH4M+5FKr5cKSrrVJBQmo8sHK82OQhbliZ7XqClc+L0b22PdNeStd8kG00MiMHe/P7v2SUOP4MAGx8F1g7SRwoqyrLPA/89QRw9ah+2anV4v9d31h/vuzxUWXmtsxz9v9kQyEryJGgvfAmUFpk/nFBADZMBnZ9Z34d6USmFWm+V3vOfFdODT6Kiorw9ttv47HHHkNYWJjsOtOnT0d4eLjuLzrakatdOyh1xWet2WXdG6bLlDoYz+0FHF0ErHiufLtVKPi4UD6pVdZFBzfgpOCjKr3HzsD3x9SJv8X/ZzeK/3Mui73HNk8FriW4rVhOs+hhMdiY28fBDVhpdpE7dsv+xu34LqpLxSbjywdsf449inLErsCWRhhNPwns/R+waYr5daSv3dzkd9bObasnAF80E3syegCnBR+lpaV4+OGHIQgC5syZY3a9yZMnIzs7W/d36dIlZxWpnC01HxrrA944VMWl8AH62mnnbNcZNGqxi5qzI29Pqflwdne/ClO4itabaznMkQvI1k4S5xGZ3d3RjVaoSE6VmSj+Fyxcmf/7KrDpQ/nH5L5Dzu7hcnKVndNfSIOA8vJe3ANseNdwoDOt1CPi/yILPWP2mj8/yu7X7Ptr5b068qdYjsOeMVKsU4IPbeBx8eJFbNq0yWytBwAEBgYiLCzM4M+pbPky/zUG+KQ2UJxr23ZsbnZR+ACt3V5luOrcMk3sorZ2kpN35CHBx/Yq2vZsjqUTjrtdOeSi0S7NHVskyy+X9yLzxCDCqcmdKrFZ5tACsQlGo4FNAbDVwRwr+BuXji1iL20NxPyBwN7Zjk9OF/+n9XVsqfmw9b3wkPOF4t82beBx7tw5bN68GTVr1lR6F65zfpuFBx2IyCvyoZcWAX8+COz9UbK9SnS1uetb8b+zo25PqfmwdKXjCZS+ovTkkRbn3SOOdpl+0t0lUYATawKc3bNELa1NljkWWs35kFHRE2lFjhHGQUDmuYqVBbDweqTBh5lA35MvAGTY/W3Ly8tDfHw84uPjAQBJSUmIj49HSkoKSktL8dBDD+HgwYNYuHAh1Go10tLSkJaWhpISa80YLmLPl/WvMcDpNfKPOdTVtgJf9CN/iGNNbHhbsr3y/ZbKVPdVVhWOyj0k+PA2leG9vnHB3SXwbJaCj9R4YMvHpjPLCoKY12ALaxcGss0u0jLJBSJWjhf5mcCaSWLtl2yZ7Pzeqmxp/qgAW45/Fa358BB2Bx8HDx5Ex44d0bFjRwDApEmT0LFjR3zwwQe4cuUK/vnnH1y+fBkdOnRA/fr1dX+7d+9WvPAOsfcDWjpayZ2LWcuOnGDlqge1r2XbZxUr1o0LwA9dgcO/V2w7nsBTaj48ntI5Hx5w1bV5KrDoUQtXhp5R3ey5LHwn5vYCdswCthsN5b32deDzaOCileO7cU2brYmi0ufZnHAqsf4t4OAvYu2X3Odv6TthrSbGXBBQEeZej/T3ZW6/zhrE0knsHuejd+/eECx8YJYe8wwKlc+RYbyzUoAf7wJuexh4YK4CZSj/omqz6R219g1xpNB/XgY6PVHxcrmK7Bwazgo+ZD7j1Higeh0gLErB/Tho309AcKT43XJEzlXxKjO0rmPP94QDn7bNPXkH0Ky3W4viPA4ev1L2it/VGs3Mr2NLs4tx09XBX8T/9l4AydZ8aP9LXqO1Mlk79kpHJM26CEQ2MbNTuW1rAJWv+cdLi4Bgy7s33J7geA6Lxobgw+bjnWecoz1t+DjnU+yE5MAHuO9H8XmOzN4oWwQbX0tpIZAUp5/10ViZhT7ogFh16Sr25CLM6WG6zJGaj5xU+6+Kr50Vrwa/am3f8+Sc22Q0MqidbiSJV3grnnXs+aWFwFe3it0BHQ0ilK75UJcCvw0Th+63l6Weamf/A5aMdlF3w/LvlLvmJNGODXH9HPDrAOC7juIFUGGW/Pq2BB/mfic+Fk7SNm+nfJn0O2g1+DD6jRsPfy59XO67ben7Li3juc3ixc51SRfp7zpaLpul7Zldx8wxKy9Nf9tbm10qPaVqZjzhg9aWwdrBbfkz4oF8iwMHckC50UttIf18Cm7Y3p6s34D8tsw5sVwMIFa/ZGVFo/dYO/dNRZQWAXGzgIUPAWsmAlePObYdR7r1Sr8z0poz6WBG9lC65uPsBjFg3vmVDd3eLTD+DiwaCZxZA/z3fsXK54kKs8RazEsHgMN/AJ/UAU6uNDwhf9MemNFY/vkVCZJsSla1lvNR/rhBIGutTEaf74kVRg9rDG8vexLYP89yOYwfSz8FLHxQvNjZPFX/eFmhlbKZ2Z4j6yx/Rn+7os2Kp9cACRtsW9eJvC/40A78U1GONLvYGvfsmQ38dp9Rv3EH2ju1zpQnze7+Xhzm2V7uas+f2VRsT7aH9LOw5YSorS62pbubAQWuZnd/D2z9WH8/N838upZUZCbfohxg2Vj9fUdPQEoH49Ig6Icujm/HXLlyrzq+TU+1+UPgwDzgl37AP+XB9LJxtn026rKK9XbJsfZ+2pHzIb2yN/g+2nAMND5WSR8/uQo4ucJwAEhL78210+JI0mcVOlHbchw1Vx7pBYalmo/UeOvH+KvxwOJHHb/QUIjdOR+VWsZpcaAVJTiz5mPju+L/w78Bt483v57u5GrHCWPl80DMo/aVx93t+ba0lco+z5YrDTe2f5rMgmpDWQpvisNXN7kb8NGeLIyCD5veq/J1Coya1Bx9P5z5HXF4tFtUweRTC5+to6Ombp4qDnRlrfnVEt2Ah2aoVDb0drHS7CL3vTYOeow/b+l+irJkCmbhezCvr/y0AVL21DoKGuu/T+P3JeuSftRnLbPBh1psCratMGJzq1+gjesrz7tqPrJSFNyYYOa2rc+xgXG3NnPbc3absrubmBzdvzMTTiv6npcW6WukdLuw4fsxry/w+3AxOTjjtOnzbH3NuvIb7dPh91rB4ONGkvkDbEmBOPePduRI+cLobzqjR4KtdJ+LmQEJ475wZKM27M/I8qfNP+fyQTFR1zjwUKJaPsvCaNWWaj4sNbtoNOIAfhdigeuJwK/9jR5Xm68JvBBruktL3w9rgQcg/h5tlbgFmBYBTA0HctPl1zF+X+b0FH/rBuVSKOfDnb8NeFvwoWT3wrwMIHaGOF10JRtZzm6eUPPhyLo2fS6OfibWvktWHje+mrG1LDfOi//j/wT+d7vp82w+AJWXz+S9dfD9sGW/uemWT0iAOBLpdx3EGjo5cTPFk/bc3ua3IU3olh5gr50xXff8NjEPQIkEVHsDUkvTo7vKz2ZOnosfqdh2Tyw3rFUz/p4JgulPRLbmw2ilkyuAbZ+KAfghmSTtxM363whg+L3UzsCtdfxvYMtHFl+GVdJ9WSMdtuG/98QRtC8b5Y4JGjH/TPvdKJYZrNBsbV7lCj68q9lFyRoC7TDh8X/anhBnLkHp+N9A8k5g8CzAV/qR2HoicHbNh6XgQwWnd92y60dVScb5kMvvkD1A2zgXkdxtW5js08H3zFqAWlasn1xr8mUgMFR+vT3/s7wdW5JyTywHHvpVvC397m771HTdP0aI/30DgAcUnhVV+9lZ+gyPLQNuG6m/X5gldhVu0V++SrysyPB7kbBB7On0wDzTdZ3hZpJ44VW9juX1/n4KqNNWf1/QwPpvUyb4MCatvd7zg+njxqOMWvo+W6oRcra8DGD+YCDN6PucfkIcUBIA7jGTFG0uaMi/Zl8ZzPV+dBEvq/lwgqwUw25QlpxcKb98+dNiFH98meFym2MPJwcfxgeDY8uABUOBvGu27fvALxXbvz0nROnJNOeKfeu7SvIusXeLMbUkAez8VuDLVraN4WJQ22NvLZVMwOMIS/vNTQNmNNXfz75sYTuONvuYKbfZE5nR+trvikYj9rIqvAn8Ogg48LNj5ZGWydJ7uuIZw/uLHhGHgpf2qrhkNOPqtAjg2xjxBLb4ETEn5s8HHC+nPW5cAGa1sG3dDMmYIIJxc4iNvV0M3juVA116XVhre3Kl7RNnChrTwAMwnJbB3JQFxk1LjpKtfXUdLws+3NTf3lb5GUYLPKSZxvgHvOIZ8eps60ew6T21azI5C/3/7X3+qvHAsb/seK4drAVd6ceB3T/IX13s/k7+OcvG6Xs4/XE/kJcuTlNuiSDA5tqepB362yozzS7WTv7Zl8XaCeNJFy3N7bJqPFAqyV+yOKqkteDDzHPNDs5kZ9XymoliL6vlzwIpu8URPJVk6XuTvAu4tFe8Hb9Iv/yXfqbr3kwG5tyhv281P8yIIFifj+esPbO9WqFRG31GMp/jjlnAvxMNPzPjni+WBv2Sk2cmt8JZdsyyLTAw971c+rj+trmack2ZWLtXUca5JC7mXcGHh8ceFr+05g5agmA5S12JCNnc1WPBDWUnozJ3MHS05gMw7Moq/wTbt21Ohkym/9WjYrvu/nliVrnBLi3s89I++/adeR74Q3LVK/delZZ/P2S7DNr5+n/uB2ycDGx4x2gzFq4wC27Yvn1rn7W5986WkSEt0X6PtRMfJm6y7Xk2bduGA4+6FFgwWH/f+Dsjx95qdilBYz2hUi6nQmvBUGD1BPv2Z1CjYebzOjQfKJZMJSG9cNGU2V/zYc7CkdbXcUTsdODfV/X3zQ6FbkNQ/Ptw+eUZp/UTdVZi3hV8VGSwIlcw6bNuw4lh8WOWH1ciWdTSCUF2vgUHT+izWpipjnUw50NJJq9J8rp1iZ8yDv4KfFpPHPzJFgHV7CvXD50Nr5CM36sza4FP6xrOhgwAKXvK17ez5kM7PkbiVsPlajO/rbJimP1MbiaLVdUaK1fEtqhozYetTZdlJcC6t8TPM/Zz5ZrtjGvI1PaOwWBUA2aNRm29zT9hnWlCpFbyDvuGLTDphaKB2atB6cWU9PbGd+2v+TDnnIK1OsakM3ebC/AuH5Bfbss6G96Wb7KpZLwr+JBrZ3eXH7oCW40T4BzoeXB2veXHlchotjhWgtEBRF0K/HS3mHBmrwIzPQ6MD/A5qYZXF5bWNfcWnl4jDjrk7JwPbQLcgXli10Br/IMrVibpc7OvAH+XJ9VteNv0BHtpP0xzPoxO4hqNaRMLoK8pKCspr30zc7LMumT+9XwbIzY1SbscW3vt5tqp7Q0+jPdzIdawvd3cugd+Bvb/JH6esdOBBEu/P5mutmZXdXGPMkFtW1fSn/vqa84qtD+N4WeUfdnMuBuwPEu3j3edsqoy7/okXd3+Z8n1s2K3QUtJWI6ehJY/Cxwt726oyEHNQlW3cbPLpX1iVK5Em6R0P1o5qeJw6IcWmFvZyn2IB9Olo8WRPc0dAI2pVGJV5+zbxbluHEny3f65+TJJy3Z0seEye8an0b5X++cBX7cxajc2KvPVo/I9bKT+GA5Mb2haBpVKTHic2VTsFmvxKtrK91g6ZofZIMJac0wFm10AYOkY69u4mWz4mC1JzVL2JsY6KzhePcH25Mg1r1V8f4JRzsevA8yva6nJKSe14mUhj+BdwYcn+vY2/W3j48yxpaazSNri+F/AyufEXgZKd6cyGBxHriuo5P6lA+KkVsbsHVNBetD6qZeVde1M9jJ3tSu33U0fiCM57vrGtueYsCFg+aWfmKAptWGy7bvQvlfGORnmnyD/fK2kOPH/8WWm7+3h34CSPPF7alczgdF2QmqY37+15YAYiP54l/xj9vQCSdpufd8WaxJt+HxzzPT0kQs+Nr5nmICopBPL9cmt1hxdZH0da0wSTi2wlDy748uKl4VEHZ303bIRgw93k15RGv84byYZZrTbqzDLcBIlR0nbR1MP62+f3QCDA27mecBHMk7JL/3k5+WQG2nQViY9gozZ0HXUloNgwnrzw1W7enAeuWYPc+zNzzE+6alLgF8HiqMwnpaOwKoyDILyrxmeJMzmUwnyFR/S/foGSFY3EzzK5ZQIgtg76N9XzQ/2lG1mUDNBU97sZANp7aHxZy8I+gDfZGjsFOB/PYBiGyZHlKuh3POD6Si4lkiTlW1JBHdWYCPHuNnFEulcQ+Q8w2e7dfcMPjyJ4oNiCcA2mVEU8zKAC9ttr9I1Nz4JYNiF8vcRRoOkmWFvEqw974ulcRnyr4uv2VpTVMo+ceKl2d3kH5c7sNtSw+ToeCxlRcCu70ynC5eje6/k9mXDxFyHf9Mno0pHZFSpgH1z9Pc1ZYbBh7mEU+OuwICYa3PlkP6+tJnJ3Gf9WX0xeVbq14HicnMsBRcXdwG/3Gv+cSnp99X4u7v+TWBWS3HI7x2zTJ+bccq2pj2lA1pPG2DPnuCDnC/aQpK8i3hP8GFr+6ZbmQkGHG1zNZctPbsb8Pt9YjY7YHpQWPeW6XMytVeWFk6g2TbmJtibh2LPQctcLsjJVcAXzYEt08znD+RfF/+uHrW8D5WP6fDfn1gZ8VFLo7G/Hf/SPmDTFODHO62vm7DefNW0bM8ko8/CXJu6dNArQDwJS3siWGp2MX69cTMNT/wG87RYeG+WjDK8b6nZIOeq7cGFNdaaXQpvyI+eag8lc6Q8kUbt/mkaqqrGNhwXjD38u/LlsJP3BB/Gw+56InMn2YO/2lf1rmVuEBntTIzaanXtlS4AbJsuZvMbWzDEtn3acoDRdrvLkJlrQ46gEScbq0jynXawqJ1fmw9+vmgu/lmbRfX6OZnqdxsCpGNLxQTNm0nW13XU2knifBVyPRlKZHoRGL+ntowvIT7R8G5Fc4tuXpQvj6O+ulWZ7QCWm10qyjdQfO3/mRlKu6oQ1LZ1LyX72XLsaVqeK/d2MvBeGhBa16lFsoX3zO3iF+TuElhn6cDrjKsGuS5tuh4ZRnKvitXs1poObDk4C2ogbpZ8k5CcBUPF4LHHS7atb7gz8Z80sfSwlahfbs4IKWvdmy0pyrK9h43S5IJK4+/VqVWObdtcV1tbx59Y+QLQ513PrJrX1pQV5QAn/lZ22zWaGiadV1WCBtj9vbtLUTWZGwlVauw/zi+HnRh8eBJNmfmxIHZ+pfz+bL7KLWdxGvNytlwBa8psDzwAfa2VtaDA0qRj0poAq6OeypA7cVd2tuS/2MpszoeNzUwpu4HfhgI1b1GmPEr6awxQq6VzylZkQzJqVVB4E4hoZFvyrTdodIf4nbdFVEegThvxeH1yheFj0bfbkITvmbyn2UVuhkhPs+sbccRKV9HOkGkXBWo+lJ4vQ+snM90tybyK1DTkSYb3NtczyHg2U2vsDYhd4eIucdhvS9MYOKrQjqHnPcHzcY49T1MmTkpHomHfAL3ftW3dJ1YDI/4HRDYxfSy0numykFpAr7fFJj0P5j3Bh3+wu0vgeQSN8s05npZUZssEWt5KpXL88/IPMWx+Om5mAr9zm+yb28XeQbtcyRmzRzsjoHGWtg8A9WOAlgPdXRJ5DVx04TboC8AvGKhtIa+og1E35m7PA68cAcbvBkYvB2q3Anq9BQRFmD535AJghKRnmV/5uau6TFL7oBnAA/PEdWq1Ahp2BZ7+T2zCDAqz95W5lPc0u1iLAts+YFqlVdXlXzc/6JE5SuR8uFJuKvBRpLtL4blsGWJbjqUhsKU2f+jY9j3Rlo/cXQL36l7ew+uxJWLQ+nFN95YHAFoOEoPg5vcAj68Qg9eSAmB2V8P1IpuYjk5rqxFzDAf+6zAK6PKUeKzb9a04xsytQ8Su4MfKR5aWNkOOmCM+R6tuW/G/SiUGJH89AYRHi/PlDJoJ3DrYsKy+/uJ/42a/qI5izUdoPeD9NNNy3/2W2BU8xsr8X27iPTUfxnMCSAdY6TAaGDlfjGi9ybXT4vwaSnL0ZEbuIZ1BlCqufgd3l8BUh9FAl6ctrzN+j+XHASC4PIhXqcTxfDqMNny8fvmxpP+nwLtXgdseMb+tRxUYNRUA6rYRe288vkIsV3hDwC/AdL3SQsf22XGMGDgM+06/zD9EfP3+QUDvt4EH5gJthov/x+8BJp02DD46jDLdrlZIDWDcGuD+OcBrJ8TAAxCDpRE/Ao8u1l/wGectaoMYc7o9C0zY7/bBxMzxnuBDqs1wcWjZh+YDzfoA95Zf0dRr595yebrEzbCa85Fz1SVFIYWUMPhQ1DNb3F0CU0ERwMDPgSc3AA/+Yvp4aH3xJN7HQndfv2CgRnPDZcNnG77eRxYCU7OBO14CAkKAIV8BYQ1MtzXhgFhTICe6u+H9RlZGeFb5ik3q0hpZuVruWi3N77NGM8P74Y30t4eXJ7lXq61fZmlyu7ptgLAooNMT4n3j12OPDo/pgxHAcCTgjmOA/laS9lUqsXnHR6GZgBXmncFHYKj4v90DwBOrgGq1xPs+/m4rUqWw82vr62x42/nlIGWkHmFTgpJGLbNthF9XueNloMldwN1viLUBjXsArYcB9dobrhfVUfzf603gwyzg9hfF+yG19OtMOmX62lQqsRZAq1otw8cDq8vPH6IdY+KFXeK+HpBMAdFqMBAoyVV4cp3hc4NrGN6XS7iUnqQfWyqW4f4fxfsmeRoq4IWdYg3Bi/uAZ7fKbzNcJoiy5Ja+wMuHgbH/2vc8S3wl56fBs/Q1UZWUB/1SXKDrM+IYD+bGi/CkAweRs8UvdHcJqhZtNfhdbwBnNwLpZobD7zhGnLDP2mB21qh8gac2inMoyYl5zPTq2C9QPNmumgBc2CYGI9LjoUoFDPgMaDkAqNtOvGr2CzKfsC/tRSg3nIFc1/uA6uL/eu2AgdPF2yueFf8HVhebTjJO6csj9fQm8f/VeLH82hoGKelJum4boJUkQXbsGnG+nHObgIS1wF2vAwHVxBoCrTteEvMwWvTXL6sfIzaDRDY23Z85NZtbX8ce0qDKt/JfKHvX2XbIl+KP0WzPF6Mv+psXxKvDhQ86vWiVhicOAuVt6ncQD76VxfDZ4hTuVZ32uNJ3ivi39VNxKHlArFXV5kPVbStW508NN3x+83uA81ut7ye4hthFt/c7QHRXsXYjeYf42HOxwNze4m1Lyd8jLOQBqFRAs97WywGIJ9h+08SeGLLJ6JJu1v2mibUjcs0Afd4DErcAMaOAJneLkwX2MprmoXZroFZ50mWtW4D2D8mXSXqSNq7Nrl4b6PKkmK9yNR6I6mT6/DbDxVqLiEaGyzu4OXGz9q3ieB/BkR7blGIP72t2sdTl1nigpGo1gRb9DKsFpTyxfdfZFo10dwnoqQ1idW5ILevreoJGPZTf5msnxROVVN8PbH9+qGRCOu3Q01pvWRj+fti3YnKjj8x1W0A1w/v3vAe8ekxMvpSeyOSCgt6TgTEWJnAcMB2AStzvxOPA8zvEGhbAsDeDNOHVVQMr3jnRfFJl62H62z1fNT+Ne6+3gKc3irkitVsCT60HmvcRH3thJ9DtOTEx0xb+QeIYGj0nAmFmJh70CwCiu5mv7a7Z3PNqF3z9xKaqcWutr1sJeFfNhzXSaPKVeP3t9iPF+VW0c6DcOQmoXhdo2MW+q9CYUcBRhbK8ybn8qxnO2OspnosVA+imdwNvnTe9evZEEXZUVdti3Fqxav6e9w1/T3e9bnsOyyN/Aj/3FW+P/Uf/Pt71htgDwZyYUeKJ6700cdCsGxeA89vEE57cQIbaavrWQ8VumYD82Crawf6eXA8k7xLz0b6XXJW3uQ+IeVQMXAKrA/UlQ7JrmwfqthdrH+79CMhNN2xKcJcGncVxK/xDHB8npV57YLCdPRF7V9HcM0vJrpVM1XklSojqJF5F9HlPnHNBS6USR5m7c5LYxtrvQ+D2F8THuj2rX6/L08AtkvbXmrcYZmvfLxk4RkqaSU3y6lawJ1L9GHEb/abatr4zAg/jalx7dXpCnxyoVV/hrtKWNL9HfjZMbRu+nPeviVdsfhZqHPtNM112n9FQ+ndIJklsUj6LZ3gDMQgYvRx4L11mw5KT3Yg5+t4aMaPEk2L7keLVsZSlWpr+n+q7cfr6iyf3VoOAwTP1PebMkY5mKVdl3rW8K2zjO8TEz5rNxVoTLXWpGBTJDTRVvTbwziUxMAXEGoaBn1kujyu1vV/MISGSYM2HlEqlz4o25hcoBh3GYkaJVy3R3cXqQkB/FeUbANRpbXkM/37TrM9ZQmI3ufQTjj33rjfEgBKCeOBv2guY18eODajE2SBnVOAK3jdQrDpfOtr6uubInaSf3QZ8ZOFKHQBqtjA/q7Otgy+FRonNAnJTv9/1OrClvGzhjYDsFP1j2pP1q0fFMqx/xzQRU1ttP6uFflmnMcA/5YmQA6aLPTXkJibzDxabRo016y32pFj0sHi/wyjTpoEHf9bffmwJcO2M2EsBAMatE5Mew6PFmoY6bSzXiFjjHyQOeX1mLdBprLjs/p+Aje+JtTByQUVEIzGwKy0Qu29a4uGjWRIZY81HRfn4iAdKbeAB6Ntam/UxrEGRan2f2Cf+zonyjzexME9JHSuDy1RFjra/Dp4lJv/5+OivOBt0MqyResLKjI8qHyA4ApiSKdaAOaJJT8tJYnXbA9Vluvhp3T9X/uRnS+KZjy/Mjs/yyJ/Wnw8Ao5aI/9Uy+QpdntTfbvcA0E4mQTu0rlhjMcjMrMlyJ9/n48SAq9uzYo3PiDm2t3dXqyOfl2FOq0HAna/pmwaa9BT322qgWO6KBB5afd4Fxu/SBwoxjwJvJoqBlRyVCnjzPDD5cuWYm4rIDgw+nOHFPWJ3tXveF8f07/qsOAaAVOOe+tvtZZI4x/4rjpQ3YLrpY9KDfVUSZCF/ISDU/GPtH9bfnnhcHHJZy1zQIj1pW0uIVJX/THz9xCvqF3aJQY1UeCOg3UNAw25m9mflRBjZGHj9DNBmhPzjze8x/9zRf4ujSZpLgFb5Ag//JjYrvrjXqNzRlssVXEPcrrZ5p819ho+/nSxm33d+UvyMur+gv7KX0+RO4N1UsSYkqhPw0K/6x576DwhrqP/O148Rg3PtZ9hhlL7JxZz7fhDzsO6dZnnuDU9hLQ/CP8g0kZWoCmDw4Qw1mgE9JoiZ234BwJBZQMv+hutIB/rp+6E4uqC2tiOikXhQCosyDTTa3g90Hme4zJ01IUoe4C29jkCjvIKer+oDuHunAZFNxbKER+uv0gHbZkn1CwAe/sNwmTQrX2X0M6nXTrwqbtBFvN+0lzhHw0O/iJM6yfHxM51B+O439bfrthM/c22Q4RtgGJRWt5AX1OJecWjneu2BEJn5Nnx8xe6Dz20TmwFH/qZ/TC44C2soNhE8sRp464KYWK3lHyyOltn0bjGI0A50NOwb4O0ksXdBs15ik4I2B8FYQDWxuee5bYa1JI26A5NOAj1eNP9arek0Bnh+u/jbCW8gDhr18mHHt0dETsHgw9Ve3Cue6JpIaj78g8Qks3FrxCvCF/dJHgsWE121Bs4QTxjSSYbkrgaNA5SKJmwai2gsNhM8tsT0sejuYoa7HLlpoW1hnNRYo5k4YNDkK+KJ5qWD4oyRxleScqMVAoZjAQDiFf2tQ/X3pfMhGA+/rDXqL3HOh0f+1HfZk+5/lGSmVx8/w+2ENxJrxp7dJk4ApW1+6/i4WBvw8mH7eyv4BQKTzpgub2EU+EqTVo3fB0B8LX3eFWt55K7Mbx8v1swZX5FLA5mYR02TY92hQWflB3siogpjwqmr1Wkt/pkjd3K+Zwpw+l9xgC/jIYwB+YmUpH38P7gp5jwkrAcWP2pfeW8dKo4IaOz57eKcEXInp6c2isuXjTN9rGYLwwTH52KBK4fFPv0b3jVdX8s4+IjuLr4mbY2IcX/9J1YDKfuA1sPlt9f/E2DZZcOBjIZ+I+Z2dC6vbXp6M7BjltiEJqdaTaCzhSaGepLukD5+QJ1bxfkv0o7rezc06CT+6dbz1dcG9HgZKC2ybwpzvwCgeV9x0KlnNgNXj5pO8BXZWAx8/ILlaz4q2iuHiMgKBh+VgY8PMGGfWG2vzVWoeQuQmSjevu1R054A0jkXtH3DWw0SayTWviFWjcv1XNAaPEtsYw+oJj+WhKV5BbQByUsHxSG8pXPCGI+QGtVRf4Xc9G5xemxArE1IWK8fAlza7PLMVssBHCBetVsapbFuG+Cl/YbLqtc2rPGI7gqMWmp5P3Je3AsU5YhNECE1gYJMcZwHQPzfeqjl52v5B4nJsvYa/TegLhZrzcx1xZU2+Wh1eBwouG6az0JEpDAGH5WFca+GYd8CGyYDbUfIz8Z7x8vA5QPiCV2q7f1iUqO6VBzs6NQq/WP3fizmD/gFmm9qkDNmlRj8nDdKeKzVQsyHMAg+ZAZY0ur+PFBWKD6nYRext1DhTbFnkLSKXzrAkieSBkYT9os1HbYOV60EHx/Ax8K4GsYe+Bm4uEucfqAKDNtMRJ5PJQjGWXDulZOTg/DwcGRnZyMsjH3XbTY9GijOEZM2Ry2xvepcEMRmkOBIsclBzj8vixPySU3NNl0v47Q4I6V0BsikOOC38iGWgyLEJhkAOPKHmFdgaya/uhT4NkbsffHCDsdHSyQiIqew5/zN4KOquH5ODBDueMVyzwhHlJWIE+ydWg3sLW+WkAs+5Ny8CHxbXlOhzT1xlLoUgIqzDxMReSB7zt88ilcVtVoA/T92zrb9AsRukHXbijUO5saikBPZWOz1ERxZ8XkJPG2iJyIicgiDD7JdYHVgwKf2P4/zOhARkQTH+SAiIiKXsjv4iIuLw7BhwxAVFQWVSoVVq1YZPC4IAj744APUr18fwcHB6NevH86dMzOpFREREXkdu4OP/Px8xMTEYPbs2bKPz5w5E9999x1+/PFH7Nu3D9WqVcOAAQNQVFRU4cISERFR5Wd3zsegQYMwaNAg2ccEQcA333yD999/H8OHiyNL/v7776hbty5WrVqFRx+1c3RNIiIiqnIUzflISkpCWloa+vXrp1sWHh6O7t27Y8+ePbLPKS4uRk5OjsEfERERVV2KBh9paWkAgLp16xosr1u3ru4xY9OnT0d4eLjuLzrayhTfREREVKm5vbfL5MmTkZ2drfu7dOmSu4tERERETqRo8FGvnjh9eXp6usHy9PR03WPGAgMDERYWZvBHREREVZeiwUfTpk1Rr149bNmin2AsJycH+/btQ48ePZTcFREREVVSdvd2ycvLQ2Jiou5+UlIS4uPjUaNGDTRq1AgTJ07EJ598ghYtWqBp06aYMmUKoqKiMGLECCXLTURERJWU3cHHwYMH0adPH939SZMmAQDGjh2LBQsW4K233kJ+fj6ee+45ZGVl4c4778SGDRsQFBSkXKmJiIio0uKstkRERFRh9py/3d7bhYiIiLwLgw8iIiJyKQYfRERE5FIMPoiIiMilGHwQERGRSzH4ICIiIpdi8EFEREQuxeCDiIiIXIrBBxEREbkUgw8iIiJyKbvndiEiInIFtVqN0tJSdxeDJAICAuDjU/F6CwYfRETkUQRBQFpaGrKystxdFDLi4+ODpk2bIiAgoELbYfBBREQeRRt41KlTByEhIVCpVO4uEgHQaDRITU3F1atX0ahRowp9Lgw+iIjIY6jVal3gUbNmTXcXh4zUrl0bqampKCsrg7+/v8PbYcIpERF5DG2OR0hIiJtLQnK0zS1qtbpC22HwQUREHodNLZ5Jqc+FwQcRERG5FIMPIiIiBfTu3RsTJ050dzEqBQYfRERE5FIMPoiIiMilGHwQEREp7ObNm3jiiScQGRmJkJAQDBo0COfOndM9fvHiRQwbNgyRkZGoVq0a2rZti3Xr1umeO3r0aNSuXRvBwcFo0aIF5s+f766X4hQc54OIiDyaIAgoLK1Y105HBPv7Oty7Y9y4cTh37hz++ecfhIWF4e2338bgwYNx6tQp+Pv7Y8KECSgpKUFcXByqVauGU6dOoXr16gCAKVOm4NSpU1i/fj1q1aqFxMREFBYWKvnS3I7BBxERebTCUjXafLDR5fs99dEAhATYf5rUBh27du3CHXfcAQBYuHAhoqOjsWrVKowcORIpKSl48MEH0b59ewBAs2bNdM9PSUlBx44d0aVLFwBAkyZNKv5iPAybXYiIiBR0+vRp+Pn5oXv37rplNWvWRKtWrXD69GkAwCuvvIJPPvkEPXv2xIcffohjx47p1h0/fjyWLFmCDh064K233sLu3btd/hqcjTUfRETk0YL9fXHqowFu2a+zPPPMMxgwYADWrl2L//77D9OnT8eXX36Jl19+GYMGDcLFixexbt06bNq0CX379sWECRMwa9Ysp5XH1VjzQUREHk2lUiEkwM/lf47me7Ru3RplZWXYt2+fbllmZiYSEhLQpk0b3bLo6Gi88MILWLFiBV5//XXMmzdP91jt2rUxduxY/Pnnn/jmm28wd+5cx99AD8SaDyIiIgW1aNECw4cPx7PPPouffvoJoaGheOedd9CgQQMMHz4cADBx4kQMGjQILVu2xM2bN7Ft2za0bt0aAPDBBx+gc+fOaNu2LYqLi7FmzRrdY1UFaz6IiIgUNn/+fHTu3BlDhw5Fjx49IAgC1q1bp5sJVq1WY8KECWjdujUGDhyIli1b4n//+x8AcfK2yZMn47bbbsPdd98NX19fLFmyxJ0vR3EqQRAEdxdCKicnB+Hh4cjOzkZYWJi7i0NERC5UVFSEpKQkNG3aFEFBQe4uDhmx9PnYc/5mzQcRERG5FIMPIiIicikGH0RERORSDD6IiIjIpRh8EBERkUsx+CAiIiKXYvBBRERELsXgg4iIiFyKwQcRERG5FIMPIiIiD9CkSRN88803Nq2rUqmwatUqp5bHmRh8EBERkUsx+CAiIiKXYvBBRERUQXPnzkVUVBQ0Go3B8uHDh+Opp57C+fPnMXz4cNStWxfVq1dH165dsXnzZsX2f/z4cdxzzz0IDg5GzZo18dxzzyEvL0/3eGxsLLp164Zq1aohIiICPXv2xMWLFwEAR48eRZ8+fRAaGoqwsDB07twZBw8eVKxschh8EBGRZxMEoCTf9X92TPo+cuRIZGZmYtu2bbplN27cwIYNGzB69Gjk5eVh8ODB2LJlC44cOYKBAwdi2LBhSElJqfDbk5+fjwEDBiAyMhIHDhzAsmXLsHnzZrz00ksAgLKyMowYMQK9evXCsWPHsGfPHjz33HNQqVQAgNGjR6Nhw4Y4cOAADh06hHfeeQf+/v4VLpclfkpvUK1WY+rUqfjzzz+RlpaGqKgojBs3Du+//77uhRIREdmstAD4LMr1+303FQioZtOqkZGRGDRoEBYtWoS+ffsCAP7++2/UqlULffr0gY+PD2JiYnTrf/zxx1i5ciX++ecfXZDgqEWLFqGoqAi///47qlUTy/vDDz9g2LBhmDFjBvz9/ZGdnY2hQ4eiefPmAIDWrVvrnp+SkoI333wTt956KwCgRYsWFSqPLRSv+ZgxYwbmzJmDH374AadPn8aMGTMwc+ZMfP/990rvioiIyGOMHj0ay5cvR3FxMQBg4cKFePTRR+Hj44O8vDy88cYbaN26NSIiIlC9enWcPn1akZqP06dPIyYmRhd4AEDPnj2h0WiQkJCAGjVqYNy4cRgwYACGDRuGb7/9FlevXtWtO2nSJDzzzDPo168fPv/8c5w/f77CZbJG8ZqP3bt3Y/jw4RgyZAgAsevQ4sWLsX//fqV3RURE3sA/RKyFcMd+7TBs2DAIgoC1a9eia9eu2LFjB77++msAwBtvvIFNmzZh1qxZuOWWWxAcHIyHHnoIJSUlzii5ifnz5+OVV17Bhg0bsHTpUrz//vvYtGkTbr/9dkydOhWjRo3C2rVrsX79enz44YdYsmQJ7r//fqeVR/Hg44477sDcuXNx9uxZtGzZEkePHsXOnTvx1Vdfya5fXFysixIBICcnR+kiERFRZaZS2dz84U5BQUF44IEHsHDhQiQmJqJVq1bo1KkTAGDXrl0YN26c7oSel5eH5ORkRfbbunVrLFiwAPn5+braj127dsHHxwetWrXSrdexY0d07NgRkydPRo8ePbBo0SLcfvvtAICWLVuiZcuWeO211/DYY49h/vz5Tg0+FG92eeedd/Doo4/i1ltvhb+/Pzp27IiJEydi9OjRsutPnz4d4eHhur/o6Gili0REROQSo0ePxtq1a/Hrr78anPdatGiBFStWID4+HkePHsWoUaNMesZUZJ9BQUEYO3YsTpw4gW3btuHll1/GmDFjULduXSQlJWHy5MnYs2cPLl68iP/++w/nzp1D69atUVhYiJdeegmxsbG4ePEidu3ahQMHDhjkhDiD4jUff/31FxYuXIhFixahbdu2iI+Px8SJExEVFYWxY8earD958mRMmjRJdz8nJ4cBCBERVUr33HMPatSogYSEBIwaNUq3/KuvvsJTTz2FO+64A7Vq1cLbb7+tWE1/SEgINm7ciFdffRVdu3ZFSEgIHnzwQV2LQ0hICM6cOYPffvsNmZmZqF+/PiZMmIDnn38eZWVlyMzMxBNPPIH09HTUqlULDzzwAKZNm6ZI2cxRCYIdfYlsEB0djXfeeQcTJkzQLfvkk0/w559/4syZM1afn5OTg/DwcGRnZyMsLEzJohERkYcrKipCUlISmjZtiqCgIHcXh4xY+nzsOX8r3uxSUFAAHx/Dzfr6+ipWvURERESVm+LBx7Bhw/Dpp59i7dq1SE5OxsqVK/HVV185NXGFiIioqli4cCGqV68u+9e2bVt3F08Riud8fP/995gyZQpefPFFZGRkICoqCs8//zw++OADpXdFRERU5dx3333o3r277GPOHnnUVRQPPkJDQ/HNN9/YPC0wERER6YWGhiI0NNTdxXAqzu1CRERELsXgg4iIPI7CHTFJIUp9Lgw+iIjIY2hzGgoKCtxcEpKjHQ7e19e3QttRPOeDiIjIUb6+voiIiEBGRgYAcYAszojuGTQaDa5du4aQkBD4+VUsfGDwQUREHqVevXoAoAtAyHP4+PigUaNGFQ4IGXwQEZFHUalUqF+/PurUqYPS0lJ3F4ckAgICTAYSdQSDDyIi8ki+vr4Vzi0gz8SEUyIiInIpBh9ERETkUgw+iIiIyKUYfBAREZFLMfggIiIil2LwQURERC7F4IOIiIhcisEHERERuRSDDyIiInIpBh9ERETkUgw+iIiIyKUYfBAREZFLMfggIiIil2LwQURERC7F4IOIiIhcisEHERERuRSDDyIiInIpBh9ERETkUgw+iIiIyKUYfBAREZFLMfggIiIil2LwQURERC7F4IOIiIhcisEHERERuRSDDyIiInIpBh9ERETkUgw+iIiIyKUYfBAREZFLMfggIiIil2LwQURERC7F4IOIiIhcisEHERERuRSDDyIiInIpBh9ERETkUgw+iIiIyKUYfBAREZFLOSX4uHLlCh5//HHUrFkTwcHBaN++PQ4ePOiMXREREVEl46f0Bm/evImePXuiT58+WL9+PWrXro1z584hMjJS6V0RERFRJaR48DFjxgxER0dj/vz5umVNmzZVejdERERUSSne7PLPP/+gS5cuGDlyJOrUqYOOHTti3rx5Su+GiIiIKinFg48LFy5gzpw5aNGiBTZu3Ijx48fjlVdewW+//Sa7fnFxMXJycgz+iIiIqOpSCYIgKLnBgIAAdOnSBbt379Yte+WVV3DgwAHs2bPHZP2pU6di2rRpJsuzs7MRFhamZNGIiIjISXJychAeHm7T+Vvxmo/69eujTZs2Bstat26NlJQU2fUnT56M7Oxs3d+lS5eULhIRERF5EMUTTnv27ImEhASDZWfPnkXjxo1l1w8MDERgYKDSxSAiIiIPpXjNx2uvvYa9e/fis88+Q2JiIhYtWoS5c+diwoQJSu/KYYkZecjIKXJ3MYiIiLyS4jkfALBmzRpMnjwZ586dQ9OmTTFp0iQ8++yzNj3XnjYjR1zNLkSP6VsBAMmfD1F8+0RERN7InvO34s0uADB06FAMHTrUGZuusDNXc91dBCIiIq/mdXO7qFTuLgEREZF387rgw9eH0QcREZE7eV3woQKDDyIiInfyuuBDWvGh0Siea0tERERWeF/wIYk+1Mp39CEiIiIrvC74kDa6qFnzQURE5HJeF3wY1Hww+CAiInI57ws+JH1tyxh8EBERuZzXBR/FpWrdbdZ8EBERuZ7XBR+jft6nu83gg4iIyPW8LviQkgs+nDDVDREREUl4dfBRptEY3L90owBdPtmM77acc1OJiIiIqj6vDj6MYg/M2HAGmfkl+GrTWfcUiIiIyAt4VfBRUFJmcN+45oMtLkRERM7nVcHH5tMZBvdL1QKeWnAA09edFhdw2hciIiKn83N3AVzpem6xwf3d569j65kMbD2TgR3nruN6XrGZZxIREZFSvCb4KCnTICEt12DZtH9P6W6fuprj6iIRERF5Ja9pdknOzMfSg5fcXQwiIiKv5zXBR5CfryLbySsuQ0mZxvqKREREJMt7gg//ir/UolI1es3chrtnbuNgZERERA7ympyPQAVqPq5mFyEzvwQAcC2vGHVCgyq8TSIiIm/jNTUfgRWo+dBoBAiCAD8ffV/c67klShSLiIjI63hP8OHn2EstVWsw4Js4PP3bQYO5YDRsdiGiKiS/uAxTVp3A3guZ7i4KeQGvCT5UKsdGEDtxJRvnMvKw9UwG1JKA441lR7HtTIaFZxIRVR7fbTmHP/ZexKNz97q7KOQFvCb4sNcd07dg8f4UBEhqTPKL9cOzn0nLxZMLDrijaEREiku6nu/uIpAXYfBhRmp2ESavOI6jl7J1y3IKy8yur9YIePjHPRj/5yGb91Gq1uBsei57zhARkVdh8GHFuyuP625nF5aaXS8hLRf7k29g/Yk02WAit6gUKZkFBsteXHgY/b+Ow+L9HPyMiIi8B4MPO+SXmNZ8xF/Kwtn0XIMZckvVpsFHj+lbcfcX23D+Wp5u2aZT6QCAn3dccEJpiYiIPBODDzu89fcxk2UjZu9C/6/jDHrClKpNR0DNK88X2Z143XTDnE2XiIi8CIMPhfy6K1l3Wy740JHpdcPYg4iIvIlXBR9Th7Vx2rb/PZqqu330cjaSrufL5n74yEQajnYDJiJSCtPeyZW8KvgY17OpS/Yz9tf96DMrFtP+PWXymNzIqHKhR/ylLLy2NB7pOUVOKCEREZH7eFXwAQAH3++Hfq3rumRfC3YnAwB+2n5et+zrzWex5XS6wXoqFUxqSUbM3oWVR67gjWVHnV5OIiIiV/K64KNW9UD8NKYzfny8k0v2l5FThOnrzxgs+35rosH9s+l56Pn5Vryx7Cg+XnMKiRm5usfOpeeBiMjZ2PhLruR1wQcA+PqoMLBdffz2VDcAwO3NamDSvS2dsq8v/ztrsiz+Uhb+keSIAOKgZn8fuoxfdiah31dxuuWcQ4aIiKoaP3cXwJ16tayN7W/2Rv3wYMyNO2/9CQ5YelB+ALFXFh+x6fkaxh5ERFTFeHXwAQCNa1YDAPjIdUPxCIw+iIioavHKZhc5YUH+JstqVgtwQ0kMsdXFfpwrh8g1didex8QlR3Az37QXH5ElXl/zofVApwaIO3sNzetUR+v6YegYHYHaoYG4dcoGt5ZL6ZyPUrUG/r5VN+b8+9BlzNxwBr+M7Yr2DcPdXRyiKm3Uz/sAAL4+Pvjy4Rg3l4Yqk6p7FrJTSIAf5j7RBW8PvBX3xUQhukYIgvx93V0sRXM+pqw6gTYfbMClGwXWV66k3lh2FBm5xXh1qW05Nd5m34VMvLz4CDJyOX4MKefyzap7TCHnYPDh4ZSs+fhj70WUqgXMjav6E9mVyUzuR8Ajc/fi36OpeG/lCXcXxeuxeZC8GYMPD6c9PmUVlODz9WdwLl0cA+Rcei4++vcUruUW279NSRLrvguZuJiZr0hZqfKoirVfBSVl+OjfUziQfMPdRbHq6KUsdPx4E5bsT3F3UXQYCpErMfiw4pexXRAS4IvvH+volv3nFZfh07Wn8N6qE/hx+3nc+7U4BsjQ73fi111JeGf5MZQZTWRXUFIGjQ3tNQlpuXhk7l70+iJWtywzr5hXZF6gKn7EP2xNxK+7kjDyxz3uLopVryw5gqyCUryz4ri7i6KIKvh1Iidj8GFF39Z1cXzqAAyLiXJbGebtSMLaY1d1909cyUZxmRhwbDmTgfZT/8MvO5MAANdyi9Hmg414bN5eg22oJcGI9sRzMjXbYJ2d566j8yeb8drSeCe8CiLnSrpeeWrwqmLwR44RBAFvLDuKL/9LcHdRXMrpwcfnn38OlUqFiRMnOntXTuPrYWOADP1+p8H9wlI1Pl4jTmK34WQaAGBfkr7qeeWRy7ht6kaT7RgfAH/Ydg4AsCo+1WRdqloEXqu6FSeyJq2TqTn4+9Blk2k3qjqnBh8HDhzATz/9hNtuu82Zu3GrZ+9qisXP3u7uYuj4yQRKry09ivwStdXnqji7g9fglTeRZ9DWYnsbpwUfeXl5GD16NObNm4fIyEhn7cal1r96l8myl/u2QI/mNT3mSkYafHy16Swy80wTUs2dd6y9BkEQkF1YWoHSERHgmZO4VahMDGbJTk4LPiZMmIAhQ4agX79+FtcrLi5GTk6OwZ+nal0/DDFGA1cF+olv4W9PdnNHkQy8vPgI3vz7mO7+d1vO4Y1lR82uLz1epOcUWQ0+3lh2DDHT/kP8payKFdQFPCUY9FRV8VzBz5yo8nBK8LFkyRIcPnwY06dPt7ru9OnTER4ervuLjo52RpEU882jHdGsVjXd/YDy0ULvblkbW1/v5a5iAQD+PWqaq7HrfKbsuhqNALVGX93X/+s4k2aXrIISHEi+oev9svzwZQBirwJ7FZepkVXAIZiJAEDFSMljnUrNcWl3bW/9KigefFy6dAmvvvoqFi5ciKCgIKvrT548GdnZ2bq/S5fkZ4H1FE1rVcOmSb0Q0zAcA9rWNXsQ2fduXxeXTF6JTHvion0paPbuOry9XN/NL7uw1ORH0P/rOIz8cQ82nkw3WF5QUmZ3Ofp8EYsOH22SbQaqKq5mF+KhObtlg0BPUxW7UzNnyY2q0Fs/+LsdGPnjHmTkuH4U4Kr4uzRH8eDj0KFDyMjIQKdOneDn5wc/Pz9s374d3333Hfz8/KBWGyY+BgYGIiwszODP0/n6qLBqQk/8NKaLwXLpaKQRIf4IDaq8U+cIgoCM8gHMNp0yDD52n89Esky3xpIyDXafv47iMtPk1tRs8Ye8P8nzB4By1IerT+LgxZt4ebHnD+3uPYc4cokq+IW6mu364EPJ6TQ8neLBR9++fXH8+HHEx8fr/rp06YLRo0cjPj4evr7uny9FCdaqTf19fGwa6MtTNZ28Tnf7SlYBrhvVWPSeFQsAmLHhDJq8sxa7z1/H1H9PYtS8fRaH7q7Kk9plFTAZl6gyk9Y8uKM5hDUfFRAaGop27doZ/FWrVg01a9ZEu3btlN6dR2lWqzruaF4Tw2Ki4OOjwtDbxIHJbmsYjtBAz68FSc0qlF2+98INdPlks+xjc2LPAwBGzduHRfvEoaL/PiTmhly6UYCSMo3BD+rY5Sy8t/K4Qa+ZuXHnMf7PQyYjtVpzMjUbA76Ow2ajmhmyQVU8xlWiqn9HiqquxBczlYX0LXZVM550L970CVfdy1A38PFRYdGzt+uGYv/wvjaYNTIGC57sZvCl6tok0qTXjCc4f82+ESKf/f2g2cdeXnwEd83chsfm7UWJJKj4bmsiFu5Lwefrz+iWfbbuDNafSDPJLbHmhT8PISE9F8/IlKMSnYfcwpsOcvaSy5Nyt2OXs9D2ww34eYfzJoWsyHfCmYPWLd6fgqUHXDMHjkahmg9BEHA1u9DumgwlJxL1dC4JPmJjY/HNN9+4YlceJSTADw91boga1QIMli974Q78/nR3TLq3JQAgNMgPy8ff4Y4iVohxLoiUNuny0MWbsoPonL+WZ7Isr9h8s0V6ThGmrDqBs+UT6wFANps5PFKZWoPYhAyXjwmjRMD51aazaPn+ehxOuanA1iyws7BvLz+OolINPll72jnl8VA5RaWYvOI43l5+HPnF9ie620tau1SR4ON/sefRY/pWu3sGelHswZoPVzGOgMOD/fFK3xY4/dFAHHivHzo3rhoDsclJyZSZQVXmR2auVvmvA5fQ/bMt+GPvRfT/Og7HLmd5ZNtoZRqy3Jnv37wdSRg3/wAem7vX+soe5rst4hQDn3rZSd5TFZfqL1ycNRLozfwSXZK8oFCzyxcbxXlavtx01uq60vxBSz/Lj9ecwuBvd6DQhtGqKwMGHy7Sql6o7PLgAF8E+Zsm4W567W7Z9R/q3FDRcrmC8Vw0gHz1onRZXnEZ9l3IhEYj4K3lxwzWu++HXZi+/kwlOtV7lxXl48Gcuuq5AwZa4+zgls2CtpHOFuGMJom07CJ0/HgT7pm1HQCgdnfCqYWj2i87k3Dqak6l6MpvCwYfLvLDqE54qHNDrH3lTqvrRtcIRp0w/RgpXzyknxvntXtbYvMk9w5mpgQBYrfb/8XqqyWlNR8v/HEIj8zdi6UH5cd9mRt3AblFFa+G3XAiDYkZudZXVJhaI2DDiTSku2EsAcC5OR/uCgqVHLiLga19nBWr+Ug+U2cEH9vPZgAArpQn2yuV8+EoW15iWRVJPPb8LhhVRFREMGaNjLG4Tu3QQFzLLUavlrURLKkNuatFbd1tX5UKDSODnVZOVxEEAQ//tMdg2cK9FzFrYwIWPtMdOxOvAwB+253stDLsOZ+JF/48BABI/nyI0/Yj56+DlzB5xXFUD/TDiWkDXLpvwLlty57YJGYvT3sJVeE9rSiNC/KABck+3DFonTclnDL48CCrJ/TEplPpGNmlIQL8fLB50t0AVAgP9teto22m+XhEO5SWacSTV2o2ft9z0X0Fd8DhlCyTZWfSxBoIaTONdpm9kjMLoNYI8JWZ5Tfpej4S0nKRcsO+3j3W2HPciDt7DYDYvFSZaDQC8kvKEBrkb3Yddx0/2ZRR9UhPxs44MRsHGO5odvHWrrYMPjxIVEQwxt7RRHf/ljr6PJHvH+sIjSDoApExtzfWPfYwohES4Icft593WVk9TWJGHm6pU91g2ZpjqRjeoYHJun3KB0i7q0WtCu1z25kMTPv3JL58OAadG9cweVwQBLNNAXJ5Pq7kaHLs47/sw+7zmYh7sw8a1QyRXacqXL05+xV44twuFSmRs94v6XZd8b1y93dX8Lxe3k7DnI9KYlhMlOyJVOvtga3Mzifz8YiqPbgbAPT7arvJ+AzW8il2nLtu8fHswlKcuJJt9vEnFxxAcmYBnvhlv8lj/51MQ5dPNmOnmX0EuHmkV0ePsbvLJyrUTjIou23HNu1ZFDwJlao12HYmAzlF3tc1fO+FTLy2NN7inE5J1/MRm5Ah+5g0GHDFIGvSUandEYdUph5zFcXgo4pQqVSoGxaEzZN64d42dXXLlzx3O9rUl+9pU9WsP3HV4L42WW3SX/GYtDTe7u31/TIWQ7/fib0X5GcG1sqX6fr23B+HkJlfgjG/7pN9TqB/5Qg+/th7Ec/8dgBFpYav0dLT3XX1qGRlgvQVbDqVjl2JlgNVS77fcg5PLjiAcb/qg1TPq/dwjkfn7sXKI1cw9d9TZtfpMysW4+YfwKGLMvM+ST4IpyRaGn0Q0l2443us1C5LyjQ4lZrj0blCDD6qmFvqVMfsUZ3w7aMdcOC9fri9WU34+Xjnx+zro8LN/BKsOHwFK45cwdn0XLM/RumIq1rX80oAiD1ibCG3ZXO//UC/yvGZTFl1AptPZ2DpAaNeRxYOah58vLOZ9jVk5Bbh2d8PYvTP+xw+kGunG5DLc/IWFzOt51fFXzKtZTQIBlxQ8yHN+XDH91ipgGf8n4cw+Lsd+HOv5+YCVo4jINklwM8Hwzs0QO3QQABAdcnsupsn3Y0Lnw3Gd+VDwFcl1Y3mz1l3/CpWxV/R3e//dRz+FyufF/Pj9vM4mZqNcfP3m8wVs2B3Mi7flBkozQZyCa+A+BlpefLViVauUZOBpfNAVUo4lU4W6OiVt59ME5sHpnxUbHh1Kx+6LSdVueBC2gzhjJoP449BWga31HwotJ0tZ8RmrF93JSu0ReUx+PAC2iAEAPx8fODjo7LYFNO+gefNO2OLaUZVuweSb5os0448KGfIdzsRm3ANz/x+0GD8EQD4eUcSAODVJUfQ5J21Jo+rNQJKZSbG8zMTfAT66RNOS+ycUA8QhzD/X2wijl82n5Niib3t58arW2qbrgzBlDXa1ycdZ8LRnANz3wFv4uj4FdJFLsn5cHPA4e6EV1di8OEFQgP9EBHijyB/H9QLFwcvu6VOKP5+oYfJutve6G0yF01lkXLDsdoJOTM3GAYpC3Yno8k7a7E6PlX28aHf78QxmUDAv/yqNzOvGAt2JeFmfglSMgsMml0sDZecmVeMzafSTU7o83clY+aGBAz7wXT0WFvkl9jXxdf4oGjpGFkVxkDSvj5p3CAXXNrCXO2XN7HlOyF34hVcnXDqhpwPg992Ffjt2Ipdbb2ASqXC3sl9UaYRDLp4dmlSAzvf7oMv/zsLXx8V3hrQCnXCggx+dGNub4zLNwsw9b62GPLdTpvGpejVsja2l49j4S1OmxlK3M9XPPE89dtBHL2UpUu8kza7FJaqEWH0vOWHLqN2aCBmb0vEvqQb+GBoGzx1Z1Pd43HnLL+/f+xJRliwv9keUvaOL2Ja82FpXecdQQVBQEGJGtUCTQ9dio5wWv4SpIFDmdqx16VE8OHpF8TWimdLbZhccCF9mitG9lS7obeLYOa2+fU9/MtgI9Z8eIkgf1+TnAgAaBgZgq8f6YBZI2N0Q7pPGdoGoUF+eGtgK3w8oh3mP9kNjWtWw4aJd+meN/S2+lg1oScA4MFODfFYt0aIiY7AuU8H4cfHO6Nf67om+/JGWQWlOH01B0cvZRksl3YLvplfanD/bHouXl92FE/8uh/7ksQeAPOMplKPN9qe1NXsQkxZfRKvLok3m6QnCGIyZalag5kbzmDwtzsszhpqfPKwdGB25qHxvVUn0PbDjSZdoAtL1E65MpYOQlXq4BCb2gDU3Ha1ruUWY8n+FBTYWStVGdgSkMp9fs7uamscsEq/5+6o+fCmZhfWfJCJlnVDEf9Bf5MrtoaR+kGlBAHoEB2BY1P7IzTQz+BH7O8L/Dy2C77fcg61QwPxzorjLiu7Jxr07Q6Ljw/+Tny8T6vaeGdQayTL9AyQHpMy84pN5rWZG3ce8ZeyMPOhGJy5qh8V9kxaLk6kZmNEhwYGtS0A0O3TLbitYbiuuajthxtx6qMBCAkwPSxcuJ6P5Ov6clm6krX3+KnWCJj0VzxiGkYY1O7IWbQvBQDww9ZE/DimMwCxFue2qRtlq/ZLyjQmr9sWgu6/JOHR4ZoP2/Y/8sfdSM4swOWbhXhjQCuH9uWpbPlOyDe76G87J7g0LoNk34rvTZ5Bq4vCO/Xk/CvWfJAsc1XFnRpFAAAe7hoNAAgL8jdb3f1y3xZ4tFsjHHy/H16/t6VTylmVbEu4hgHfxMle+ablFGHML/tw6UYBDl28afL4Z+vOYN3xNLT7cCOeXHBAt3zwdzvw1t/HMCf2vOyByDhPZfmhy8jIKcLWM+kGTTNrj11F71mxNr0Oew94W06nY3V8Kj5aY34sCGNFZfo8mYPJN2QDjzXHUtHy/fVYecT8gGjmaF+DEgmPcgmncj+Z5EwxZ2lHBcYU8VSeWvNhsC+NYNTs4qKaD2kZPDhYUBprPsgui5+7HalZRWhaq5rNz6lVPRAv922BLzedLb8fgFvrhaFz40h8u+UcACD2jd6IrBaAmGn/mTy/RZ3qOJeRh0e7RmOJ8XgTVZBxDx2tHeeu466Z20yWT19/2uo2v958Fgt2J1ldLzEjD1NWbwEA9GhW0+x6cofIbzefw/7kTGTml1jdj1SBhYRbc4pL9U0g5saxeWnREQDAa0uP4v6ODe3avqZ84sOcQn1XW1clnFaWMWCkrJ0zbTmnyuZ8SG6XOXlmOY0gGJz8XZbz4cSaD09W+b7l5FaBfr52BR5yBrWrjz+f6Y67W+pn660fEYTwYH+sePEOPNIlGu0ahOke2zSpF5KmD8bnD95Wof1WFtKxJWzx0/YL1lcCcNOG7a45ph8ldo+FkV21uSRn03PR98tYrDh8GV9vPotdiZZHg5VjKU/00o0CZBeallta8+GM3iRn0/OwP+mGwcSGcgmPO89dx5Pz9+umZAfEfJzbP9uC1eVjzNjd1dZoN99vOYeEdMcmWPQUtiVSyixzYT6ERjCeyM6pu9NxZg2LJ8cyDD7IZZaPvwNjezTGmwPF9mzpMdm//Oq1U6NIzHjoNvz9ghiE/FTerq9t2qmsY5BUFnLJkXK0B7VJf8Xj/LV8TPrrqNl1b+SXYPKKYzicIjYXHUy+gSbvrMW0f08CMEz6m/bvSd3B+Gp2Ie6auQ2dP95ksk1p92S54OOSUbdrtUbAqiNXTJbbQ67m4/Ff9mFbwjVMluQ1vbToMNJyivDqkniz5bPEuKlBW2Mou65GwKGLNxSZHdmZHYJtOcHKrWPQ28XBnBtLpIGvWPNhuTxKK1Nr3D6ku7sw+CCX6dw4EtOGt0NY+XTszWrpZ6H1MTpAB/n7YsZDt2FA23oGy7W5JsZmPngbnu/VDOc/G4wHOpl2L90vmXSvl6TGhQz52zjhnfYgeeKKfBdjrdyiUkz79yQW77+EB/63GwDw0I97AIhjlQCGJ735u5Ix/s/DeHflcV1uS1n5AG7S2oUzablYuO8i5sadtyn4aP7uOkxcGm/SbJWRU4R/j6aizIYmFUs5B2nZ+rIZT3Bob82HRhCQV1yGH7aew1krNR5/H7qMB+fswZhf5OcQ8hRyb50gCAb5OMcuZ+NUquH3ydmDjEmDD0Ew3Iezaz6+23IObT7caNBN397Y48K1PGwzMymfp2POB7lNeIg/drzVx65J1kZ1a4RbaldHyo18dG5cA1P/OYkgfx+M7NJQdwU99b628FWpsOyQ/sBWJywIi57pjjnbz2PafW0x6NsdKCy1P9egqrt8s9D6ShBPsMYnWTntp5rm8EjN3pZoEihsOCnOpXPhWp5u2eM/79N1O9Z6b+UJAMAncrM2Wzjff77+DF6+5xZUC/TDsB92Ij2nGJMH3YrnezW3WNZSC1fe0q6zxsGQXG8XS2OSaARg5oYz+H3PRcz6z3ytBwAsPSjmQB1RYN6Yipxrtc/VaASTCwlA/op+/Yk0vLZUX2O2L+kGBn+3A4mfDtINSe/I8OpZBSX4fmsiHuzUEG2iwqw/QVJGaW2Hs8fT+Kq8RuuTtfocL3M1H+ZqYe75cjsAYMWLd6BTo0iFS+hcrPkgt4quEYI6oUE2r+/ro0KP5jXxSNdGuKVOdfz5THf8PLarwcE8LMgfX4yMwUOdxSTDiBCxpuWOW2rhj6e7o0mtavA307wwunujCrwa71FUqsGDc3bb/bwm76w1uP/FxgSzScR7L+iDDePAQ+parul07XLjaGj9uP08vi4/8KfniM+dLjOxoDFbakcA05oP4+9amVpjcLW7O/G6QQAmADiYbNqjSY4nDd1++WYBOn68SXaSRrlzp1yvLQAolrx/0hxTW5skpq87g192Jum6sNtKIxj2djEXe5SUaSyOiWMvW7r3Wnvpjk6z4E4MPqjK+mBYG7w1sBX+mXCnyWMPddY334zq3ggt61bH4Sn34sHO+l4RzWrLJ9Y+1s18gNKmvu1XWpVZWk4hjl/xjAOebK6AlavWA8k3sOZYqsEy44HLjEmvvPOKyzD65726+9rY99KNApPePtKaEEEQsCrecL+jft5n0BwkCILZ4NiYrTk6WjlFpVgdf0WRHBFjX/13FtmFpfhxu+nkjXKBg7lgQppb48g4K4mSGjMAuJJVaNOsumLCqeF9OX1mxaLthxud8h6aCzKsvXJzNSNyiwVBwKnUHId7bymFwQdVWWFB/nix9y1oVDPE5LE3JYM4vT+kNf57rRdqVAtAp0aR+GVsF2ye1AsrX+wps00/TH+gPVa8eIdsrsF7Q1rLlkUuD6Uys7V5xhXmy8zc+eayYxafc/Rytq4rrtbw2bssPkd6sP55xwWTnj2ZecUmOSVlao1B7YRaI+AvK93FNYIgOxOuVpN31iI2IQNFpWqz3YzNeWXxEby6JB7vLNe/P+ev5eH1v44aNHMBwPW8YptPsGqNBjlF5ntTXc0uwnsrjyM9pwgZuUUAzJ9opRMtGg8yVlSqxvJDl3Ettxh7L2Ril8yYKOHB/rrbGo2Anp9vRa8vYmVfi7SGTLCx2UWbe+SM2gbzQYTl8MOeBqLfdidj8Hc78LLR99/VmPNBXik4wBebXrsbZRrBZETPvuVDwwuCgAYRwSjTaPD+kDb4ZO0p/DCqEwCxV86xD/vjz70X8eV/Z7Fh4l1oVltMoD3z8UAcu5yNSX/F4/LNQtSsFoA7b6mFFYev6Pbx+1Pd8MSv+130apV3MVO5SfwqKlfmpCJNTrWVtYTGT9aeRu9WdQAA645fNXjsTFouOn+y2eQ5nT/ZjL631tHdH/r9ToPuu3IKitUIksx6LGfcfHEguTua68diOXopCzHREQbrHU65idrVA1FUqkZESABiE8Q5gdYcu4ofRgEfrj6B3/ZcNNn+jfwSdPlkM2pVD8DB9++1WBZAbIootpIDtHBfChaWj1C7460+Zk+o0mYrg54/goBZGxPw884khAb56Ub5PfPxQN2cVWqNgK1n9AmY0kAmI6cI1ct/owUlZdifdMPgcbVGgNpKV1tn94Axt3XDXjgyzzNbY2L6wE9xYtd8bW6VuzD4IK/Vom6oxcdVKhVi3+wNdfmEfMNiogwerxboh+d7NTdJVAzy90W3pjWw+Nnb8fueZDx1Z1PkFOpPkD8+3hl33lLLZH9v9G9pkmDYp1VtbEvwrkn6PFViRh5aT9mAacPb4mx6nvUnAMguLMXKeH3QaS3wAMSh7C9ct95MAAC7z+trXx6btxcD29VDdkEp5j3RBYdTbup6FpkjF3gAYrMUAFzPMx0wLrugFBMWHUYPSeBTUqaxq0vxE7/uR/Pa1WUf+2JjAr59tCMA04nltCdM6fQCRaVqXfBhHBSWGfVeuXAtDzWqBeDNv49h06l01KoeqHtcbUNXW7mk1/ScIiw9cAmPdou2K39NjrkA2FozYmXsoMvgg8gCf18f+Fu+CDUrukYI3hvSBgBQPxz4dVwX1AkNQrvysUq+fiQGry09io+Gt8Xj3RvDx0eFCX1uweGULKw7fhWv92+JjJxiHEzeqbu63/5mb/T6IlaJl0YOKCxV462/LTfpGHPV0A0FJWpd7drGk2kYv/CwxfUttflr59ABypuOJM1AX21KwM7E69gpafIoLjNsXvpp+3mLkx8mXc9HkpkAa3V8Kr59tCNKyjTYcjpdt1ytlg9wbhaU4npeMW6pE4q07CKDx6RJwqlZhXji1/2oHuina4K5nlcsWVcwmIhR7nOTyzt57o9DOHopC9sSMmSbaqUEQcD2s9fw10H5prciMz3wrI8gKyAxIxfvrjyBiX1bWF7ZQzD4IHKRe241nOn3/o4NMahdfd1VGyDWtnRuHInOjcVuc01q+eHIB2K1d1GZRnZmYjnP3NkU56/l6WpNRndvpKvyrojn7m6GuXG2jahK7mMt8ACAAV/HmX1s+1l9bdu4+QcQWS0AzWpVw6mrObLNKyVlGmw+rW/usKX3kCVJ1/PR98tYg5qIj9eelq0Z6DMrFgCw9pU7TZJYpSf5E6lijoa5PJaiUrXB69bWNmg0AhKv5aFRjRCDmY21j2tnrDbX3fm9lceRkJaLGtUCcC4jz2zQBRgOngeITUVz4y4YJMLr9m/0Wl9adARn0nIx6mf5MV80GsFgZGB3Y/BB5EZBNlSraK86q5f/X/fKXUjNKkS7BuH4dVcSfH1UeLF3c1zPK0FkiD9OpuagR7OauvEWtFdzTWtVwydrLc8D8+9Ld+LgxRtm55d56Z5bDIKPhpHBBsmnT/Vsil93JVl9TeR+tjbt7LRhojt75/OxRhtQSFnLydlx7rpJB+vP1umDoCtWkqSLSjVYsDtZd18bZ3y27jR+3il+p3e/c4/u8f1JNzB9nWGQVVymRqBRvo49QX++UfDx0qIj2J98AyuO6Jvu/juVjsdvb2ySByLXRAaIAc34hYd0+T6WpjNwJQYfRJVMm6gw3eBJ7w7W964JLR85tqdRPok2CHnmrmZ4smdTfLr2tEGAcGLaADy94AAGtK2H9g3D0b5hOO5qUQv9vjK8Mv7vtbsRFuSvm+gPAN4f0ga+Pio0q10NzWtXR6laIxt8TLq3pW5QJWNzRnfC0fKeA3LdNIlsER7sLzsPkJa1IKDYqFZgZ+J1fL35LE5KRl2Vjk3yzeZzJtto9f4GvDWwlclyW2lntC5Ta3A4JQv7y3NvbkiCu7jy2hlps9n09adRPzzYZHuCAPx7NFUXeGiXeQIGH0RexNdHhQ+GtUH3ZjUw7Z+T+OT+dqge6Ielz/cwWO+WOqFImj4YZRoBx69ko039MF0tzfIX78DW02JXzwFt6xoM8Obv64PpD7RHYYkaH63R156MvaMJft+TjMiQALRvGG7Q82dQ+/oY1L4+sgtL8U/8FaRmFyEyxB+zR3XCjI0JumptAPjz6e543MOHEif3mLziOO5tU9f6imYUlRo2J0lrQbR+32O6zNjMDQkOl2Hd8au4cC1fN9u3ObsSr+uSggHzY5JkFZRihWQIe0+iElwxe44dcnJyEB4ejuzsbISFeceATURV0ZGUm5gTex7vDWmNxjWroaw8YVClUulGOr2rhTjqrFapWoNStUbX/Xn0z3t142l0bhyJv57vgQ9Wn8DCfSk49dEAXMwswFebzqKwRG1T84At5HodeTppbRQ5RpsA7k2SPx+i6PbsOX9zkDEicoqOjSIx94kuaFxTHCnWz9dHV0vy+r0t0ahGCL4cGWPwHH9fH4NxV17sfQsAYFhMFJaPFwd2+/T+9kj+fAhCAvzQun4Y5j3RBf97vBNmPngbTn80EPd3FAd0++2pbhjYth6iawRj/riuum3+NKYzTkwbYLbc0tFvzenUKMJk2fGp/RES4GDXqAp6e+CtbtlvVeJtgYe7seaDiDza9bxi1AgJkJ2wTI5GI+BGQQlqVQ+EUD52g6+PCsnX8xES6Ksbi+HD1SdwOCULY3o0Nug+m/z5ECzYlQQBYkLw5BXHDbb/wdA2eOrOpgDEng7DZ+9CndBA7J3cF1mFpUjLLsKzvx/UDXQ27o4muir8T+9vhz/2XLQ43seLvZsjPacYCek5VmcN1vrtqW4YKxm07u2Bt2LGhor1ODH2UOeGOJWag1NXbSsTeT531nww+CAir5eRW4RHftqLh7tEY3xv09ltT1/NQVGpGjcLSky6TF/LLYaPCqgpGbDqZGo2Xlsaj9f7t0L/NnVxPa8EtaoHQKVS4WZ+CXYkXsfUf07qEgkjQ/wxoG09vHZvS9QN0w9UdejiTey9kIlR3Rqh48ebDPbbpGYIkjMLMPS2+vj20Y54Y9lRhAT44okeTdCqnjiA3qZT6Ziy6gSmP9geP8ae103Qt+2N3gY9Sn4Z2wVXs4vw/qoTsu/P+c8Gw9dHhVK1BnFnr+Hp3w7a8e6Sp2LwIcHgg4i8QZlag70XbmDt8VRM7GcYdMiRzgjcpn4YFjzVFeuPp6Ffm7poEGHa08HY4v0pmLziOBpEBGPXO/cgIS0Xo+btxccj2mFw+/om+2hWq5quO67xSerer7bblGPStFY1pNwo0HWTfaBjA2xLyMA9t9ZFuwZhZrt0f/9YR7y82L65RxI+GYj1x9MwcWm8blmt6oFQqeRnPiYGHwYYfBARmZq9LRGr469gyXM9EBnib9DLyBYajYCdidfRvkE4IqsFyK6jDT4e7tIQ7w1ug5NXs9GjWU2Tfak1AjJyixARHICfd1zAl5vO4v6ODXB7sxp4e7nYTLV5Ui80jAyGWiPgrb+PoX/buhjewXCCxb8PXcYby0xzLTZPuhvVA/3xU9x5tI0KN1hnfO/mmBMrdslePaEnhs/ehVvrhWLDxLsBiAOULTt4CU/f2RQ1qgVgzvbzFnug/PvSnRj2w05rb59NujSOxEFJd1wA6NGsJvZcyDTzDHmNa4Zg0r0t8eqSeEXKJWfO6E4YVB50KoXBBxER2W3jyTQsO3gJXzwUYzZAkXMzv0Q3m+ziAyno2qQGWlqZO0nq0MWb+OvAJSwtH5HU+Ir85x0X8Mna0/jx8U4Y2K4+MnKKUDs0ECqVCpduFKBm9QCTCSK1StUabDmdgS5NIqERBHz07ymsOXYVnRpFYEX5cOhvLjuKZYcu44VezdEgMhhTZJqfQgP9oBYE+KhUqB8ehHMZeZj3RBdoBAEdoyNQJywI2QWlSLyWi1OpOZiy+iQAYP97fdHt0y02vxcAsP7Vu9C6fhjm70oyWzsEGOYT2Sv+g3sREWL7Z2wLBh9ERFSppGUX4Z4vY3Fvm7q6ieWkpBPIVYRGI2B/8g20axCum66gpEyDE6nZiGkYAV8fFc6m56L/13EY2bkhmtepjsX7U7D0uR6oFuiL4jINAv18cP5aPmIahsvWQCVfz0fv8pya5M+H4K2/j+Kvg5cxvEMUnr2rGZrVrgYflQo+KhWGfLcDtaoH6mpH/n6hB7o0qaHbVkmZBik3CnBLnepIySzAxRv5OJB8E8M7REEQgH5fbTfY90OdGyLI3wd/7jUdVC3I3wdFpRoMbl8Ps0d1srv2zBoGH0REVOkUlaoR6Oej+EnRHbafvYaa1QLQrkE4ikrV2HM+E7c3q4lgo+7YGo0AlQrYe+EGLt0swMNdrHf1lsouLEVooJ9Bb7BStQYf/XsKd7aohS2n0/HXwctoVrsa/pt4t8EkgUpj8EFERETILy7D8sOX0b9NPdQLt5zUXFH2nL85vDoREVEVVS3QD0/0aOLuYpjgCKdERETkUooHH9OnT0fXrl0RGhqKOnXqYMSIEUhIcHyiHSIiIqpaFA8+tm/fjgkTJmDv3r3YtGkTSktL0b9/f+Tn5yu9KyIiIqqEnJ5weu3aNdSpUwfbt2/H3XffbXV9JpwSERFVPh41q212djYAoEaNGlbWJCIiIm/g1N4uGo0GEydORM+ePdGuXTvZdYqLi1FcrB93PyeHMyYSERFVZU6t+ZgwYQJOnDiBJUuWmF1n+vTpCA8P1/1FR9s3wAoRERFVLk7L+XjppZewevVqxMXFoWnTpmbXk6v5iI6OZs4HERFRJeLWQcYEQcDLL7+MlStXIjY21mLgAQCBgYEIDAxUuhhERETkoRQPPiZMmIBFixZh9erVCA0NRVpaGgAgPDwcwcHBSu+OiIiIKhnFm13MTQg0f/58jBs3zurz2dWWiIio8nF7swsRERGROZzbhYiIiFzK42a11daccLwPIiKiykN73ralBcTjgo/c3FwA4HgfRERElVBubi7Cw8MtruP0uV3spdFokJqaitDQULPJq47SjiFy6dIlJrN6AH4enoWfh+fhZ+JZ+HlYJggCcnNzERUVBR8fy1kdHlfz4ePjg4YNGzp1H2FhYfzieBB+Hp6Fn4fn4WfiWfh5mGetxkOLCadERETkUgw+iIiIyKW8KvgIDAzEhx9+yOHcPQQ/D8/Cz8Pz8DPxLPw8lONxCadERERUtXlVzQcRERG5H4MPIiIicikGH0RERORSDD6IiIjIpbwm+Jg9ezaaNGmCoKAgdO/eHfv373d3kaqkqVOnQqVSGfzdeuutuseLioowYcIE1KxZE9WrV8eDDz6I9PR0g22kpKRgyJAhCAkJQZ06dfDmm2+irKzM1S+lUoqLi8OwYcMQFRUFlUqFVatWGTwuCAI++OAD1K9fH8HBwejXrx/OnTtnsM6NGzcwevRohIWFISIiAk8//TTy8vIM1jl27BjuuusuBAUFITo6GjNnznT2S6u0rH0m48aNM/nNDBw40GAdfibKmD59Orp27YrQ0FDUqVMHI0aMQEJCgsE6Sh2jYmNj0alTJwQGBuKWW27BggULnP3yKhWvCD6WLl2KSZMm4cMPP8Thw4cRExODAQMGICMjw91Fq5Latm2Lq1ev6v527type+y1117Dv//+i2XLlmH79u1ITU3FAw88oHtcrVZjyJAhKCkpwe7du/Hbb79hwYIF+OCDD9zxUiqd/Px8xMTEYPbs2bKPz5w5E9999x1+/PFH7Nu3D9WqVcOAAQNQVFSkW2f06NE4efIkNm3ahDVr1iAuLg7PPfec7vGcnBz0798fjRs3xqFDh/DFF19g6tSpmDt3rtNfX2Vk7TMBgIEDBxr8ZhYvXmzwOD8TZWzfvh0TJkzA3r17sWnTJpSWlqJ///7Iz8/XraPEMSopKQlDhgxBnz59EB8fj4kTJ+KZZ57Bxo0bXfp6PZrgBbp16yZMmDBBd1+tVgtRUVHC9OnT3ViqqunDDz8UYmJiZB/LysoS/P39hWXLlumWnT59WgAg7NmzRxAEQVi3bp3g4+MjpKWl6daZM2eOEBYWJhQXFzu17FUNAGHlypW6+xqNRqhXr57wxRdf6JZlZWUJgYGBwuLFiwVBEIRTp04JAIQDBw7o1lm/fr2gUqmEK1euCIIgCP/73/+EyMhIg8/j7bffFlq1auXkV1T5GX8mgiAIY8eOFYYPH272OfxMnCcjI0MAIGzfvl0QBOWOUW+99ZbQtm1bg3098sgjwoABA5z9kiqNKl/zUVJSgkOHDqFfv366ZT4+PujXrx/27NnjxpJVXefOnUNUVBSaNWuG0aNHIyUlBQBw6NAhlJaWGnwWt956Kxo1aqT7LPbs2YP27dujbt26unUGDBiAnJwcnDx50rUvpIpJSkpCWlqawfsfHh6O7t27G7z/ERER6NKli26dfv36wcfHB/v27dOtc/fddyMgIEC3zoABA5CQkICbN2+66NVULbGxsahTpw5atWqF8ePHIzMzU/cYPxPnyc7OBgDUqFEDgHLHqD179hhsQ7sOzzl6VT74uH79OtRqtcEXBQDq1q2LtLQ0N5Wq6urevTsWLFiADRs2YM6cOUhKSsJdd92F3NxcpKWlISAgABEREQbPkX4WaWlpsp+V9jFynPb9s/RbSEtLQ506dQwe9/PzQ40aNfgZOcnAgQPx+++/Y8uWLZgxYwa2b9+OQYMGQa1WA+Bn4iwajQYTJ05Ez5490a5dOwBQ7Bhlbp2cnBwUFhY64+VUOh43qy1VboMGDdLdvu2229C9e3c0btwYf/31F4KDg91YMiLP9Oijj+put2/fHrfddhuaN2+O2NhY9O3b140lq9omTJiAEydOGOSkketU+ZqPWrVqwdfX1yRbOT09HfXq1XNTqbxHREQEWrZsicTERNSrVw8lJSXIysoyWEf6WdSrV0/2s9I+Ro7Tvn+Wfgv16tUzScQuKyvDjRs3+Bm5SLNmzVCrVi0kJiYC4GfiDC+99BLWrFmDbdu2oWHDhrrlSh2jzK0TFhbGi7ByVT74CAgIQOfOnbFlyxbdMo1Ggy1btqBHjx5uLJl3yMvLw/nz51G/fn107twZ/v7+Bp9FQkICUlJSdJ9Fjx49cPz4cYOD7aZNmxAWFoY2bdq4vPxVSdOmTVGvXj2D9z8nJwf79u0zeP+zsrJw6NAh3Tpbt26FRqNB9+7ddevExcWhtLRUt86mTZvQqlUrREZGuujVVF2XL19GZmYm6tevD4CfiZIEQcBLL72ElStXYuvWrWjatKnB40odo3r06GGwDe06POdIuDvj1RWWLFkiBAYGCgsWLBBOnTolPPfcc0JERIRBtjIp4/XXXxdiY2OFpKQkYdeuXUK/fv2EWrVqCRkZGYIgCMILL7wgNGrUSNi6datw8OBBoUePHkKPHj10zy8rKxPatWsn9O/fX4iPjxc2bNgg1K5dW5g8ebK7XlKlkpubKxw5ckQ4cuSIAED46quvhCNHjggXL14UBEEQPv/8cyEiIkJYvXq1cOzYMWH48OFC06ZNhcLCQt02Bg4cKHTs2FHYt2+fsHPnTqFFixbCY489pns8KytLqFu3rjBmzBjhxIkTwpIlS4SQkBDhp59+cvnrrQwsfSa5ubnCG2+8IezZs0dISkoSNm/eLHTq1Elo0aKFUFRUpNsGPxNljB8/XggPDxdiY2OFq1ev6v4KCgp06yhxjLpw4YIQEhIivPnmm8Lp06eF2bNnC76+vsKGDRtc+no9mVcEH4IgCN9//73QqFEjISAgQOjWrZuwd+9edxepSnrkkUeE+vXrCwEBAUKDBg2ERx55REhMTNQ9XlhYKLz44otCZGSkEBISItx///3C1atXDbaRnJwsDBo0SAgODhZq1aolvP7660JpaamrX0qltG3bNgGAyd/YsWMFQRC7206ZMkWoW7euEBgYKPTt21dISEgw2EZmZqbw2GOPCdWrVxfCwsKEJ598UsjNzTVY5+jRo8Kdd94pBAYGCg0aNBA+//xzV73ESsfSZ1JQUCD0799fqF27tuDv7y80btxYePbZZ00ujPiZKEPucwAgzJ8/X7eOUseobdu2CR06dBACAgKEZs2aGeyDBEElCILg6toWIiIi8l5VPueDiIiIPAuDDyIiInIpBh9ERETkUgw+iIiIyKUYfBAREZFLMfggIiIil2LwQURERC7F4IOIiIhcisEHERERuRSDDyIiInIpBh9ERETkUgw+iIiIyKX+D57HQvWDXKdTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(history)\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n",
    "\n",
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.5, 1])\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_acc)\n",
    "\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: out\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: out\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
